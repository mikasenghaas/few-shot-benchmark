{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook contains the analysis of the data tracked on\n",
    "[Weights & Biases](https://wandb.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "We will first setup everything so that we can easily analyse the experiment\n",
    "results. This includes importing the necessary libraries, setting paths, loading\n",
    "the experiment results from W&B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:04:59.989806Z",
     "start_time": "2023-12-11T14:04:58.077712Z"
    }
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Bult-in modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# External modules\n",
    "# - Data Representation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# - Data Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotnine as pn\n",
    "\n",
    "# - Machine Learning\n",
    "import torch\n",
    "\n",
    "# - Experiment Configuration and Logging\n",
    "import wandb\n",
    "\n",
    "# Custom modules\n",
    "from utils import eval_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:05:00.037347Z",
     "start_time": "2023-12-11T14:04:59.990981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup of global variables\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
    "\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "METHODS = [\"baseline\", \"baseline_pp\", \"matchingnet\", \"protonet\", \"maml\"]\n",
    "METHODS_WITH_SOT = []\n",
    "for method in METHODS:\n",
    "    METHODS_WITH_SOT.append(method)\n",
    "    METHODS_WITH_SOT.append(method + \"_sot\")\n",
    "\n",
    "STYLED_METHODS = [\"Baseline\", \"Baseline++\", \"MatchingNet\", \"ProtoNet\", \"MAML\"]\n",
    "STYLED_METHODS_WITH_SOT = []\n",
    "for method in STYLED_METHODS:\n",
    "    STYLED_METHODS_WITH_SOT.append(method)\n",
    "    STYLED_METHODS_WITH_SOT.append(method + \" (SOT)\")\n",
    "\n",
    "styled_methods_dict = dict(zip(METHODS, STYLED_METHODS))\n",
    "\n",
    "\n",
    "def get_name(name, sot=False):\n",
    "    return styled_methods_dict[name] + (\" (SOT)\" if sot else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:05:00.090704Z",
     "start_time": "2023-12-11T14:05:00.023567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "sns.set_style(\"dark\")\n",
    "colorstyle = \"YlGn_r\"\n",
    "sns.set_palette(colorstyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:05:00.325492Z",
     "start_time": "2023-12-11T14:05:00.052982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"few-shot-benchmark\"\n",
    "WANDB_ENTITY = \"metameta-learners\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Benchmark\n",
    "\n",
    "---\n",
    "\n",
    "All models on all datasets with and without SOT for fixed few-shot learning\n",
    "setting (5-way 5-shot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Experiment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:08:35.639863Z",
     "start_time": "2023-12-11T14:08:35.569809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all runs for experiment `benchmark`\n",
    "GROUP = \"new-benchmark\"\n",
    "\n",
    "group_runs = [run for run in runs if run.group ==\n",
    "              GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load all runs from the given experiment group into a single\n",
    "dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:08:36.955554Z",
     "start_time": "2023-12-11T14:08:36.862970Z"
    }
   },
   "outputs": [],
   "source": [
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Each experiment is uniquely identified by the following parameters:\n",
    "\n",
    "- `dataset`: The dataset used (`swissprot`, `tabula_muris`)\n",
    "- `method`: The model used (`baseline`, `baseline_pp`, `protonet`,\n",
    "  `matchingnet`, `maml`)\n",
    "- `use_sot`: Whether to include the SOT module (`True`, `False`)\n",
    "- `n_way`: The number of classes in each episode\n",
    "- `n_shot`: The number of support samples per class in each episode\n",
    "\n",
    "For each experiment setting, there are multiple trained models because of\n",
    "hyper-parameter tuning. We will group the runs by the above parameters and only\n",
    "use the best-performing model on the validation set for the following analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:05:02.607240Z",
     "start_time": "2023-12-11T14:05:02.504502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "# Let's also save two separate dataframes for the two different datasets\n",
    "df_best_runs_tm = df_best_runs[df_best_runs[(\n",
    "    \"config\", \"dataset\")] == \"tabula_muris\"]\n",
    "df_best_runs_sp = df_best_runs[df_best_runs[(\n",
    "    \"config\", \"dataset\")] == \"swissprot\"]\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Performance for all models\n",
    "\n",
    "Here, we plot a simple bar plot for all methods (5 methods, each with and\n",
    "without SOT) on all three splits (train, val, test). Performances are shown in\n",
    "two separate plots for Swissprot and Tabula Muris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.025367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance by split for all methods\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 8))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "\n",
    "def pivot_acc(df):\n",
    "    tmp = []\n",
    "    for i, best_model in df.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            method_name = get_name(\n",
    "                best_model[(\"config\", \"method\")\n",
    "                           ], best_model[(\"config\", \"use_sot\")]\n",
    "            )\n",
    "            tmp.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"split\": split,\n",
    "                    \"acc\": best_model[(\"eval\", f\"{split}/acc\")],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_tm),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_sp),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[1],\n",
    ")\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"Split\")\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "# Set y-axis limits\n",
    "axs[0].set_ylim(0, 100)\n",
    "axs[1].set_ylim(0, 100)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Method\")\n",
    "    ax.set_ylabel(\"Acc. (%)\")\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-split-perf.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by method with and without SOT\n",
    "\n",
    "Here, we compare the performance of the different methods with and without SOT.\n",
    "The left subplot shows the test performance on the Tabula Muris dataset, while\n",
    "the right subplot shows the test performance on the Swissprot dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.026775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performance by method with and without SOT\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_tm,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    order=METHODS,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_sp,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    ci=\"sd\",\n",
    "    order=METHODS,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\", fontsize=14)\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\", fontsize=14)\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xticklabels([get_name(name.get_text()) for name in axs[0].get_xticklabels()])\n",
    "axs[1].set_xticklabels([get_name(name.get_text()) for name in axs[1].get_xticklabels()])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(\"Test Acc. (%)\", fontsize=18)\n",
    "    ax.set_xlabel(\"Method\", fontsize=18)\n",
    "\n",
    "# Remove legend in left plot\n",
    "axs[0].get_legend().remove()\n",
    "\n",
    "# Increasing tick size\n",
    "axs[0].tick_params(labelsize=14)\n",
    "axs[1].tick_params(labelsize=14)\n",
    "\n",
    "axs[0].set_ylim(0, 100)\n",
    "axs[1].set_ylim(0, 100)\n",
    "\n",
    "# Adding padding between ticks and labels\n",
    "axs[0].xaxis.labelpad = 10\n",
    "axs[1].xaxis.labelpad = 10\n",
    "\n",
    "# Add percentage to each bar\n",
    "for p in axs[0].patches:\n",
    "    axs[0].annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "# Add percentage to each bar\n",
    "for p in axs[1].patches:\n",
    "    axs[1].annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "# Add percentage to each bar\n",
    "errors = df_best_runs_sp[(\"eval\", \"test/acc_ci\")]\n",
    "df_best_runs_sp[\"method_order\"] = pd.Categorical(\n",
    "    df_best_runs_sp[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    ")\n",
    "df_best_runs_tm[\"method_order\"] = pd.Categorical(\n",
    "    df_best_runs_sp[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    ")\n",
    "\n",
    "# Sort by the custom order\n",
    "sp_errors = df_best_runs_sp.sort_values([\"method_order\", (\"config\", \"use_sot\")])[\n",
    "    (\"eval\", \"test/acc_ci\")\n",
    "]\n",
    "tm_errors = df_best_runs_tm.sort_values([\"method_order\", (\"config\", \"use_sot\")])[\n",
    "    (\"eval\", \"test/acc_ci\")\n",
    "]\n",
    "\n",
    "for i, p in enumerate(axs[0].patches):\n",
    "    axs[0].errorbar(\n",
    "        p.get_x() + p.get_width() / 2.0,\n",
    "        p.get_height(),\n",
    "        yerr=sp_errors[i],\n",
    "        fmt=\"none\",\n",
    "        color=\"black\",\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "for i, p in enumerate(axs[1].patches):\n",
    "    axs[1].errorbar(\n",
    "        p.get_x() + p.get_width() / 2.0,\n",
    "        p.get_height(),\n",
    "        yerr=tm_errors[i],\n",
    "        fmt=\"none\",\n",
    "        color=\"black\",\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-perf.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_runs_sp[\"method_order\"] = pd.Categorical(\n",
    "    df_best_runs_sp[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    ")\n",
    "\n",
    "# Sort by the custom order\n",
    "df_best_runs_sp.sort_values([\"method_order\", (\"config\", \"use_sot\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Ablation for models with SOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cols = df_runs[(\"hyperparam\")].columns.tolist()\n",
    "param_tuples = [(\"hyperparam\", param) for param in param_cols]\n",
    "param_tuples.append((\"config\", \"method\"))\n",
    "param_tuples.append((\"config\", \"use_sot\"))\n",
    "param_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.grid(\n",
    "    df_runs, param_tuples, metric=\"median\", cmap=\"YlGn\"\n",
    ")  # vmin, vmax automatically set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LaTeX table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the results for both the report\n",
    "tm_res = utils.exp2results(df_best_runs_tm)\n",
    "sp_res = utils.exp2results(df_best_runs_sp)\n",
    "\n",
    "# Create a MultiIndex for the results where the first level is the dataset\n",
    "tm_res = tm_res.set_index(\"Method\")\n",
    "tm_res.index = pd.MultiIndex.from_product([[\"TM\"], tm_res.index])\n",
    "sp_res = sp_res.set_index(\"Method\")\n",
    "sp_res.index = pd.MultiIndex.from_product([[\"SP\"], sp_res.index])\n",
    "\n",
    "# Concatenate the results\n",
    "df_results = pd.concat([tm_res, sp_res])\n",
    "\n",
    "# Get the latex\n",
    "latex = utils.exp2latex(df_results)\n",
    "\n",
    "# Save the latex\n",
    "with open(os.path.join(TABLE_DIR, \"tuned-benchmark.tex\"), \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Way-Shot Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Varying the number of shots per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.028317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load experiment data for `way-shot` experiment\n",
    "GROUP = \"new-way-shot\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.029875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the best run for each experiment configuration. This only has an\n",
    "effect if hyperparameter tuning was performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T14:05:03.032924Z",
     "start_time": "2023-12-11T14:05:03.031549Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot-Way-Analysis\n",
    "\n",
    "Display the test/acc as a function of the number of shots per class and the\n",
    "number of classes to distinguish between the different methods for ProtoNet\n",
    "without and with SOT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.033046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set context to paper\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Plot test/acc vs. n_shot for SOT and non-SOT methods\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(7, 3.5), sharey=True)\n",
    "\n",
    "# test/acc ~ n_shot\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    legend=True,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# test/acc ~ n_way\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set label size\n",
    "labsize = 14\n",
    "\n",
    "# Set title of the legend, adjust font size\n",
    "axs[0].get_legend().set_title(\"SOT\", prop={\"size\": 1})\n",
    "for t in axs[0].get_legend().texts:\n",
    "    t.set_fontsize(12)\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xlabel(\"N-Way\", size=labsize)\n",
    "axs[1].set_xlabel(\"N-Shot\", size=labsize)\n",
    "\n",
    "# Add padding between label of x-axis and tick labels\n",
    "axs[0].xaxis.labelpad = 10\n",
    "axs[1].xaxis.labelpad = 10\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_ylabel(\"Test. Acc. (%)\", size=labsize)\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "# Make the ticks descroptors bigger\n",
    "axs[0].tick_params(labelsize=12)\n",
    "axs[1].tick_params(labelsize=12)\n",
    "\n",
    "# Set the tight layout\n",
    "fig.tight_layout(pad=1.0)\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"way-shot.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Understanding model performance\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this section will be to understand the behaviour of the SOT module.\n",
    "We will try to understand the improvements by looking at:\n",
    "\n",
    "- **Embeddings during forward-pass**. Visualise the embeddings of support and\n",
    "  query samples during episodes with and without SOT enabled.\n",
    "- **Visualise the self-optimal transport plan.** Visualise the self-optimal\n",
    "  transport plan for a few episodes via a heat map.\n",
    "- **Understand model prediction patterns and errors.** Visualise the model\n",
    "  predictions and errors for a few episodes with and without SOT enabled.\n",
    "\n",
    "To get started, we will load two pre-trained models from the benchmarking\n",
    "experiment. We will use two instances of `protonet` that were both trained on\n",
    "the `tabula_muris` dataset. The first model was trained with the default\n",
    "configuration, while the second model was trained with the same configuration\n",
    "but with the `use_sot` flag set to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.041139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiments\n",
    "GROUP = \"model-behaviour\"\n",
    "\n",
    "# Filter runs by group\n",
    "runs = list(\n",
    "    reversed([run for run in runs if run.group == GROUP and run.state == \"finished\"])\n",
    ")\n",
    "print(f\"✅ Loaded {len(runs)} runs\")\n",
    "\n",
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(runs)\n",
    "df_runs = df_runs.iloc[::-1]\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.042125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise data loaders and model\n",
    "models = []\n",
    "for run in runs:\n",
    "    # Load data loaders and model\n",
    "    train_loader, val_loader, test_loader, model = utils.init_all(run)\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"✅ Initialised data loader and model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.043069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download artifact (model weights)\n",
    "for run in runs:\n",
    "    if not os.path.exists(os.path.join(ARTIFACT_DIR, run.id)):\n",
    "        utils.download_artifact(\n",
    "            api,\n",
    "            wandb_entity=WANDB_ENTITY,\n",
    "            wandb_project=WANDB_PROJECT,\n",
    "            artifact_dir=ARTIFACT_DIR,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "    print(f\"✅ Downloaded artifact for run {run.id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.044010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[0].id, \"best_model.pt\")\n",
    "models[0].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[1].id, \"best_model.pt\")\n",
    "models[1].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[2].id, \"best_model.pt\")\n",
    "models[2].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[3].id, \"best_model.pt\")\n",
    "models[3].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "print(f\"✅ Loaded all model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.045066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"\\nEvaluating ProtoNet w/o SOT...\")\n",
    "models[0].test_loop(train_loader)\n",
    "models[0].test_loop(val_loader)\n",
    "models[0].test_loop(test_loader)\n",
    "\n",
    "print(\"Evaluating ProtoNet w SOT...\")\n",
    "models[1].test_loop(train_loader)\n",
    "models[1].test_loop(val_loader)\n",
    "models[1].test_loop(test_loader)\n",
    "\n",
    "print(\"\\nEvaluating Baseline w/ SOT...\")\n",
    "models[2].test_loop(train_loader)\n",
    "models[2].test_loop(val_loader)\n",
    "models[2].test_loop(test_loader)\n",
    "\n",
    "print(\"\\nEvaluating ProtoNet w/ SOT w/o SE...\")\n",
    "models[3].test_loop(train_loader)\n",
    "models[3].test_loop(val_loader)\n",
    "models[3].test_loop(test_loader)\n",
    "\n",
    "print(f\"✅ Evaluated both models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly loaded the model weights by confirming the performance on the\n",
    "`train`, `val` and `test` split for ProtoNet on SwissProt with and without SOT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.045954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/ SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(train_loader, models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.046829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/o SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(train_loader, models[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise SOT transport plan\n",
    "\n",
    "Here we visualise the self-optimal transport plan for a few episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.047715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise transport plan for Protonet w/ SOT on splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[1], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan for ProtoNet w/ SOT w/o SE on splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[3], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan for Baseline w/ SOT on splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[2], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transport plan of ProtoNet w/ and w/o SE and w/ SOT\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(16, 5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "\n",
    "utils.visualise_transport_plan(x, models[1], ax=axs[0])\n",
    "utils.visualise_lstm(x, models[1], ax=axs[1])\n",
    "utils.visualise_transport_plan(x, models[3], ax=axs[2])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transport plan of ProtoNet with Baseline on test split\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "utils.visualise_transport_plan(x, models[1], ax=axs[0])\n",
    "utils.visualise_transport_plan(x, models[2], ax=axs[1])\n",
    "\n",
    "# Remove heatbar from axs[0]\n",
    "bar = axs[0].collections[0].colorbar\n",
    "axs[0].collections[0].colorbar.remove()\n",
    "axs[1].collections[0].colorbar.remove()\n",
    "pn_acc = axs[0].title.get_text().split(\" \")[-1]\n",
    "b_acc = axs[1].title.get_text().split(\" \")[-1]\n",
    "axs[0].set_title(f\"ProtoNet SOT Embeddings (Acc. = {pn_acc}\", fontsize=14)\n",
    "axs[1].set_title(f\"Baseline SOT Embeddings (Acc. = {b_acc}\", fontsize=14)\n",
    "\n",
    "# Add colbar in between the two plots\n",
    "# axs[0].figure.colorbar(bar, ax=axs, location=\"right\")p\n",
    "cbar_ax = fig.add_axes([0.485, 0.25, 0.02, 0.5])\n",
    "fig.colorbar(axs[0].collections[0], cax=cbar_ax, orientation=\"vertical\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"sot-embeddings.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise confusion patterns\n",
    "\n",
    "p fig.subplots_adjust(bottom=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.048603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/ SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:05:03.049573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/o SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[1], ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
