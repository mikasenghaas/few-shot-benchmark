{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook contains the analysis of the data tracked on\n",
    "[Weights & Biases](https://wandb.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "We will first setup everything so that we can easily analyse the experiment\n",
    "results. This includes importing the necessary libraries, setting paths, loading\n",
    "the experiment results from W&B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Bult-in modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# External modules\n",
    "# - Data Representation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# - Data Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotnine as pn\n",
    "\n",
    "# - Machine Learning\n",
    "import torch\n",
    "\n",
    "# - Experiment Configuration and Logging\n",
    "import wandb\n",
    "\n",
    "# Custom modules\n",
    "from utils import eval_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of global variables\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"report\", \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"report\", \"tables\")\n",
    "\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "METHODS = [\"baseline\", \"baseline_pp\", \"matchingnet\", \"protonet\", \"maml\"]\n",
    "METHODS_WITH_SOT = []\n",
    "for method in METHODS:\n",
    "    METHODS_WITH_SOT.append(method)\n",
    "    METHODS_WITH_SOT.append(method + \"_sot\")\n",
    "\n",
    "STYLED_METHODS = [\"Baseline\", \"Baseline++\", \"MatchingNet\", \"ProtoNet\", \"MAML\"]\n",
    "STYLED_METHODS_WITH_SOT = []\n",
    "for method in STYLED_METHODS:\n",
    "    STYLED_METHODS_WITH_SOT.append(method)\n",
    "    STYLED_METHODS_WITH_SOT.append(method + \" (SOT)\")\n",
    "\n",
    "styled_methods_dict = dict(zip(METHODS, STYLED_METHODS))\n",
    "\n",
    "\n",
    "def get_name(name, sot=False):\n",
    "    return styled_methods_dict[name] + (\" (SOT)\" if sot else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "REPORT = True\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"YlGn_r\" if REPORT else \"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"few-shot-benchmark\"\n",
    "WANDB_ENTITY = \"metameta-learners\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Benchmark\n",
    "\n",
    "---\n",
    "\n",
    "All models on all datasets with and without SOT for fixed few-shot learning\n",
    "setting (5-way 5-shot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Experiment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs for experiment `benchmark`\n",
    "GROUP = \"new-tuned-benchmark\"\n",
    "\n",
    "exp1_runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(exp1_runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load all runs from the given experiment group into a single\n",
    "dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp1 = utils.load_to_df(exp1_runs)\n",
    "print(f\"✅ Loaded {len(df_exp1)} runs.\")\n",
    "\n",
    "df_exp1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the number of runs for each unique experiment to verify that we have\n",
    "the same number of runs for each experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of runs per experiment\n",
    "experiment_config = [(\"config\", c) for c in df_exp1.config.columns]\n",
    "df_exp1.groupby(experiment_config).size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Each experiment is uniquely identified by the following parameters:\n",
    "\n",
    "- `dataset`: The dataset used (`swissprot`, `tabula_muris`)\n",
    "- `method`: The model used (`baseline`, `baseline_pp`, `protonet`,\n",
    "  `matchingnet`, `maml`)\n",
    "- `use_sot`: Whether to include the SOT module (`True`, `False`)\n",
    "- `n_way`: The number of classes in each episode\n",
    "- `n_shot`: The number of support samples per class in each episode\n",
    "\n",
    "For each experiment setting, there are multiple trained models because of\n",
    "hyper-parameter tuning. We will group the runs by the above parameters and only\n",
    "use the best-performing model on the validation set for the following analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_exp1_best = utils.get_best_run(df_exp1, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_exp1_best)} best runs.\")\n",
    "\n",
    "# Let's also save two separate dataframes for the two different datasets\n",
    "df_exp1_best_tm = df_exp1_best[df_exp1_best[(\"config\", \"dataset\")] == \"tabula_muris\"]\n",
    "df_exp1_best_sp = df_exp1_best[df_exp1_best[(\"config\", \"dataset\")] == \"swissprot\"]\n",
    "\n",
    "df_exp1_best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Performance for all models\n",
    "\n",
    "Here, we plot a simple bar plot for all methods (5 methods, each with and\n",
    "without SOT) on all three splits (train, val, test). Performances are shown in\n",
    "two separate plots for Swissprot and Tabula Muris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by split for all methods\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 8))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "\n",
    "def pivot_acc(df):\n",
    "    tmp = []\n",
    "    for i, best_model in df.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            method_name = get_name(\n",
    "                best_model[(\"config\", \"method\")], best_model[(\"config\", \"use_sot\")]\n",
    "            )\n",
    "            tmp.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"split\": split,\n",
    "                    \"acc\": best_model[(\"eval\", f\"{split}/acc\")],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    pivot_acc(df_exp1_best_tm),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.barplot(\n",
    "    pivot_acc(df_exp1_best_sp),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[1],\n",
    ")\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"Split\")\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "# Set y-axis limits\n",
    "axs[0].set_ylim(0, 100)\n",
    "axs[1].set_ylim(0, 100)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Method\")\n",
    "    ax.set_ylabel(\"Acc. (%)\")\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-split-perf.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by method with and without SOT\n",
    "\n",
    "Here, we compare the performance of the different methods with and without SOT.\n",
    "The left subplot shows the test performance on the Tabula Muris dataset, while\n",
    "the right subplot shows the test performance on the Swissprot dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by method with and without SOT\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    df_exp1_best_tm,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    order=METHODS,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    df_exp1_best_sp,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    ci=\"sd\",\n",
    "    order=METHODS,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\", fontsize=14)\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\", fontsize=14)\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xticklabels([get_name(name.get_text())\n",
    "                       for name in axs[0].get_xticklabels()])\n",
    "axs[1].set_xticklabels([get_name(name.get_text())\n",
    "                       for name in axs[1].get_xticklabels()])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(\"Test Acc. (%)\", fontsize=18)\n",
    "    ax.set_xlabel(\"Method\", fontsize=18)\n",
    "\n",
    "# Remove legend in left plot\n",
    "axs[0].get_legend().remove()\n",
    "\n",
    "# Increasing tick size\n",
    "axs[0].tick_params(labelsize=14)\n",
    "axs[1].tick_params(labelsize=14)\n",
    "\n",
    "axs[0].set_ylim(0, 105)\n",
    "axs[1].set_ylim(0, 105)\n",
    "\n",
    "# Adding padding between ticks and labels\n",
    "axs[0].xaxis.labelpad = 10\n",
    "axs[1].xaxis.labelpad = 10\n",
    "\n",
    "# Add percentage to each bar\n",
    "for p in axs[0].patches:\n",
    "    axs[0].annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "# Add percentage to each bar\n",
    "for p in axs[1].patches:\n",
    "    axs[1].annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "# Add percentage to each bar\n",
    "errors = df_exp1_best_sp[(\"eval\", \"test/acc_ci\")]\n",
    "df_exp1_best_sp[\"method_order\"] = pd.Categorical(\n",
    "    df_exp1_best_sp[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    ")\n",
    "df_exp1_best_tm[\"method_order\"] = pd.Categorical(\n",
    "    df_exp1_best_tm[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    ")\n",
    "\n",
    "# Sort by the custom order\n",
    "sp_errors = df_exp1_best_sp.sort_values([\"method_order\", (\"config\", \"use_sot\")])[\n",
    "    (\"eval\", \"test/acc_ci\")\n",
    "]\n",
    "tm_errors = df_exp1_best_tm.sort_values([\"method_order\", (\"config\", \"use_sot\")])[\n",
    "    (\"eval\", \"test/acc_ci\")\n",
    "]\n",
    "\n",
    "for i, p in enumerate(axs[0].patches):\n",
    "    axs[0].errorbar(\n",
    "        p.get_x() + p.get_width() / 2.0,\n",
    "        p.get_height(),\n",
    "        yerr=sp_errors[i],\n",
    "        fmt=\"none\",\n",
    "        color=\"black\",\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "for i, p in enumerate(axs[1].patches):\n",
    "    axs[1].errorbar(\n",
    "        p.get_x() + p.get_width() / 2.0,\n",
    "        p.get_height(),\n",
    "        yerr=tm_errors[i],\n",
    "        fmt=\"none\",\n",
    "        color=\"black\",\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "# Save figure\n",
    "name = \"benchmark-method-perf\" if REPORT else \"present-benchmark-method-perf.png\"\n",
    "fig.savefig(os.path.join(FIGURE_DIR, name), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_best_runs_sp[\"method_order\"] = pd.Categorical(\n",
    "#     df_best_runs_sp[(\"config\", \"method\")], categories=METHODS, ordered=True\n",
    "# )\n",
    "# \n",
    "# # Sort by the custom order\n",
    "# df_best_runs_sp.sort_values([\"method_order\", (\"config\", \"use_sot\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Ablation\n",
    "\n",
    "For each method including the SOT module we are searching over the following\n",
    "hyperparameters:\n",
    "\n",
    "- `lr`: The learning rate of the optimizer (`0.001`, `0.01`, `0.1`)\n",
    "- `hidden_dim`: The hidden dimension of the backbone (`64`, `512`, `1024`)\n",
    "- `sot_reg`: Regularization strength for the SOT module (`0.01`, `0.1`, `1.0`)\n",
    "- `sot_dist`: The distance function used for the SOT module (`euclidean`,\n",
    "  `cosine`)\n",
    "\n",
    "This leads to a total of $3 \\times 3 \\times 3 \\times 2 = 54$ different tuning\n",
    "runs for each model. Let's first verify that we have all runs necessary and then\n",
    "analyse the effect of the hyperparameters on the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best hyper-parameters for each method\n",
    "df_exp1_best[[\"config\", \"hparams\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the following:\n",
    "\n",
    "- Using `cosine` distance seems to be better across all methods and datasets.\n",
    "- A regularisation parameter during the SOT phase of `0.1` is preferred across\n",
    "  almost all methods and datasets.\n",
    "- A low learning rate is generally preferred `0.001`\n",
    "- Feature dimensions vary across methods and datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualise the effect of each hyperparameter individually for both\n",
    "datasets separately. We are averaging the results of all tuning runs for each\n",
    "hyperparameter value and method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hparams = len(df_exp1.hparams.columns)\n",
    "\n",
    "# Hyperparameter Effect on SwissProt for all methods\n",
    "fig, axs = plt.subplots(ncols=num_hparams, figsize=(20, 4))\n",
    "fig.suptitle(\"Effect of Hyperparameters on SwissProt\", fontsize=16)\n",
    "for ax, hparam in zip(axs, df_exp1.hparams.columns):\n",
    "    utils.visualise_hparams(df_exp1, dataset=\"swissprot\", hparam=hparam, ax=ax)\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"hparams-swissprot.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")\n",
    "\n",
    "# Hyperparameter Effect on Tabula Muris for all methods\n",
    "fig, axs = plt.subplots(ncols=num_hparams, figsize=(20, 4))\n",
    "fig.suptitle(\"Effect of Hyperparameters on Tabula Muris\", fontsize=16)\n",
    "for ax, hparam in zip(axs, df_exp1.hparams.columns):\n",
    "    utils.visualise_hparams(\n",
    "        df_exp1, dataset=\"tabula_muris\", hparam=hparam, ax=ax)\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"hparams-tabula-muris.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the interaction of all hyper-parameters pairs and the method and\n",
    "dataset performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters to visualise\n",
    "param_cols = df_exp1[(\"hparams\")].columns.tolist()\n",
    "param_tuples = [(\"hparams\", param) for param in param_cols]\n",
    "param_tuples.append((\"config\", \"method\"))\n",
    "# param_tuples.append((\"config\", \"dataset\"))\n",
    "param_tuples\n",
    "subset = df_exp1[\n",
    "    df_exp1[(\"config\", \"use_sot\")] \n",
    "    & df_exp1[(\"config\",\"dataset\")].isin([\"swissprot\"]) \n",
    "    # & (df_exp1[(\"eval\", \"test/acc\")] >= 25.0)\n",
    "]\n",
    "\n",
    "rename_dict = {\n",
    "    \"lr\": \"LR\",\n",
    "    \"feat_dim\": \"Backbone\\nDimension\",\n",
    "    \"sot_dist_metric\": \"SOT\\nDistance\\nMetric\",\n",
    "    \"use_sot\": \"SOT\",\n",
    "    \"sot_reg\": \"SOT\\nReg.\",\n",
    "    \"method\": \"Method\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "}\n",
    "\n",
    "fig = utils.grid(\n",
    "    subset, param_tuples, metric=\"mean\", cmap=\"YlGn\", figsize=(10, 10), rename_dict=rename_dict\n",
    ")  # vmin, vmax automatically set\n",
    "\n",
    "fig.savefig(\n",
    "    os.path.join(FIGURE_DIR, \"hparams-interaction-swissprot.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters to visualise\n",
    "param_cols = df_exp1[(\"hparams\")].columns.tolist()\n",
    "param_tuples = [(\"hparams\", param) for param in param_cols]\n",
    "param_tuples.append((\"config\", \"method\"))\n",
    "# param_tuples.append((\"config\", \"dataset\"))\n",
    "param_tuples\n",
    "subset = df_exp1[\n",
    "    df_exp1[(\"config\", \"use_sot\")]\n",
    "    & df_exp1[(\"config\", \"dataset\")].isin([\"tabula_muris\"])\n",
    "    & (df_exp1[(\"eval\", \"test/acc\")] >= 25.0)\n",
    "]\n",
    "fig = utils.grid(\n",
    "    subset, param_tuples, metric=\"mean\", cmap=\"YlGn\", figsize=(10, 10)\n",
    ")  # vmin, vmax automatically set\n",
    "\n",
    "fig.savefig(\n",
    "    os.path.join(FIGURE_DIR, \"hparams-interaction-tabula_muris.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combined grid\n",
    "param_cols = df_exp1[(\"hparams\")].columns.tolist()\n",
    "param_tuples = [(\"hparams\", param) for param in param_cols]\n",
    "param_tuples.append((\"config\", \"method\"))\n",
    "# param_tuples.append((\"config\", \"dataset\"))\n",
    "\n",
    "rename_dict = {\n",
    "    \"lr\": \"Learning\\nRate\",\n",
    "    \"feat_dim\": \"Backbone\\nDimension\",\n",
    "    \"sot_dist_metric\": \"SOT\\nDistance\\nMetric\",\n",
    "    \"sot_reg\": \"SOT\\nReg.\",\n",
    "    \"method\": \"Method\",\n",
    "    \"dataset\": \"Dataset\",\n",
    "    \"baseline\":\"B\",\n",
    "    \"baseline_pp\":\"B++\",\n",
    "    \"matchingnet\":\"MN\",\n",
    "    \"protonet\":\"PN\",\n",
    "    \"maml\":\"MAML\",\n",
    "    \"euclidean\":\"EUCL\",\n",
    "    \"cosine\":\"COS\",\n",
    "}\n",
    "\n",
    "subset1 = df_exp1[\n",
    "    df_exp1[(\"config\", \"use_sot\")]\n",
    "    & df_exp1[(\"config\", \"dataset\")].isin([\"tabula_muris\"])\n",
    "    & (df_exp1[(\"eval\", \"test/acc\")] >= 25.0)\n",
    "    ]\n",
    "\n",
    "subset2 = df_exp1[\n",
    "    df_exp1[(\"config\",\"use_sot\")]\n",
    "    & df_exp1[(\"config\",\"dataset\")].isin([\"swissprot\"])\n",
    "    & (df_exp1[(\"eval\",\"test/acc\")] >= 25.0)\n",
    "]\n",
    "\n",
    "fig = utils.combined_grid(\n",
    "    subset1, subset2, param_tuples, metric=\"mean\", cmap=\"YlGn\", figsize=(10, 10), rename_dict=rename_dict\n",
    ")  # vmin, vmax automatically set\n",
    "\n",
    "fig.savefig(\n",
    "    os.path.join(FIGURE_DIR, \"hparams-interaction-combined.png\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LaTeX table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the results for both the report\n",
    "tm_res = utils.exp2results(df_exp1_best_tm)\n",
    "sp_res = utils.exp2results(df_exp1_best_sp)\n",
    "\n",
    "# Create a MultiIndex for the results where the first level is the dataset\n",
    "tm_res = tm_res.set_index(\"Method\")\n",
    "tm_res.index = pd.MultiIndex.from_product([[\"TM\"], tm_res.index])\n",
    "sp_res = sp_res.set_index(\"Method\")\n",
    "sp_res.index = pd.MultiIndex.from_product([[\"SP\"], sp_res.index])\n",
    "\n",
    "# Concatenate the results\n",
    "df_results = pd.concat([tm_res, sp_res])\n",
    "\n",
    "# Get the latex\n",
    "latex = utils.exp2latex(df_results)\n",
    "\n",
    "# Save the latex\n",
    "with open(os.path.join(TABLE_DIR, \"tuned-benchmark.tex\"), \"w\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Way-Shot Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Varying the number of shots per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data for `way-shot` experiment\n",
    "GROUP = \"new-way-shot\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = [run for run in runs if run.group ==\n",
    "              GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the best run for each experiment configuration. This only has an\n",
    "effect if hyperparameter tuning was performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot-Way-Analysis\n",
    "\n",
    "Display the test/acc as a function of the number of shots per class and the\n",
    "number of classes to distinguish between the different methods for ProtoNet\n",
    "without and with SOT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set context to paper\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Plot test/acc vs. n_shot for SOT and non-SOT methods\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(7, 3.5), sharey=True)\n",
    "\n",
    "# test/acc ~ n_shot\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    legend=True,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# test/acc ~ n_way\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set label size\n",
    "labsize = 14\n",
    "\n",
    "# Set title of the legend, adjust font size\n",
    "axs[0].get_legend().set_title(\"SOT\", prop={\"size\": 1})\n",
    "for t in axs[0].get_legend().texts:\n",
    "    t.set_fontsize(12)\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xlabel(\"N-Way\", size=labsize)\n",
    "axs[1].set_xlabel(\"N-Shot\", size=labsize)\n",
    "\n",
    "# Add padding between label of x-axis and tick labels\n",
    "axs[0].xaxis.labelpad = 10\n",
    "axs[1].xaxis.labelpad = 10\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_ylabel(\"Test. Acc. (%)\", size=labsize)\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "# Make the ticks descroptors bigger\n",
    "axs[0].tick_params(labelsize=12)\n",
    "axs[1].tick_params(labelsize=12)\n",
    "\n",
    "# Set the tight layout\n",
    "fig.tight_layout(pad=1.0)\n",
    "\n",
    "# Save figure\n",
    "name = \"way-shot\" if REPORT else \"present-way-shot.png\"\n",
    "fig.savefig(os.path.join(FIGURE_DIR, name), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe with n_way and n_shot, and the test accuracy\n",
    "way_shot = df_best_runs[\n",
    "    [\n",
    "        (\"config\", \"n_way\"),\n",
    "        (\"config\", \"n_shot\"),\n",
    "        (\"eval\", \"test/acc\"),\n",
    "        (\"config\", \"use_sot\"),\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Flatten the multi-index\n",
    "way_shot.columns = way_shot.columns.get_level_values(1)\n",
    "\n",
    "# Split the dataframe into SOT and non-SOT\n",
    "way_shot_sot = way_shot[way_shot[\"use_sot\"]].drop(columns=[\"use_sot\"])\n",
    "way_shot = way_shot[~way_shot[\"use_sot\"]].drop(columns=[\"use_sot\"])\n",
    "\n",
    "# Group by n_way and n_shot\n",
    "way_shot = way_shot.groupby([\"n_way\", \"n_shot\"]).mean().reset_index()\n",
    "way_shot_sot = way_shot_sot.groupby([\"n_way\", \"n_shot\"]).mean().reset_index()\n",
    "\n",
    "# Pivot the dataframe\n",
    "way_shot = way_shot.pivot(index=\"n_way\", columns=\"n_shot\", values=\"test/acc\")\n",
    "way_shot_sot = way_shot_sot.pivot(\n",
    "    index=\"n_way\", columns=\"n_shot\", values=\"test/acc\")\n",
    "\n",
    "# Compute the diff\n",
    "way_shot_diff = way_shot_sot - way_shot\n",
    "\n",
    "# Setup the figure\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(\n",
    "    way_shot_sot,\n",
    "    annot=True,\n",
    "    cmap=\"YlGn\",\n",
    "    vmin=50,\n",
    "    vmax=100,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(\n",
    "    way_shot_diff,\n",
    "    annot=True,\n",
    "    cmap=\"RdBu_r\",\n",
    "    ax=axs[1],\n",
    "    center=0,\n",
    "    cbar_kws={\"label\": \"Accuracy Difference (%)\"},\n",
    ")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"N-Shot\")\n",
    "    ax.set_ylabel(\"N-Way\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "name = \"way-shot-heatmap\" if REPORT else \"present-way-shot-heatmap.png\"\n",
    "fig.savefig(os.path.join(FIGURE_DIR, name), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run the rest of the notebook\n",
    "raise ValueError(\"Don't run the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Understanding model performance\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this section will be to understand the behaviour of the SOT module.\n",
    "We will try to understand the improvements by looking at:\n",
    "\n",
    "- **Embeddings during forward-pass**. Visualise the embeddings of support and\n",
    "  query samples during episodes with and without SOT enabled.\n",
    "- **Visualise the self-optimal transport plan.** Visualise the self-optimal\n",
    "  transport plan for a few episodes via a heat map.\n",
    "- **Understand model prediction patterns and errors.** Visualise the model\n",
    "  predictions and errors for a few episodes with and without SOT enabled.\n",
    "\n",
    "To get started, we will load two pre-trained models from the benchmarking\n",
    "experiment. We will use two instances of `protonet` that were both trained on\n",
    "the `tabula_muris` dataset. The first model was trained with the default\n",
    "configuration, while the second model was trained with the same configuration\n",
    "but with the `use_sot` flag set to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "GROUP = \"model-behaviour\"\n",
    "\n",
    "# Filter runs by group\n",
    "runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Loaded {len(runs)} runs\")\n",
    "\n",
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(runs)\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise data loaders and model\n",
    "models = []\n",
    "for run in runs:\n",
    "    # Load data loaders and model\n",
    "    train_loader, val_loader, test_loader, model = utils.init_all(run)\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"✅ Initialised data loader and model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifact (model weights)\n",
    "for run in runs:\n",
    "    if not os.path.exists(os.path.join(ARTIFACT_DIR, run.id)):\n",
    "        utils.download_artifact(\n",
    "            api,\n",
    "            wandb_entity=WANDB_ENTITY,\n",
    "            wandb_project=WANDB_PROJECT,\n",
    "            artifact_dir=ARTIFACT_DIR,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "    print(f\"✅ Downloaded artifact for run {run.id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[0].id, \"best_model.pt\")\n",
    "models[0].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[1].id, \"best_model.pt\")\n",
    "models[1].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "print(f\"✅ Loaded all model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"\\nEvaluating X...\")\n",
    "models[0].test_loop(train_loader)\n",
    "models[0].test_loop(val_loader)\n",
    "models[0].test_loop(test_loader)\n",
    "\n",
    "print(\"Evaluating Y...\")\n",
    "models[1].test_loop(train_loader)\n",
    "models[1].test_loop(val_loader)\n",
    "models[1].test_loop(test_loader)\n",
    "\n",
    "\n",
    "print(f\"✅ Evaluated both models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly loaded the model weights by confirming the performance on the\n",
    "`train`, `val` and `test` split for ProtoNet on SwissProt with and without SOT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/ SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(loader, models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/o SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(loader, models[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise SOT transport plan\n",
    "\n",
    "Here we visualise the self-optimal transport plan for a few episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan of X\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan for ProtoNet w/ SOT w/o SE on splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[1], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan for Baseline w/ SOT on splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    x, _ = next(iter(loader))\n",
    "    utils.visualise_transport_plan(x, models[2], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transport plan of ProtoNet w/ and w/o SE and w/ SOT\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(16, 5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "\n",
    "utils.visualise_transport_plan(x, models[0], ax=axs[0])\n",
    "utils.visualise_transport_plan(x, models[1], ax=axs[2])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transport plan of ProtoNet with Baseline on test split\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "x, _ = next(iter(test_loader))\n",
    "utils.visualise_transport_plan(x, models[1], ax=axs[0])\n",
    "utils.visualise_transport_plan(x, models[2], ax=axs[1])\n",
    "\n",
    "# Remove heatbar from axs[0]\n",
    "bar = axs[0].collections[0].colorbar\n",
    "axs[0].collections[0].colorbar.remove()\n",
    "axs[1].collections[0].colorbar.remove()\n",
    "pn_acc = axs[0].title.get_text().split(\" \")[-1]\n",
    "b_acc = axs[1].title.get_text().split(\" \")[-1]\n",
    "axs[0].set_title(f\"ProtoNet SOT Embeddings (Acc. = {pn_acc}\", fontsize=14)\n",
    "axs[1].set_title(f\"Baseline SOT Embeddings (Acc. = {b_acc}\", fontsize=14)\n",
    "\n",
    "# Add colbar in between the two plots\n",
    "# axs[0].figure.colorbar(bar, ax=axs, location=\"right\")p\n",
    "cbar_ax = fig.add_axes([0.485, 0.25, 0.02, 0.5])\n",
    "fig.colorbar(axs[0].collections[0], cax=cbar_ax, orientation=\"vertical\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"sot-embeddings.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise confusion patterns\n",
    "\n",
    "p fig.subplots_adjust(bottom=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/ SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/o SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[1], ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
