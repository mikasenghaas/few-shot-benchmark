{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook contains the analysis of the data tracked on\n",
    "[Weights & Biases](https://wandb.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "We will first setup everything so that we can easily analyse the experiment\n",
    "results. This includes importing the necessary libraries, setting paths, loading\n",
    "the experiment results from W&B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Bult-in modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# External modules\n",
    "# - Data Representation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# - Data Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotnine as pn\n",
    "\n",
    "# - Machine Learning\n",
    "import torch\n",
    "\n",
    "# - Experiment Configuration and Logging\n",
    "import wandb\n",
    "\n",
    "# Custom modules\n",
    "from utils import eval_utils as utils\n",
    "from utils import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of global variables\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"figures\")\n",
    "TABLE_DIR = os.path.join(ROOT_DIR, \"tables\")\n",
    "\n",
    "METHODS = [\"baseline\", \"baseline_pp\", \"matchingnet\", \"protonet\", \"maml\"]\n",
    "METHODS_WITH_SOT = []\n",
    "for method in METHODS:\n",
    "    METHODS_WITH_SOT.append(method)\n",
    "    METHODS_WITH_SOT.append(method + \"_sot\")\n",
    "\n",
    "STYLED_METHODS = [\"Baseline\", \"Baseline++\", \"MatchingNet\", \"ProtoNet\", \"MAML\"]\n",
    "STYLED_METHODS_WITH_SOT = []\n",
    "for method in STYLED_METHODS:\n",
    "    STYLED_METHODS_WITH_SOT.append(method)\n",
    "    STYLED_METHODS_WITH_SOT.append(method + \" (SOT)\")\n",
    "\n",
    "styled_methods_dict = dict(zip(METHODS, STYLED_METHODS))\n",
    "\n",
    "\n",
    "def get_name(name, sot=False):\n",
    "    return styled_methods_dict[name] + (\" (SOT)\" if sot else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "sns.set_style(\"dark\")\n",
    "colorstyle = \"RdBu\"\n",
    "sns.set_palette(colorstyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"few-shot-benchmark\"\n",
    "WANDB_ENTITY = \"metameta-learners\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Benchmark\n",
    "\n",
    "---\n",
    "\n",
    "All models on all datasets with and without SOT for fixed few-shot learning\n",
    "setting (5-way 5-shot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Experiment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs for experiment `benchmark`\n",
    "GROUP = \"benchmark\"\n",
    "\n",
    "group_runs = [run for run in runs if run.group ==\n",
    "              GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load all runs from the given experiment group into a single\n",
    "dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Each experiment is uniquely identified by the following parameters:\n",
    "\n",
    "- `dataset`: The dataset used (`swissprot`, `tabula_muris`)\n",
    "- `method`: The model used (`baseline`, `baseline_pp`, `protonet`,\n",
    "  `matchingnet`, `maml`)\n",
    "- `use_sot`: Whether to include the SOT module (`True`, `False`)\n",
    "- `n_way`: The number of classes in each episode\n",
    "- `n_shot`: The number of support samples per class in each episode\n",
    "\n",
    "For each experiment setting, there are multiple trained models because of\n",
    "hyper-parameter tuning. We will group the runs by the above parameters and only\n",
    "use the best-performing model on the validation set for the following analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "# Let's also save two separate dataframes for the two different datasets\n",
    "df_best_runs_tm = df_best_runs[df_best_runs[(\"config\", \"dataset\")] == \"tabula_muris\"]\n",
    "df_best_runs_sp = df_best_runs[df_best_runs[(\"config\", \"dataset\")] == \"swissprot\"]\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the results for both the report\n",
    "tm_res = utils.exp2results(df_best_runs_tm)\n",
    "sp_res = utils.exp2results(df_best_runs_sp)\n",
    "\n",
    "# Create a MultiIndex for the results where the first level is the dataset\n",
    "tm_res = tm_res.set_index(\"Method\")\n",
    "tm_res.index = pd.MultiIndex.from_product([[\"Tabula Muris\"], tm_res.index])\n",
    "sp_res = sp_res.set_index(\"Method\")\n",
    "sp_res.index = pd.MultiIndex.from_product([[\"SwissProt\"], sp_res.index])\n",
    "\n",
    "# Concatenate the results\n",
    "df_results = pd.concat([tm_res, sp_res])\n",
    "\n",
    "# Get the latex\n",
    "latex = utils.exp2latex(df_results)\n",
    "\n",
    "# Save the latex\n",
    "with open(os.path.join(TABLE_DIR, \"results.tex\"), \"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Performance for all models\n",
    "\n",
    "Here, we plot a simple bar plot for all methods (5 methods, each with and\n",
    "without SOT) on all three splits (train, val, test). Performances are shown in\n",
    "two separate plots for Swissprot and Tabula Muris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by split for all methods\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 8))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "\n",
    "def pivot_acc(df):\n",
    "    tmp = []\n",
    "    for i, best_model in df.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            method_name = get_name(\n",
    "                best_model[(\"config\", \"method\")], best_model[(\"config\", \"use_sot\")]\n",
    "            )\n",
    "            tmp.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"split\": split,\n",
    "                    \"acc\": best_model[(\"eval\", f\"{split}/acc\")],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_tm),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_sp),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[1],\n",
    ")\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"Split\")\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Method\")\n",
    "    ax.set_ylabel(\"Acc. (%)\")\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-split-perf.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by method with and without SOT\n",
    "\n",
    "Here, we compare the performance of the different methods with and without SOT.\n",
    "The left subplot shows the test performance on the Tabula Muris dataset, while\n",
    "the right subplot shows the test performance on the Swissprot dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by method with and without SOT\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_tm,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    order=METHODS,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_sp,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    ci=\"sd\",\n",
    "    order=METHODS,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xticklabels([get_name(name.get_text()) for name in axs[0].get_xticklabels()])\n",
    "axs[1].set_xticklabels([get_name(name.get_text()) for name in axs[1].get_xticklabels()])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(\"Test Acc. (%)\")\n",
    "    ax.set_xlabel(\"Method\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-perf.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Way-Shot Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Varying the number of shots per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data for `way-shot` experiment\n",
    "GROUP = \"way-shot\"\n",
    "USER = \"mikasenghaas\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = [run for run in runs if run.group ==\n",
    "              GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the best run for each experiment configuration. This only has an\n",
    "effect if hyperparameter tuning was performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot-Way-Analysis\n",
    "\n",
    "Display the test/acc as a function of the number of shots per class and the\n",
    "number of classes to distinguish between the different methods for ProtoNet\n",
    "without and with SOT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test/acc vs. n_shot for SOT and non-SOT methods\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "# test/acc ~ n_shot\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# test/acc ~ n_way\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    ax=axs[1],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xlabel(\"N-Way\")\n",
    "axs[1].set_xlabel(\"N-Shot\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_ylabel(\"Val. Acc. (%)\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "# Set legend title\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"way-shot.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: SOT Interaction\n",
    "\n",
    "---\n",
    "\n",
    "We found that SOT interacts in an interesting manner with the re-embedding\n",
    "modules in distance-based few-shot learners like `ProtoNet` and `MatchingNet`.\n",
    "In this experiment, we investigate this interaction in more detail. To do this,\n",
    "we trained `MatchingNet` by enabling the SOT and LSTM embedding for support and\n",
    "query samples and observe the performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "GROUP = \"sot-interaction\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = {\n",
    "    run.id: run for run in runs if run.group == GROUP and run.state == \"finished\"\n",
    "}\n",
    "print(f\"✅ Loaded {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs.values())\n",
    "\n",
    "# Load additional information\n",
    "embed_support = [run.config[\"method\"][\"embed_support\"] for run in group_runs.values()]\n",
    "embed_query = [run.config[\"method\"][\"embed_query\"] for run in group_runs.values()]\n",
    "df_runs[(\"config\", \"embed_support\")] = embed_support\n",
    "df_runs[(\"config\", \"embed_query\")] = embed_query\n",
    "print(f\"✅ Added meta-information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test/acc vs. name\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "sns.barplot(\n",
    "    data=df_runs.sort_values(by=(\"eval\", \"test/acc\"), ascending=False),\n",
    "    x=(\"info\", \"name\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    palette=\"RdBu\",\n",
    ")\n",
    "ax.set_xlabel(\"Method\")\n",
    "ax.set_ylabel(\"Test Acc. (%)\")\n",
    "\n",
    "# Set axis labels\n",
    "labels = []\n",
    "for exp in [l.get_text().split(\"-\")[1::2] for l in ax.get_xticklabels()]:\n",
    "    name = []\n",
    "    if exp[0] == \"true\":\n",
    "        name.append(\"SOT\")\n",
    "    if exp[1] == \"true\":\n",
    "        name.append(\"SE\")\n",
    "    if exp[2] == \"true\":\n",
    "        name.append(\"QE\")\n",
    "\n",
    "    labels.append(\" + \".join(name))\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"sot-interaction.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of interaction between support, query and SOT embeddings\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(\n",
    "    df_runs,\n",
    "    x=(\"config\", \"embed_support\"),\n",
    "    y=(\"config\", \"embed_query\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    size=(\"eval\", \"test/acc\"),\n",
    "    sizes=(100, 5000),\n",
    "    linewidth=0,\n",
    ")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels([\"No\", \"Yes\"])\n",
    "ax.set_yticklabels([\"No\", \"Yes\"])\n",
    "ax.set(\n",
    "    xlabel=\"Embed Support\",\n",
    "    ylabel=\"Embed Query\",\n",
    ")\n",
    "ax.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interaction(row):\n",
    "    support, query, sot = (\n",
    "        row[(\"config\", \"embed_support\")],\n",
    "        row[(\"config\", \"embed_query\")],\n",
    "        row[(\"config\", \"use_sot\")],\n",
    "    )\n",
    "    if (support | query) & sot:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "df_runs[(\"config\", \"interaction\")] = df_runs.apply(is_interaction, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.barplot(\n",
    "    df_runs,\n",
    "    x=(\"config\", \"interaction\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    ax=ax,\n",
    ")\n",
    "ax.scatter(\n",
    "    x=df_runs[(\"config\", \"interaction\")],\n",
    "    y=df_runs[(\"eval\", \"test/acc\")],\n",
    "    c=\"black\",\n",
    "    alpha=1,\n",
    "    marker=\"x\",\n",
    "    zorder=100,\n",
    ")\n",
    "\n",
    "ax.set_xticklabels([\"No\", \"Yes\"])\n",
    "ax.set_xlabel(\"Interaction\")\n",
    "ax.set_ylabel(\"Test Acc. (%)\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"sot-interaction-2.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Understanding model performance\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this section will be to compare the improvements of the SOT feature\n",
    "transform. We will try to understand the improvements by looking at:\n",
    "\n",
    "- **Embeddings during forward-pass**. Visualise the embeddings of support and\n",
    "  query samples during episodes with and without SOT enabled.\n",
    "- **Visualise the self-optimal transport plan.** Visualise the self-optimal\n",
    "  transport plan for a few episodes via a heat map.\n",
    "- **Understand model prediction patterns and errors.** Visualise the model\n",
    "  predictions and errors for a few episodes with and without SOT enabled.\n",
    "\n",
    "To get started, we will load two pre-trained models from the benchmarking\n",
    "experiment. We will use two instances of `protonet` that were both trained on\n",
    "the `tabula_muris` dataset. The first model was trained with the default\n",
    "configuration, while the second model was trained with the same configuration\n",
    "but with the `use_sot` flag set to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "GROUP = \"model-behaviour\"\n",
    "\n",
    "# Filter runs by group\n",
    "runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Loaded {len(runs)} runs\")\n",
    "\n",
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(runs)\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise data loaders and model\n",
    "models = []\n",
    "for run in runs:\n",
    "    # Load data loaders and model\n",
    "    train_loader, val_loader, test_loader, model = utils.init_all(run)\n",
    "    models.append(model)\n",
    "\n",
    "print(f\"✅ Initialised data loader and model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifact (model weights)\n",
    "for run in runs:\n",
    "    utils.download_artifact(\n",
    "        api,\n",
    "        wandb_entity=WANDB_ENTITY,\n",
    "        wandb_project=WANDB_PROJECT,\n",
    "        artifact_dir=ARTIFACT_DIR,\n",
    "        run_id=run.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[0].id, \"best_model.pt\")\n",
    "models[0].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "weight_path = os.path.join(ARTIFACT_DIR, runs[1].id, \"best_model.pt\")\n",
    "models[1].load_state_dict(torch.load(weight_path))\n",
    "\n",
    "print(f\"✅ Loaded both model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(\"Evaluating model with SOT...\")\n",
    "models[0].test_loop(train_loader)\n",
    "models[0].test_loop(val_loader)\n",
    "models[0].test_loop(test_loader)\n",
    "\n",
    "print(\"\\nEvaluating model without SOT...\")\n",
    "models[1].test_loop(train_loader)\n",
    "models[1].test_loop(val_loader)\n",
    "models[1].test_loop(test_loader)\n",
    "\n",
    "print(f\"✅ Evaluated both models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly loaded the model weights by confirming the performance on the\n",
    "`train`, `val` and `test` split for ProtoNet on SwissProt with and without SOT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/ SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(train_loader, models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise episode for Protnet w/ SOT\n",
    "for loader in [train_loader, val_loader, test_loader]:\n",
    "    utils.visualise_episode(train_loader, models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise SOT transport plan\n",
    "\n",
    "Here we visualise the self-optimal transport plan for a few episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise transport plan for Protonet w/ SOT on train\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_transport_plan(train_loader, models[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise confusion patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/ SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Protonet w/o SOT on all splits\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "for ax, loader in zip(axs, [train_loader, val_loader, test_loader]):\n",
    "    utils.visualise_confusion_matrix(loader, models[1], ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
