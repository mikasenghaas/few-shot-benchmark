{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook contains the analysis of the data tracked on\n",
    "[Weights & Biases](https://wandb.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "We will first setup everything so that we can easily analyse the experiment\n",
    "results. This includes importing the necessary libraries, setting paths, loading\n",
    "the experiment results from W&B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Bult-in modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# External modules\n",
    "# - Data Representation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# - Data Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotnine as pn\n",
    "\n",
    "# - Machine Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "# - Experiment Configuration and Logging\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Custom modules\n",
    "from utils import eval_utils as utils\n",
    "from utils import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of global variables\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"artifacts\")\n",
    "FIGURE_DIR = os.path.join(ROOT_DIR, \"figures\")\n",
    "\n",
    "METHODS = [\"baseline\", \"baseline_pp\", \"matchingnet\", \"protonet\", \"maml\"]\n",
    "METHODS_WITH_SOT = []\n",
    "for method in METHODS:\n",
    "    METHODS_WITH_SOT.append(method)\n",
    "    METHODS_WITH_SOT.append(method + \"_sot\")\n",
    "\n",
    "STYLED_METHODS = [\"Baseline\", \"Baseline++\", \"MatchingNet\", \"ProtoNet\", \"MAML\"]\n",
    "STYLED_METHODS_WITH_SOT = []\n",
    "for method in STYLED_METHODS:\n",
    "    STYLED_METHODS_WITH_SOT.append(method)\n",
    "    STYLED_METHODS_WITH_SOT.append(method + \" (SOT)\")\n",
    "\n",
    "styled_methods_dict = dict(zip(METHODS, STYLED_METHODS))\n",
    "\n",
    "\n",
    "def get_name(name, sot=False):\n",
    "    return styled_methods_dict[name] + (\" (SOT)\" if sot else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "sns.set_style(\"dark\")\n",
    "colorstyle = \"RdBu\"\n",
    "sns.set_palette(colorstyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "WANDB_PROJECT = \"few-shot-benchmark\"\n",
    "WANDB_ENTITY = \"metameta-learners\"\n",
    "\n",
    "# Initialize W&B API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get all runs\n",
    "runs = api.runs(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Benchmark\n",
    "\n",
    "---\n",
    "\n",
    "All models on all datasets with and without SOT for fixed few-shot learning\n",
    "setting (5-way 5-shot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Experiment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs for experiment `benchmark`\n",
    "GROUP = \"benchmark\"\n",
    "USER = \"mikasenghaas\"\n",
    "\n",
    "group_runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load all runs from the given experiment group into a single\n",
    "dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Each experiment is uniquely identified by the following parameters:\n",
    "\n",
    "- `dataset`: The dataset used (`swissprot`, `tabula_muris`)\n",
    "- `method`: The model used (`baseline`, `baseline_pp`, `protonet`,\n",
    "  `matchingnet`, `maml`)\n",
    "- `use_sot`: Whether to include the SOT module (`True`, `False`)\n",
    "- `n_way`: The number of classes in each episode\n",
    "- `n_shot`: The number of support samples per class in each episode\n",
    "\n",
    "For each experiment setting, there are multiple trained models because of\n",
    "hyper-parameter tuning. We will group the runs by the above parameters and only\n",
    "use the best-performing model on the validation set for the following analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "# Let's also save two separate dataframes for the two different datasets\n",
    "df_best_runs_tm = df_best_runs[df_best_runs[(\n",
    "    \"config\", \"dataset\")] == \"tabula_muris\"]\n",
    "df_best_runs_sp = df_best_runs[df_best_runs[(\n",
    "    \"config\", \"dataset\")] == \"swissprot\"]\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Performance for all models\n",
    "\n",
    "Here, we plot a simple bar plot for all methods (5 methods, each with and\n",
    "without SOT) on all three splits (train, val, test). Performances are shown in\n",
    "two separate plots for Swissprot and Tabula Muris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by split for all methods\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(20, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "def pivot_acc(df):\n",
    "    tmp = []\n",
    "    for i, best_model in df.iterrows():\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            method_name = get_name(\n",
    "                best_model[(\"config\", \"method\")\n",
    "                           ], best_model[(\"config\", \"use_sot\")]\n",
    "            )\n",
    "            tmp.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"split\": split,\n",
    "                    \"acc\": best_model[(\"eval\", f\"{split}/acc\")],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_tm),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.barplot(\n",
    "    pivot_acc(df_best_runs_sp),\n",
    "    x=\"method\",\n",
    "    y=\"acc\",\n",
    "    hue=\"split\",\n",
    "    order=STYLED_METHODS_WITH_SOT,\n",
    "    ax=axs[1],\n",
    ")\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"Split\")\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Method\")\n",
    "    ax.set_ylabel(\"Acc. (%)\")\n",
    "\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-split-perf.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance by method with and without SOT\n",
    "\n",
    "Here, we compare the performance of the different methods with and without SOT.\n",
    "The left subplot shows the test performance on the Tabula Muris dataset, while\n",
    "the right subplot shows the test performance on the Swissprot dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by method with and without SOT\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_tm,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    order=METHODS,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    df_best_runs_sp,\n",
    "    x=(\"config\", \"method\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    ci=\"sd\",\n",
    "    order=METHODS,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set title\n",
    "axs[0].set_title(\"Tabula Muris\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"SwissProt\", fontweight=\"bold\")\n",
    "\n",
    "# Disable legend on first subplot\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xticklabels([get_name(name.get_text())\n",
    "                       for name in axs[0].get_xticklabels()])\n",
    "axs[1].set_xticklabels([get_name(name.get_text())\n",
    "                       for name in axs[1].get_xticklabels()])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(\"Test Acc. (%)\")\n",
    "    ax.set_xlabel(\"Method\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"benchmark-perf.pdf\"),\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Way-Shot Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Varying the number of shots per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment data for `way-shot` experiment\n",
    "GROUP = \"way-shot\"\n",
    "USER = \"mikasenghaas\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = [run for run in runs if run.group == GROUP and run.state == \"finished\"]\n",
    "print(f\"✅ Found {len(group_runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs)\n",
    "print(f\"✅ Loaded {len(df_runs)} runs.\")\n",
    "\n",
    "df_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the best run for each experiment configuration. This only has an\n",
    "effect if hyperparameter tuning was performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tuning runs by experiment configuration\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"val/acc\"))\n",
    "print(f\"✅ Filtered to {len(df_best_runs)} best runs.\")\n",
    "\n",
    "df_best_runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shot-Way-Analysis\n",
    "\n",
    "Display the test/acc as a function of the number of shots per class and the\n",
    "number of classes to distinguish between the different methods for ProtoNet\n",
    "without and with SOT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test/acc vs. n_shot for SOT and non-SOT methods\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "# test/acc ~ n_shot\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    ax=axs[0],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_way\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "# test/acc ~ n_way\n",
    "sns.scatterplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    alpha=0.25,\n",
    "    ax=axs[1],\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_best_runs,\n",
    "    x=(\"config\", \"n_shot\"),\n",
    "    y=(\"eval\", \"test/acc\"),\n",
    "    hue=(\"config\", \"use_sot\"),\n",
    "    legend=False,\n",
    "    ax=axs[1],\n",
    ")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_xlabel(\"N-Way\")\n",
    "axs[1].set_xlabel(\"N-Shot\")\n",
    "\n",
    "# Set axis labels\n",
    "axs[0].set_ylabel(\"Val. Acc. (%)\")\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "# Set legend title\n",
    "axs[0].get_legend().set_title(\"SOT\")\n",
    "axs[1].get_legend().set_title(\"SOT\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(FIGURE_DIR, \"way-shot.pdf\"), bbox_inches=\"tight\")\n",
    "print(f\"✅ Saved figure to {FIGURE_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Understanding embeddings\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this section will be to compare the embeddings learned by the models\n",
    "with and without SOT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "GROUP = \"benchmark\"\n",
    "\n",
    "# Filter runs by group\n",
    "group_runs = {\n",
    "    run.id: run for run in runs if run.group == GROUP and run.state == \"finished\"\n",
    "}\n",
    "print(f\"✅ Loaded {len(group_runs)} runs\")\n",
    "\n",
    "# Load runs into dataframe\n",
    "df_runs = utils.load_to_df(group_runs.values())\n",
    "df_best_runs = utils.get_best_run(df_runs, metric=(\"eval\", \"test/acc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_runs.loc[\"k5oxu82u\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a run for ProtoNet on Tabula Muris\n",
    "is_protonet = df_best_runs[(\"config\", \"method\")] == \"protonet\"\n",
    "is_tabula_muris = df_best_runs[(\"config\", \"dataset\")] == \"tabula_muris\"\n",
    "is_sot = df_best_runs[(\"config\", \"use_sot\")] == True\n",
    "best_run_id = df_best_runs[is_protonet & is_tabula_muris & is_sot].iloc[0].name\n",
    "\n",
    "# Load best run\n",
    "run = group_runs[best_run_id]\n",
    "print(\n",
    "    f\"✅ Loaded run {run.id} fo {run.config['n_way']}-way {run.config['n_shot']}-shot.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dataset/ method\n",
    "\n",
    "# Hacky way to initialise\n",
    "os.chdir(\"..\")  # Have to change to root directory to avoid re-downloading data\n",
    "dataset, _, _, model = train_utils.initialize_dataset_model(\n",
    "    OmegaConf.create(run.config), device=\"cpu\"\n",
    ")\n",
    "model.n_query = dataset.n_query\n",
    "loader = dataset.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "os.chdir(\"notebooks\")\n",
    "print(f\"✅ Initialised dataset and model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifact (model weights)\n",
    "utils.download_artifact(\n",
    "    api,\n",
    "    wandb_entity=WANDB_ENTITY,\n",
    "    wandb_project=WANDB_PROJECT,\n",
    "    artifact_dir=ARTIFACT_DIR,\n",
    "    run_id=run.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "weight_path = os.path.join(ARTIFACT_DIR, run.id, \"best_model.pt\")\n",
    "model.load_state_dict(torch.load(weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify hyper-parameters\n",
    "n_episodes = dataset.n_episodes\n",
    "n_way, n_support, n_query = dataset.n_way, dataset.n_support, dataset.n_query\n",
    "\n",
    "assert len(loader) == n_episodes, \"Number of episodes does not match dataset size.\"\n",
    "assert n_way == run.config[\"n_way\"], \"Number of classes does not match config.\"\n",
    "assert (\n",
    "    n_support == run.config[\"n_shot\"]\n",
    "), \"Number of support examples does not match config.\"\n",
    "assert (\n",
    "    n_query == run.config[\"n_query\"]\n",
    "), \"Number of query examples does not match config.\"\n",
    "\n",
    "print(\n",
    "    f\"✅ Loaded {n_way}-way {n_support}-shot {dataset._dataset_name} with episodic data loader (n={n_episodes}).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_loop(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise multiple episodes\n",
    "show_embeddings = [\"input\", \"backbone\", \"lstm\"]\n",
    "n_episodes = 2\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_episodes,\n",
    "    ncols=len(show_embeddings),\n",
    "    figsize=(5 * len(show_embeddings), 5 * n_episodes),\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\n",
    "    f\"{n_way}-way {n_support}-shot {dataset._dataset_name}\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    for j, show in enumerate(show_embeddings):\n",
    "        utils.visualise_episode(\n",
    "            loader,\n",
    "            model,\n",
    "            show=show,\n",
    "            ax=axs[i, j],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=1, figsize=(10, 10))\n",
    "utils.visualise_episode(loader, model, show=\"backbone\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking closer to particular runs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a run from the table above to look at it in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runid = None\n",
    "config = [run.config for run in group_runs if run.id == runid][0]\n",
    "dataset, loader, model = utils.init_run(config, ROOT_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate the run's model on the given dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping from encoding to annotation\n",
    "encoding2anot = {v: k for k, v in dataset.trg2idx.items()}\n",
    "\n",
    "# Define metric fn from sklearn assuming y_true and y_pred as input in this order\n",
    "clf_kwargs = {\"average\": \"macro\"}\n",
    "metric_fns = [\n",
    "    (metrics.accuracy_score, None),\n",
    "    (metrics.precision_score, clf_kwargs),\n",
    "    (metrics.recall_score, clf_kwargs),\n",
    "    (metrics.f1_score, clf_kwargs),\n",
    "]\n",
    "\n",
    "# Evaluate model and obtain its predictions with ground truth for each episode\n",
    "episodes_results = utils.eval_run(model, loader)\n",
    "\n",
    "# Compute metrics for each episode\n",
    "episodes_metrics = utils.compute_metrics(metric_fns, episodes_results)\n",
    "\n",
    "episodes_metrics.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
