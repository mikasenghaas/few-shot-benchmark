{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "This few-shot benchmark tests various meta-learning methods in the context of\n",
    "biomedical applications. In particular, we are dealing with the [Tabula Muris]()\n",
    "and [SwissProt]() datasets. One is a cell type classification task based on\n",
    "single-cell gene expressions and the other is a protein function prediction task\n",
    "based on protein sequences. The goal of this notebook is to explore basic\n",
    "statistics about the two datasets, as well as understand how the data loading is\n",
    "implemented for the episodic training during meta-training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "First, let's import the relevant modules needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.782504Z",
     "start_time": "2023-11-24T00:16:39.613357Z"
    }
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Module imports\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# External imports\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.808678Z",
     "start_time": "2023-11-24T00:16:39.783591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classes\n",
    "\n",
    "---\n",
    "\n",
    "Both datasets are implemented as subclasses of the `FewShotDataset` class and\n",
    "use some other generic utility classes. We will explore these here in detail.\n",
    "They are all defined in the `datasets.dataset` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotDataset\n",
    "\n",
    "The `FewShotDataset(torch.utils.data.Dataset)` is the base class for all\n",
    "few-shot datasets. It implements the `__getitem__` and `__len__` methods and has\n",
    "some utilities for checking the data validty. Furthermore, it is responsible for\n",
    "loading and extracting the dataset into the `root` directory if specified and\n",
    "not yet existent. However, as it is a abstract base class, it cannot be\n",
    "instantiated, e.g. it requires the `_dataset_name` and `_dataset_dir` as class\n",
    "attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.834211Z",
     "start_time": "2023-11-24T00:16:39.803639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: FewShotDataset\n",
    "from datasets.dataset import FewShotDataset  # noqa\n",
    "\n",
    "try:\n",
    "    few_shot_dataset = FewShotDataset()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fails with error {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.894681Z",
     "start_time": "2023-11-24T00:16:39.827479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo FewShotSubDataset\n",
    "from datasets.dataset import FewShotSubDataset  # noqa\n",
    "\n",
    "# Create a random dataset with 100 samples, 5 features and 5 classes\n",
    "samples = torch.rand(100, 5)\n",
    "targets = torch.randint(0, 5, (100,))  # 5-way\n",
    "subset_target = 4\n",
    "\n",
    "# Get all samples that belong to class 4\n",
    "subset_samples = samples[targets == subset_target]\n",
    "\n",
    "# Create a few-shot dataset for class 4\n",
    "few_shot_sub_dataset = FewShotSubDataset(subset_samples, subset_target)\n",
    "\n",
    "# Sanity checks\n",
    "assert (\n",
    "    len(few_shot_sub_dataset) == (targets == subset_target).sum()\n",
    "), \"❌ Length of few-shot dataset is not correct.\"\n",
    "assert (\n",
    "    few_shot_sub_dataset.dim == samples.shape[1]\n",
    "), \"❌ Dimension of few-shot dataset is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Batch Sampler\n",
    "\n",
    "The `EpisodicBatchSampler` is a utility class that randomly samples `n_way`\n",
    "classes (out of a total of `n_classes`) for a total of `n_episodes`. It can be\n",
    "used in episodic training to sample the classes used in each episode.\n",
    "\n",
    "The sampler is `n_episodes` long and each time samples randomly (without\n",
    "replacement) from `{0, ..., n_classes-1}` `n_way` times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.895795Z",
     "start_time": "2023-11-24T00:16:39.856954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: EpisodicBatchSampler\n",
    "from datasets.dataset import EpisodicBatchSampler  # noqa\n",
    "\n",
    "# Demo of EpisodicBatchSampler\n",
    "n_episodes, n_way, n_classes = 3, 5, 10\n",
    "episodic_batch_sampler = EpisodicBatchSampler(n_classes, n_way, n_episodes)\n",
    "\n",
    "print(f\"Episodes: {n_episodes}, Ways: {n_way}, Classes: {n_classes}\")\n",
    "for batch_idx, indices in enumerate(episodic_batch_sampler):\n",
    "    print(f\"Episode {batch_idx+1} w/ classes {indices.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabula Muris\n",
    "\n",
    "---\n",
    "\n",
    "**Tabula Muris** is a dataset of single cell transcriptome data (gene\n",
    "expressions) from mice, containing nearly `100,000` cells from `20` organs and\n",
    "tissues. The data allow for direct and controlled comparison of gene expression\n",
    "in cell types shared between tissues, such as immune cells from distinct\n",
    "anatomical locations. They also allow for a comparison of two distinct technical\n",
    "approaches:\n",
    "\n",
    "_More Resources_:\n",
    "\n",
    "- [Tabular Muris Website](https://tabula-muris.ds.czbiohub.org/)\n",
    "- [SF Biohub Article](https://www.czbiohub.org/sf/tabula-muris/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMSimpleDataset\n",
    "\n",
    "The `TMSimpleDataset` is a simple dataset class that is designed for regular\n",
    "multi-class classification training/ fine-tuning. It loads the entire\n",
    "(processed) dataset into memory and wraps inside a PyTorch Dataset object.\n",
    "Supports functionality for retrieving a single sample, a batched data loader and\n",
    "the dimensionality of the data.\n",
    "\n",
    "_Note: Upon first call, the `TMSimpleDataset` class will download the data into\n",
    "the `root` directory._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:50.756355Z",
     "start_time": "2023-11-24T00:16:39.875875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: TMSimpleDataset\n",
    "from datasets.cell.tabula_muris import TMSimpleDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "batch_size = 10\n",
    "root = \"../data\"\n",
    "min_samples = 20\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"root\": root,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "# Controls data split (returns subset of tissue types)\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Initialise TabulaMuris training dataset\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = TMSimpleDataset(**kwargs, mode=mode)\n",
    "    print(\n",
    "        f\"✅ TabulaMuris {mode} split ({len(data)}) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the raw data is downloaded and saved onto disk. Let's view the size of the\n",
    "raw and processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:51.179635Z",
     "start_time": "2023-11-24T00:16:50.751398Z"
    }
   },
   "outputs": [],
   "source": [
    "# This asssumes the data is already downloaded\n",
    "!du -sh ../data/tabula_muris/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:51.346564Z",
     "start_time": "2023-11-24T00:16:51.184369Z"
    }
   },
   "outputs": [],
   "source": [
    "!du -sh ../data/tabula_muris/processed/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, let's take a look at the data and plot some general dataset statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:57.259225Z",
     "start_time": "2023-11-24T00:16:51.335762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all data splits\n",
    "train_data = TMSimpleDataset(**kwargs, mode=\"train\")\n",
    "val_data = TMSimpleDataset(**kwargs, mode=\"val\")\n",
    "test_data = TMSimpleDataset(**kwargs, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of samples in each split\n",
    "print(f\"ℹ️ Tabula Muris dataset has {len(train_data)} train samples.\")\n",
    "print(f\"ℹ️ Tabula Muris dataset has {len(val_data)} train samples.\")\n",
    "print(f\"ℹ️ Tabula Muris dataset has {len(test_data)} train samples.\\n\")\n",
    "\n",
    "# Print number of classes (cell types) in each split\n",
    "print(\n",
    "    f\"ℹ️ Tabula Muris train dataset has {len(np.unique(train_data.targets))} unique classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"ℹ️ Tabula Muris val dataset has {len(np.unique(val_data.targets))} unique classes.\"\n",
    ")\n",
    "print(\n",
    "    f\"ℹ️ Tabula Muris tests dataset has {len(np.unique(test_data.targets))} unique classes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the samples and their features. We can obtain a single\n",
    "sample using indexing into the data. We will plot the dimensionality of each\n",
    "sample and hope to see the same number of features for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample from each split\n",
    "train_x, train_y = train_data[0]\n",
    "val_x, val_y = val_data[0]\n",
    "test_x, test_y = test_data[0]\n",
    "\n",
    "# Print sample shapes\n",
    "print(\n",
    "    f\"ℹ️ Tabula Muris train sample has shape {train_x.shape} and cell type {train_y}.\"\n",
    ")\n",
    "print(f\"ℹ️ Tabula Muris val sample has shape {val_x.shape} and cell type {val_y}.\")\n",
    "print(f\"ℹ️ Tabula Muris test sample has shape {test_x.shape} and cell type {test_y}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is a tuple of (sample, target) where sample is a torch.Tensor of\n",
    "shape (n_genes,) and target is a torch.Tensor of shape (1,) which denotes the\n",
    "cell type. We can see that the targets are encoded as integers, so we will need\n",
    "to decode them to get the actual cell type. Let's look at the feature\n",
    "distributions of some genes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` attribute of the dataset class is a `AnnData` object which contains\n",
    "annotations about the samples, here information about the cells (e.g. cell type,\n",
    "tissue, etc.), as well as the genes (e.g. gene name, mean expression, etc.). We\n",
    "can access the annotations using the `obs` and `var` attributes of the `AnnData`\n",
    "object. For now, let's convert he information into a `pd.DataFrame` with the\n",
    "cell id as index and the gene names as columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.DataFrame(\n",
    "    train_data.samples,\n",
    "    index=train_data.data.obs.index,\n",
    "    columns=train_data.data.var.index,\n",
    ")\n",
    "\n",
    "df_train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute some numeric summaries of the data. We use `info()` to obtain\n",
    "information about the data types and missing values and `describe()` to obtain\n",
    "some basic statistics about the data. We will limit the output to the first 10\n",
    "genes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show numeric summaries for first 10 cells (samples)\n",
    "num_genes = 10\n",
    "df_train_data.iloc[:, :num_genes].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All gene expressions (features) are numeric, continuous and non-null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show numeric summaries for first 10 genes (features)\n",
    "num_genes = 10\n",
    "df_train_data.iloc[:, :num_genes].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that there are no missing values. Furthermore, we observe an\n",
    "interesting pattern in the data. The mean is always in the range `~0` and the\n",
    "standard deviation is around `~1`. The vast majority (at least 75%) is at the\n",
    "minimum value which is a small negative value. This suggests a strong\n",
    "heavy-tailed distribution. The maximum value is a lot larger than the mean and\n",
    "standard deviation, suggesting strong outliers. Interestingly, the maximum value\n",
    "is never larger than 10 - this might be due to some pre-processing (e.g. cutting\n",
    "the data at 10).\n",
    "\n",
    "Let's visualise the distributions of the first 10 genes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = 5  # Visualise the first five genes\n",
    "fig, axs = plt.subplots(nrows=3, ncols=5, figsize=(20, 9))\n",
    "\n",
    "for j in range(num_genes):\n",
    "    min_val, max_val = np.inf, -np.inf\n",
    "    for i, data in enumerate([train_data, val_data, test_data]):\n",
    "        # Get data of first five cells in split\n",
    "        x = data.samples[:, :num_genes]\n",
    "\n",
    "        xmin_val, xmax_val = x[:, j].min(), x[:, j].max()\n",
    "        if xmin_val < min_val:\n",
    "            min_val = xmin_val\n",
    "        if xmax_val > max_val:\n",
    "            max_val = xmax_val\n",
    "\n",
    "        # Plot the first five genes\n",
    "        sns.histplot(\n",
    "            x=x[:, j],\n",
    "            ax=axs[i, j],\n",
    "            kde=True,\n",
    "            bins=100,\n",
    "            stat=\"density\",\n",
    "            color=[\"red\", \"green\", \"blue\"][i],\n",
    "        )\n",
    "\n",
    "        axs[i, 0].set_ylabel(\n",
    "            f\"{data.mode.capitalize()} Data\", fontweight=\"bold\")\n",
    "\n",
    "    # Set x-axis limits\n",
    "    for i in range(3):\n",
    "        axs[i, j].set_xlim(min_val, max_val)\n",
    "        axs[i, j].set_yscale(\"log\")\n",
    "\n",
    "    axs[0, j].set_title(\n",
    "        f\"Gene {j} ({data.data.var.iloc[j].name})\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Gene expression distribution of first five cells across splits\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualise the gene expression distribution of the first five (out of 2866)\n",
    "genes across the three splits using hisotgrams with overlayed kernel density\n",
    "estimates and log the y-axis to better understand the tail of the distribution.\n",
    "We can see that the gene expression distribution generally follows a\n",
    "heavy-tailed distribution, with a few genes having very high expression values.\n",
    "We can also see that the distribution are potentially different across the three\n",
    "splits, which is expected as the splits represent cells from different tissues\n",
    "and the cells were selected based on general variance in gene expression. One\n",
    "would hope that similar differences are also present when plotting by cell type,\n",
    "as such differences would be useful for classification.\n",
    "\n",
    "> Most of these findings align with the pre-processing steps described in\n",
    "> `data.utils.py` in the `preprocess` method. After filtering out samples and\n",
    "> features (e.g. too little expressions for a cell, too little variance of a\n",
    "> gene across cells), the data is a) log-transformed, b) zero-centered (and cut\n",
    "> off at 10) and c) zero-imputed. Thus, the original data must have been even\n",
    "> more heavy-tailed and sparse but now we observe zero-centered distribution\n",
    "> with no missing values and a heavy-tail data is cut at 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now understand the class distribution of the data. In this task, the label\n",
    "is the cell type (`cell_ontology_class_reannotated`). To make this a suitable\n",
    "transfer/ few-shot learning task the samples from the different splits originate\n",
    "from different tissues (group of similar cvelles that work together to perform a\n",
    "specific function). We can recover the original tissues, cell types and integer\n",
    "encoded labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tissue distribution\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "\n",
    "all_tissues = list(\n",
    "    set(train_data.data.obs.tissue.unique())\n",
    "    | set(val_data.data.obs.tissue.unique())\n",
    "    | set(test_data.data.obs.tissue.unique())\n",
    ")\n",
    "all_colors = sns.color_palette(\"Set2\", n_colors=len(all_tissues))\n",
    "tissue2idx = {tissue: i for i, tissue in enumerate(all_tissues)}\n",
    "for ax, data in zip(axs, [train_data, val_data, test_data]):\n",
    "    tissues = data.data.obs.tissue.value_counts()\n",
    "    ax = sns.barplot(x=tissues.index, y=tissues.values, ax=ax)\n",
    "\n",
    "    # Set colors\n",
    "    colors = [all_colors[tissue2idx[tissue]] for tissue in tissues.index]\n",
    "    for bar, color in zip(ax.containers[0], colors):\n",
    "        bar.set_facecolor(color)\n",
    "\n",
    "    ax.set(\n",
    "        xlabel=\"Tissue\",\n",
    "        ylabel=\"Number of cells\",\n",
    "        title=f\"{data.mode.capitalize()} Data\",\n",
    "    )\n",
    "    # Rotate x-axis labels\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "fig.suptitle(\"Tissue distribution\", fontsize=14, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aligns with the pre-processing that we set in the dataset class. There are\n",
    "15 tissues in training and four in validation and test. Crucially, these are\n",
    "disjoint, e.g. we will never have a cell from type `Lung` in our training. Let's\n",
    "next look at the target distribution across the splits, which are the cell\n",
    "types.\n",
    "\n",
    "There are three different ways to obtain the label of a sample from the dataset:\n",
    "\n",
    "1. In the `obs.cell_ontology_class_reannotated` column of the `AnnData` object\n",
    "   (`data` attribute of the dataset class). This will be the string\n",
    "   representation of the cell type.\n",
    "\n",
    "2. In the `obs.label` column of the `AnnData` object (`data` attribute of the\n",
    "   dataset class). This will be the integer representation of the cell type.\n",
    "\n",
    "3. In the `target` attribute of the dataset class. This will be the integer\n",
    "   representation that is specific to the split (e.g. does not match up with the\n",
    "   integer representation of the `AnnData` object). It creates a new mapping {0,\n",
    "   ..., n_classes-1} for all cell types that are present in the tissues of the\n",
    "   split. Thus, the integer representation of the cell type cannot be compared\n",
    "   across splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's showcase this by finding a cell type training set and validation set and\n",
    "show that they share the string representation and global integer code, but not\n",
    "the cell specific integer code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"endothelial cell\"\n",
    "\n",
    "# Obtaining label of sample from train split\n",
    "train_mask = train_data.data.obs.cell_ontology_class_reannotated == label\n",
    "val_mask = val_data.data.obs.cell_ontology_class_reannotated == label\n",
    "\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Global train encoding: {train_data.data[train_mask].obs.label.values[0]}\")\n",
    "print(f\"Global val encoding: {val_data.data[val_mask].obs.label.values[0]}\")\n",
    "\n",
    "print(f\"Split train encoding: {train_data[train_mask][1][0]}\")\n",
    "print(f\"Split train encoding: {val_data[val_mask][1][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see `endothelial cell` is encoded as `36` in the global encoding (used\n",
    "in the `label` attribute and which is saved in `trg2idx` and `idx2trg` in the\n",
    "dataset class. However, the encoding per split level may vary. Let's check the\n",
    "overlap between the cell types between the splits using the original string\n",
    "representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = set(train_data.data.obs.cell_ontology_class_reannotated.unique())\n",
    "val_classes = set(val_data.data.obs.cell_ontology_class_reannotated.unique())\n",
    "test_classes = set(test_data.data.obs.cell_ontology_class_reannotated.unique())\n",
    "\n",
    "print(\n",
    "    f\"ℹ️ Total unique cell types: {len(train_classes | val_classes | test_classes)}\\n\"\n",
    ")\n",
    "\n",
    "print(f\"ℹ️ Unique cell types in train: {len(train_classes)}\")\n",
    "print(f\"ℹ️ Unique cell types in val: {len(val_classes)}\")\n",
    "print(f\"ℹ️ Unique cell types in test: {len(test_classes)}\\n\")\n",
    "\n",
    "print(\n",
    "    f\"ℹ️ Unique cell types shared in all: {len(train_classes & val_classes & test_classes)} {train_classes & val_classes & test_classes}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"ℹ️ Unique cell types shared in train and val: {len(train_classes & val_classes)} {train_classes & val_classes}\"\n",
    ")\n",
    "print(\n",
    "    f\"ℹ️ Unique cell types shared in train and test: {len(train_classes & test_classes)} {train_classes & test_classes}\"\n",
    ")\n",
    "print(\n",
    "    f\"ℹ️ Unique cell types shared in val and test: {len(val_classes & test_classes)} {val_classes & test_classes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more unique cell types in the train split as there are more unique\n",
    "tissues in the train split. It seems that cell types are pretty closely linked\n",
    "(specific) to tissues, because there is little overlap between the cell types in\n",
    "the different splits. Most cell types seem to only occur in a specific tissue,\n",
    "and few cell types have general functions that are needed in multiple tissues.\n",
    "We see that there are only 2 cell types that are present all splits, 10 that are\n",
    "shared between train and val and 4 that are shared between train and test and\n",
    "train and val.\n",
    "\n",
    "This means that the task is not trivial. Learning a mapping from gene expression\n",
    "to cell type purely on the training set doesn't mean good performance on the\n",
    "validation or test split as these contain cell types that are not present in the\n",
    "training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the relationship between tissues and cell types by plotting the\n",
    "cooccurrence matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to get cooccurence matrix of tissue and cell type\n",
    "def tissue_cell_type_cooccurrence(data, ax):\n",
    "    tmp = data.data.obs.pivot_table(\n",
    "        index=\"tissue\",\n",
    "        columns=\"cell_ontology_class_reannotated\",\n",
    "        values=\"label\",\n",
    "        aggfunc=\"count\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "\n",
    "    # Row normalise\n",
    "    tmp = tmp.div(tmp.sum(axis=1), axis=0)\n",
    "\n",
    "    sns.heatmap(tmp, ax=ax)\n",
    "    ax.set(\n",
    "        title=f\"{data.mode.capitalize()} tissue and cell type cooccurrence matrix\",\n",
    "        xlabel=\"Cell type\",\n",
    "        ylabel=\"Tissue\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cooccurrence matrix of tissue and cell type\n",
    "fig, ax = plt.subplots(nrows=3, figsize=(20, 20))\n",
    "tissue_cell_type_cooccurrence(train_data, ax[0])\n",
    "tissue_cell_type_cooccurrence(val_data, ax[1])\n",
    "tissue_cell_type_cooccurrence(test_data, ax[2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, cell types are often very tightly coupled with tissue types. Thus,\n",
    "splitting on tissue types for a few-shot learning task means to a large extent\n",
    "splitting on cell types as well. For good measure, let's also plot the target\n",
    "distribution of the cell types in the train, val and test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the class distributions\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "\n",
    "all_labels = list(\n",
    "    set(train_data.data.obs.label.unique())\n",
    "    | set(val_data.data.obs.label.unique())\n",
    "    | set(test_data.data.obs.label.unique())\n",
    ")\n",
    "all_colors = sns.color_palette(\"Set2\", n_colors=max(all_labels) + 1)\n",
    "for ax, data in zip(axs, [train_data, val_data, test_data]):\n",
    "    labels_enc = data.data.obs.label.value_counts()\n",
    "    sns.barplot(x=labels_enc.index, y=labels_enc.values, ax=ax)\n",
    "\n",
    "    # Set colors\n",
    "    colors = [all_colors[label] for label in labels_enc.index]\n",
    "    for bar, color in zip(ax.containers[0], colors):\n",
    "        bar.set_facecolor(color)\n",
    "\n",
    "    ax.set(\n",
    "        xlabel=\"Cell Type\",\n",
    "        ylabel=\"Number of cells\",\n",
    "        title=f\"{data.mode.capitalize()} Data\",\n",
    "    )\n",
    "\n",
    "fig.suptitle(\"Cell type distribution\", fontsize=14, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMSetDataset\n",
    "\n",
    "The `TMSetDataset` is designed for few-shot learning. Most configurations are\n",
    "the same as for the `TMSimpleDataset`, but crucially the dataset class will\n",
    "return an episodic batch sampler based on the `n_way`, `n_support`, `n_query`\n",
    "and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:12.080431Z",
     "start_time": "2023-11-24T00:16:59.065850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.cell.tabula_muris import TMSetDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "# Controls data split (returns subset of tissue types)\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    TMSetDataset(**kwargs, mode=mode)\n",
    "\n",
    "    print(f\"✅ TMSetDataset {mode} split loaded in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few-shot learning dataset, a single \"sample\" is defined not as the feature\n",
    "vector, target tuple of the gene expression levels and the target tissue but as\n",
    "a set of support and query samples within a class. Thus, the `__getitem__`\n",
    "method returns a tuple of the support and query samples and targets for the\n",
    "`i`-th class. Thus, the returned tensor dimension will be\n",
    "`(n_support + n_query, n_features)` for the samples and\n",
    "`(n_support + n_query, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:17.913720Z",
     "start_time": "2023-11-24T00:17:12.073803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = TMSetDataset(**kwargs, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:17.974538Z",
     "start_time": "2023-11-24T00:17:17.904882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get sample by indexing\n",
    "tr_smp, tr_trg = data[0]\n",
    "\n",
    "# Support samples and target\n",
    "sup_tr_smp, sup_tr_trg = tr_smp[:n_support], tr_trg[:n_support]\n",
    "\n",
    "# Query samples and target\n",
    "que_tr_smp, que_tr_trg = tr_smp[n_support:], tr_trg[n_support:]\n",
    "\n",
    "print(f\"Training samples shape: {tr_smp.shape} and target {tr_trg}\")\n",
    "print(f\"Support samples shape: {sup_tr_smp.shape} and target {sup_tr_trg}\")\n",
    "print(f\"Query samples shape: {sup_tr_smp.shape} and target {sup_tr_trg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader class combines the support-query sampler per class (as\n",
    "defined above) and the `EpisodicBatchSampler` to create a data loader that\n",
    "returns batches of episodes where each time we get `n_way` classes with\n",
    "`n_support` support samples and `n_query` query samples per class. First, the\n",
    "episodic batch sample samples the `n_way` random class indices and then the\n",
    "support-query sampler samples the support and query samples for each class.\n",
    "Thus, the final tensor shapes will be `(n_way, n_query + n_support, n_features)`\n",
    "for the samples and `(n_way, n_query + n_support, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:18.224954Z",
     "start_time": "2023-11-24T00:17:17.950859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get data loader\n",
    "train_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Get batch\n",
    "tr_smps, tr_trgs = next(iter(train_loader))\n",
    "\n",
    "print(f\"Training batch shape: {tr_smps.shape} and target {tr_trgs.shape}\")\n",
    "print(f\"Targets:\\n{tr_trgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB 1:** Shuffling in meta-learning tasks is not necessary because the episodic\n",
    "batch sampler and the sub-class sampler are already random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwissProt\n",
    "\n",
    "---\n",
    "\n",
    "SWISS-PROT is an annotated protein sequence database, which was created at the\n",
    "Department of Medical Biochemistry of the University of Geneva (first started\n",
    "1987). In SWISS-PROT two classes of data can be distinguished: the core data and\n",
    "the annotation. For each sequence entry the core data consists of the sequence\n",
    "data; the citation information (bibliographical references) and the taxonomic\n",
    "data (description of the biological source of the protein), while the annotation\n",
    "consists of the description of the following items:\n",
    "\n",
    "- Function(s) of the protein\n",
    "- Post-translational modification(s). For example carbohydrates,\n",
    "  phosphorylation, acetylation, GPI-anchor, etc.\n",
    "- Domains and sites. For example calcium binding regions, ATP-binding sites,\n",
    "  zinc fingers, homeoboxes, SH2 and SH3 domains, etc.\n",
    "- Secondary structure. For example alpha helix, beta sheet, etc.\n",
    "- Quaternary structure. For example homodimer, heterotrimer, etc.\n",
    "- Similarities to other proteins\n",
    "- Disease(s) associated with deficiencie(s) in the protein\n",
    "- Sequence conflicts, variants, etc.\n",
    "\n",
    "Within this project we will focus on the function annotation of proteins, thus\n",
    "given the protein sequence (string of amino acids) we want to predict the\n",
    "function of the protein.\n",
    "\n",
    "_More Resources_:\n",
    "\n",
    "- [National Library of Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102476/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPSimpleDataset\n",
    "\n",
    "The `SPSimpleDataset` is a simple dataset class that is designed for regular\n",
    "multi-class classification training/ fine-tuning. It loads the entire\n",
    "(processed) dataset into memory and wraps inside a PyTorch Dataset object.\n",
    "Supports functionality for retrieving a single sample, a batched data loader and\n",
    "the dimensionality of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:18.269243Z",
     "start_time": "2023-11-24T00:17:17.980370Z"
    }
   },
   "outputs": [],
   "source": [
    "# The data has to be manually downloaded to this folder\n",
    "!du -sh ../data/swissprot/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:06.367946Z",
     "start_time": "2023-11-24T00:26:02.518382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.prot.swissprot import SPSimpleDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "batch_size = 10\n",
    "min_samples = 6\n",
    "\n",
    "kwargs = {\n",
    "    \"root\": root,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"min_samples\": min_samples,\n",
    "}\n",
    "\n",
    "# Show loading time for each split\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = SPSimpleDataset(**kwargs, mode=mode)\n",
    "\n",
    "    print(\n",
    "        f\"✅ SwissProt {mode} split ({len(data)} samples) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:19:14.670958Z",
     "start_time": "2023-11-24T00:19:14.506163Z"
    }
   },
   "outputs": [],
   "source": [
    "# This asssumes the data is downloaded manually\n",
    "!du -sh ../data/swissprot/processed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we can load the data fast and easy. The class also supports subsetting\n",
    "which is useful for debugging the training. However, because we are still\n",
    "loading all the data into memory before subsetting, we do not get any speed up\n",
    "(_NB: We could decide to save and load splitted data to disk, but this is not\n",
    "implemented yet_).\n",
    "\n",
    "**NB:** The SwissProt dataset as-is is relatively small and can be loaded into\n",
    "memory without many problems. In this case, subsetting the data decreases the\n",
    "number of classes that are included according to our specification of the\n",
    "minimum number of classes and is therefore generally not advised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the full training split again and do some exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:09.974262Z",
     "start_time": "2023-11-24T00:26:08.630771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: SPSimpleDataset\n",
    "data = SPSimpleDataset(**kwargs, mode=\"train\")\n",
    "\n",
    "# Get some basic statistics\n",
    "num_samples = len(data)\n",
    "dim = data.dim\n",
    "classes = np.unique([smp.annot for smp in data.samples])\n",
    "unique_classes = len(classes)\n",
    "\n",
    "print(f\"ℹ️ SwissProt train split has {len(data)} samples\")\n",
    "print(f\"ℹ️ Each sample is an encoded protein sequence of length {dim}\")\n",
    "print(f\"ℹ️ SwissProt train split has {unique_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a single sample by indexing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:23.605545Z",
     "start_time": "2023-11-24T00:26:23.567681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample by indexing\n",
    "x, y = data[0]\n",
    "\n",
    "print(f\"Training sample (encoded protein sequence): {x.shape} and target {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get a regular batch by using the data loader returned by the\n",
    "`get_data_loader` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:04.584714Z",
     "start_time": "2023-11-24T00:27:04.532062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample using data loader (use pin_memory=True if using GPU)\n",
    "data_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Create iterator\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Get five batches\n",
    "for batch_idx in range(5):\n",
    "    xb, yb = next(data_iter)\n",
    "    print(f\"Batch {batch_idx+1} Sequence: {xb.shape} and target {yb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPSetDataset\n",
    "\n",
    "The `SPSetDataset` is designed for few-shot learning. Most configurations are\n",
    "the same as for the `SPSimpleDataset`, but crucially the dataset class will\n",
    "return an episodic batch sampler based on the `n_way`, `n_support`, `n_query`\n",
    "and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:36.427002Z",
     "start_time": "2023-11-24T00:27:33.050981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.prot.swissprot import SPSetDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = SPSetDataset(**kwargs, mode=mode)\n",
    "    print(\n",
    "        f\"✅ SPSetDataset {mode} split ({len(data)} class data loaders) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the full training split again and do some exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:40.930550Z",
     "start_time": "2023-11-24T00:27:39.627158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: SPSetDataset\n",
    "data = SPSetDataset(**kwargs, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:45.424218Z",
     "start_time": "2023-11-24T00:27:45.401204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get some basic statistics\n",
    "num_samples = len(data)\n",
    "dim = data.dim\n",
    "\n",
    "print(f\"ℹ️ SwissProt train split has {len(data)} classes\")\n",
    "print(f\"ℹ️ Each sample is an encoded protein sequence of length {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in few-shot learning a single \"sample\" is defined not as the feature\n",
    "vector, target tuple but as a set of support and query samples within a class.\n",
    "Thus, the `__getitem__` method returns a tuple of the support and query samples\n",
    "and targets for the `i`-th class. Thus, the returned tensor dimension will be\n",
    "`(n_support + n_query, n_features)` for the samples and\n",
    "`(n_support + n_query, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:28:54.263329Z",
     "start_time": "2023-11-24T00:28:54.232381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample by indexing\n",
    "x, y = data[0]\n",
    "\n",
    "print(\n",
    "    f\"Training sample (encoded protein sequence): feature shape {x.shape} and target shape {y.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader class combines the support-query sampler per class (as\n",
    "defined above) and the `EpisodicBatchSampler` to create a data loader that\n",
    "returns batches of episodes where each time we get `n_way` classes with\n",
    "`n_support` support samples and `n_query` query samples per class. First, the\n",
    "episodic batch sample samples the `n_way` random class indices and then the\n",
    "support-query sampler samples the support and query samples for each class.\n",
    "Thus, the final tensor shapes will be `(n_way, n_query + n_support, n_features)`\n",
    "for the samples and `(n_way, n_query + n_support, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:29:47.161915Z",
     "start_time": "2023-11-24T00:29:47.126149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample using data loader (use pin_memory=True if using GPU)\n",
    "data_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Get one batches\n",
    "xb, yb = next(iter(data_loader))\n",
    "print(f\"Batch {batch_idx+1} Sequence: {xb.shape} and target {yb.shape}\")\n",
    "print(\"Target\")\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Explore Raw Tabula Muris Data\n",
    "\n",
    "**Note: Code here doesn't run anymore**\n",
    "\n",
    "The `MacaData` class is responsible for loading and processing the Tabula Muris\n",
    "dataset. Thus, before looking at the `TMSimpleDataset` and `TMSetDataset`, let's\n",
    "investigate the data loading/ processing first.\n",
    "\n",
    "**Changes to the original implementation:**\n",
    "\n",
    "Originally, the class loads the entire dataset and processes it within the\n",
    "constructor. This comes with several limitations:\n",
    "\n",
    "1. We cannot easily look at the raw data.\n",
    "\n",
    "2. We have to load and preprocess the entire dataset, even if we just want to\n",
    "   use samples within a specific data split.\n",
    "\n",
    "3. It does support any subsampling.\n",
    "\n",
    "To account for this, the `MacaData` class has been augmented in the following\n",
    "way.\n",
    "\n",
    "1. Data processing is not performed inside the constructor but has to be called\n",
    "   via public method `process_data`.\n",
    "\n",
    "2. The processed data may now be saved to disk via the public method\n",
    "   `save_data`.\n",
    "\n",
    "To support efficient loading of subsets and splits of the data, we later also\n",
    "implement the `MacaDataLoader` class which will be used to load the data during\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data\n",
    "\n",
    "We first look at the raw data. The `MacaData` class expects the path to a\n",
    "`.h5ad` file containing the data as input and loads the data as well as computes\n",
    "the class mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.654974Z",
     "start_time": "2023-11-24T00:17:29.603345Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets.cell.utils import MacaData  # noqa\n",
    "#\n",
    "# path = os.path.join(\"..\", \"data\", \"tabula_muris\", \"tabula-muris-comet.h5ad\")\n",
    "#\n",
    "# start = time.time()\n",
    "# maca_data = MacaData(path=path)\n",
    "# print(f\"⌛ Loaded data in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.656722Z",
     "start_time": "2023-11-24T00:17:29.622063Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Save attributes\n",
    "#       raw_data = maca_data.adata\n",
    "#       trg2idx, idx2trg = maca_data.trg2idx, maca_data.idx2trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MacaData` class stores the loaded data in the attribute `adata` (annotated\n",
    "data) as an object of type `anndata.AnnData`. It is a data structure that stores\n",
    "the data including annotations which is often used for bioinformatics data. We\n",
    "can get detailled information about the data by printing the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.673064Z",
     "start_time": "2023-11-24T00:17:29.639834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print meta-data of entire dataset\n",
    "#       raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the annotation for each cell (sample) and each gene (feature) by\n",
    "accessing the `obs` and `var` attributes of the `anndata.AnnData` object. The\n",
    "`obs` attribute is a `pandas.DataFrame` with the cell annotations and the `var`\n",
    "attribute is a `pandas.DataFrame` with the gene annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.683681Z",
     "start_time": "2023-11-24T00:17:29.659483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell annotations\n",
    "#       raw_data.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the meta data of each cell in the dataset. The meta data contains\n",
    "information about the mic (like the id, gender, age, etc.) and the cell type\n",
    "(like the cell type, (sub-)tissue, etc.) and much more. There are a total of\n",
    "105.960 cells in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.697427Z",
     "start_time": "2023-11-24T00:17:29.674833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gene annotations\n",
    "#       raw_data.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.709373Z",
     "start_time": "2023-11-24T00:17:29.692173Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "#       feature_matrix = raw_data.X\n",
    "#       targets = raw_data.obs[\"label\"].cat.codes.to_numpy()\n",
    "#\n",
    "#       print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "#       print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the distribution of the target tissues by showing the top 10\n",
    "most frequent tissues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.730372Z",
     "start_time": "2023-11-24T00:17:29.709967Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Plot Cell Type Distribution\n",
    "#       _, ax = plt.subplots(figsize=(20, 10))\n",
    "#       cell_types = [maca_data.idx2trg[idx] for idx in targets]\n",
    "#\n",
    "#       top_k = 10\n",
    "#       counts = collections.Counter(cell_types)\n",
    "#       counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "#\n",
    "#       sns.barplot(x=list(counts.keys()), y=list(counts.values()), palette=\"mako\", ax=ax)\n",
    "#       ax.set(\n",
    "#           xlabel=\"Cell type\", ylabel=\"Count\", title=f\"Cell Type Distribution (Top {top_k})\"\n",
    "#       )\n",
    "#       ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample we record the gene expression levels for all genes. The gene\n",
    "annotation contains some summary statistics about the expressivitiy of each\n",
    "genes as meta data. The index in this data frame is the column names in the gene\n",
    "expression feature matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processed Data\n",
    "\n",
    "Let's preprocess the data according to the original implementation. The\n",
    "following steps are performed in the `process_data` method:\n",
    "\n",
    "- Filter out cells with no target\n",
    "- Filter out genes that are expressed in less than 5 cells\n",
    "- Filter out cells with less than 5000 counts and 500 genes expressed\n",
    "- Normalize per cell (simple lib size normalization)\n",
    "- Filter out genes with low dispersion (retain the once with high variance)\n",
    "- Log transform and scale the data\n",
    "- Zero-imputation of Nans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.745505Z",
     "start_time": "2023-11-24T00:17:29.727746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "#       start = time.time()\n",
    "#       maca_data.process_data()\n",
    "#       print(f\"⌛ Processed data in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.765974Z",
     "start_time": "2023-11-24T00:17:29.745631Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Save attributes\n",
    "#       processed_data = maca_data.adata\n",
    "#       trg2idx, idx2trg = maca_data.trg2idx, maca_data.idx2trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.781434Z",
     "start_time": "2023-11-24T00:17:29.763839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell annotations\n",
    "#       processed_data.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.800711Z",
     "start_time": "2023-11-24T00:17:29.781561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gene annotations\n",
    "#       processed_data.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.817370Z",
     "start_time": "2023-11-24T00:17:29.799603Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "# feature_matrix = processed_data.X\n",
    "#       targets = processed_data.obs[\"label\"].cat.codes.to_numpy()\n",
    "#\n",
    "#       print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "#       print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.835678Z",
     "start_time": "2023-11-24T00:17:29.817567Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Plot Cell Type Distribution\n",
    "#       _, ax = plt.subplots(figsize=(20, 10))\n",
    "#       cell_types = [maca_data.idx2trg[idx] for idx in targets]\n",
    "#\n",
    "#       top_k = 10\n",
    "#       counts = collections.Counter(cell_types)\n",
    "#       counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "#\n",
    "#       sns.barplot(x=list(counts.keys()), y=list(\n",
    "#           counts.values()), palette=\"mako\", ax=ax)\n",
    "#       ax.set(\n",
    "#           xlabel=\"Cell type\", ylabel=\"Count\", title=f\"Cell Type Distribution (Top {top_k})\"\n",
    "#       )\n",
    "#       ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
