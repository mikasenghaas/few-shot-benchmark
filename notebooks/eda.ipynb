{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "This few-shot benchmark tests various meta-learning methods in the context of biomedical applications. In particular, we are dealing with the [Tabula Muris]() and [SwissProt]() datasets. One is a cell type classification task based on single-cell gene expressions and the other is a protein function prediction task based on protein sequences. The goal of this notebook is to explore basic statistics about the two datasets, as well as understand how the data loading is implemented for the episodic training during meta-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "First, let's import the relevant modules needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Module imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classes\n",
    "\n",
    "---\n",
    "\n",
    "Both datasets are implemented as subclasses of the `FewShotDataset` class and use some other generic utility classes. We will explore these here in detail. They are all defined in the `datasets.dataset` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotDataset\n",
    "\n",
    "The `FewShotDataset(torch.utils.data.Dataset)` is the base class for all few-shot datasets. It implements the `__getitem__` and `__len__` methods and has some utilities for checking the data validty. Furthermore, it is responsible for loading and extracting the dataset into the `root` directory if specified and not yet existent. However, as it is a abstract base class, it cannot be instantiated, e.g. it requires the `_dataset_name` and `_dataset_dir` as class attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: FewShotDataset\n",
    "from datasets.dataset import FewShotDataset # noqa\n",
    "\n",
    "try:\n",
    "    few_shot_dataset = FewShotDataset()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fails with error {e}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotSubDataset\n",
    "\n",
    "The `FewShotSubDataset(torch.utils.data.Dataset)` is a class used for using only a subset of samples that are in the same class in a PyTorch Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo FewShotSubDataset\n",
    "from datasets.dataset import FewShotSubDataset #noqa\n",
    "\n",
    "# Create a random dataset with 100 samples, 5 features and 5 classes\n",
    "samples = torch.rand(100, 5)\n",
    "targets = torch.randint(0, 5, (100,)) # 5-way\n",
    "subset_target = 4\n",
    "\n",
    "# Create a subset dataset for class 4\n",
    "subset_samples = samples[targets == subset_target]\n",
    "\n",
    "# Create a few-shot dataset for class 4\n",
    "few_shot_sub_dataset = FewShotSubDataset(subset_samples, subset_target)\n",
    "\n",
    "# Sanity checks\n",
    "assert len(few_shot_sub_dataset) == (targets  == subset_target).sum(), \"❌ Length of few-shot dataset is not correct.\"\n",
    "assert few_shot_sub_dataset.dim == samples.shape[1], \"❌ Dimension of few-shot dataset is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Batch Sampler\n",
    "\n",
    "The `EpisodicBatchSampler` is a utility class that randomly samples `n_way` classes (out of a totla of `n_classes`) for a total of `n_episodes`. It can be used in episodic training to sample the classes used in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: EpisodicBatchSampler\n",
    "from datasets.dataset import EpisodicBatchSampler # noqa\n",
    "\n",
    "# Demo of EpisodicBatchSampler\n",
    "n_episodes, n_way, n_classes = 3, 5, 10\n",
    "episodic_batch_sampler = EpisodicBatchSampler(n_classes, n_way, n_episodes)\n",
    "\n",
    "print(f\"Episodes: {n_episodes}, Ways: {n_way}, Classes: {n_classes}\")\n",
    "for batch_idx, indices in enumerate(episodic_batch_sampler):\n",
    "    print(f\"Episode {batch_idx+1} w/ classes {indices.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabula Muris\n",
    "\n",
    "---\n",
    "\n",
    "**Tabula Muris** is a dataset of single cell transcriptome data (gene expressions) from mice, containing nearly `100,000` cells from `20` organs and tissues. The data allow for direct and controlled comparison of gene expression in cell types shared between tissues, such as immune cells from distinct anatomical locations. They also allow for a comparison of two distinct technical approaches:\n",
    "\n",
    "*More Resources*: \n",
    "\n",
    "- [Tabular Muris Website](https://tabula-muris.ds.czbiohub.org/)\n",
    "- [SF Biohub Article](https://www.czbiohub.org/sf/tabula-muris/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MacaData\n",
    "\n",
    "The `MacaData` class is responsible for loading and processing the Tabula Muris dataset. Thus, before looking at the `TMSimpleDataset` and `TMSetDataset`, let's investigate the data loading/ processing first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cell.utils import MacaData # noqa\n",
    "\n",
    "path = os.path.join(\"..\", \"data\", \"tabula_muris\", \"tabula-muris-comet.h5ad\")\n",
    "\n",
    "start = time.time()\n",
    "maca_data = MacaData(src_file=path)\n",
    "annotated_data = maca_data.adata\n",
    "print(f\"⌛ Loaded data in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading and processing time for all samples takes ~30 seconds. As this function is called on each instantiation of the Tabula Muris dataset, we are loading the data in all splits in ~1.30 minutes. \n",
    "\n",
    "We can trivially speed up this time by only processing the cells relevant for the split and by introducing a subsampling flag which will load 10% of the data. The `MacaDataImproved` class inherits from the `MacaData` class and implements these two changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cell.utils import MacaDataImproved # noqa\n",
    "\n",
    "path = os.path.join(\"..\", \"data\", \"tabula_muris\", \"tabula-muris-comet.h5ad\")\n",
    "\n",
    "start = time.time()\n",
    "MacaDataImproved(src_file=path, mode=\"train\", subset=False).adata\n",
    "print(f\"⌛ Loaded training split in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "MacaDataImproved(src_file=path, mode=\"train\", subset=True).adata\n",
    "print(f\"⌛ Loaded subsetted training split in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all splits should be reduced by a factor of **3x** and loading a the subsetted data should reduce the time by a factor of **10x**. Thus, when combined we can load the data **30x** faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated data (`anndata.AnnData`) is a data structure that stores the data including annotations. We can get detailled information about the data by printing the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the annotation for each cell (sample) and each gene (feature) by accessing the `obs` and `var` attributes of the `anndata.AnnData` object. The `obs` attribute is a `pandas.DataFrame` with the cell annotations and the `var` attribute is a `pandas.DataFrame` with the gene annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell annotations\n",
    "annotated_data.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene annotations\n",
    "annotated_data.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "feature_matrix = annotated_data.X\n",
    "targets = annotated_data.obs[\"label\"].cat.codes.to_numpy()\n",
    "\n",
    "print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cell Type Distribution\n",
    "_, ax = plt.subplots(figsize=(20, 10))\n",
    "names2cells = {v: k for k, v in maca_data.cells2names.items()}\n",
    "cell_types = [names2cells[trg] for trg in targets]\n",
    "\n",
    "top_k = 10\n",
    "counts = collections.Counter(cell_types)\n",
    "counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "\n",
    "sns.barplot(x=list(counts.keys()), y=list(counts.values()), palette=\"mako\", ax=ax)\n",
    "ax.set(xlabel=\"Cell type\", ylabel=\"Count\", title=f\"Cell Type Distribution (Top {top_k})\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same analysis for the data that we get from the `MacaDataImproved` class for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MacaData for each split\n",
    "maca_data_train = MacaDataImproved(src_file=path, mode=\"train\", subset=False)\n",
    "maca_data_val = MacaDataImproved(src_file=path, mode=\"val\", subset=False)\n",
    "maca_data_test = MacaDataImproved(src_file=path, mode=\"test\", subset=False)\n",
    "\n",
    "# Load subset of MacaData for each split\n",
    "maca_data_sub_train = MacaDataImproved(src_file=path, mode=\"train\", subset=True)\n",
    "maca_data_sub_val = MacaDataImproved(src_file=path, mode=\"val\", subset=True)\n",
    "maca_data_sub_test = MacaDataImproved(src_file=path, mode=\"test\", subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_data = {\"train\": maca_data_train, \"val\": maca_data_val, \"test\": maca_data_test}\n",
    "sub_train_val_test_data = {\"train\": maca_data_sub_train, \"val\": maca_data_sub_val, \"test\": maca_data_sub_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "for mode, data in train_val_test_data.items():\n",
    "    feature_matrix = data.adata.X\n",
    "    targets = data.adata.obs[\"label\"].cat.codes.to_numpy()\n",
    "\n",
    "    print(f\"Split {mode}\")\n",
    "    print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "    print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMSimpleDataset\n",
    "\n",
    "The `TMSimpleDataset` inherits from the abstract `TMDataset` class which, in turn, inherits from the generic `FewShotDataset` class. The `TMDataset` defines the `_dataset_name` as `\"tabula_muris\"` and the `_dataset_url` and provides a convenient loader utility which loads all samples and their targets. The `TMSimpleDataset` initialises the data directory, loads the data and then does the sanity checks from the base class. It provides the basic methods `__getitem__`, `__len__`, the `dim` property.\n",
    "\n",
    "Crucially, the data loader is tied to the dataset class and is available by calling the `get_data_loader()` method. It will sample batches of size `batch_size`.\n",
    "\n",
    "*Note: Upon first call, the `TMSimpleDataset` class will download the data into the `root` directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: TMSimpleDataset\n",
    "from datasets.cell.tabula_muris import TMSimpleDataset # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "batch_size = 10 # Controls the batch_size of data loader\n",
    "root = \"./data\" # Controls where to store the data\n",
    "min_samples = 20 # Filter out tissue types with less than min_samples\n",
    "\n",
    "modes = [\"train\", \"val\", \"test\"] # Controls data split (returns subset of tissue types)\n",
    "\n",
    "# Initialise TabulaMuris training dataset\n",
    "data = {}\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    tm_data = TMSimpleDataset(\n",
    "        batch_size=batch_size,\n",
    "        root=root,\n",
    "        mode=mode,\n",
    "        min_samples=min_samples\n",
    "    )\n",
    "    data[mode] = tm_data\n",
    "\n",
    "    print(f\"✅ TabulaMuris {mode} split loaded in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cell.utils import MacaDataImproved # noqa\n",
    "\n",
    "path = os.path.join(\"..\", \"data\", \"tabula_muris\", \"tabula-muris-comet.h5ad\")\n",
    "\n",
    "start = time.time()\n",
    "annotated_train = MacaDataImproved(src_file=path, mode=\"train\", subset=False).adata\n",
    "print(f\"⌛ Loaded training split in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "start = time.time()\n",
    "annotated_train_sub = MacaDataImproved(src_file=path, mode=\"train\", subset=True).adata\n",
    "print(f\"⌛ Loaded subsetted training split in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the size of the downloaded data\n",
    "!du -sh ../data/tabula_muris/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gene_association.mgi` (`84 MB`): This file is associated with gene annotations, specifically regarding mouse genes. The file likely contains information such as gene identifiers, gene names, and possibly their associations with various biological functions or diseases.\n",
    "\n",
    "`go-basic.obo` (`32 MB`): This file is associated with the Gene Ontology (GO), which is a major bioinformatics initiative to unify the representation of gene and gene product attributes across all species. The \".obo\" format (Open Biomedical Ontologies format) is a text-based format used for ontologies. The file likely contains GO terms and their definitions, including information on biological processes, cellular components, and molecular functions.\n",
    "\n",
    "`tabula-muris-comet.h5ad` (`2.3 GB`): The \".h5ad\" extension suggests this file is an AnnData file, a format commonly used in bioinformatics for storing large annotated datasets, particularly single-cell data. AnnData files are based on the HDF5 file format, which is designed for storing and organizing large amounts of data. This particular file likely contains the main single-cell RNA sequencing data from the Tabula Muris project, including gene expression measurements for individual cells, metadata about the cells, and possibly additional layers of data like spliced/unspliced gene counts or quality metrics.\n",
    "\n",
    "As the combined size of the files is pretty large, even loading in the data after downloading takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics on the dataset\n",
    "print(f\"ℹ️ Tabula Muris dataset has {len(tm_train)} train samples and {len(tm_test)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample\n",
    "tr_smp, tr_trg = tm_train[0]\n",
    "te_smp, te_trg = tm_test[0]\n",
    "\n",
    "print(f\"Training sample shape: {tr_smp.shape} and target {tr_trg}\")\n",
    "print(f\"Test sample shape: {te_smp.shape} and target {te_trg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batches\n",
    "tm_train_loader = tm_train.get_data_loader()\n",
    "tm_test_loader = tm_test.get_data_loader()\n",
    "\n",
    "\n",
    "# Get batch\n",
    "tr_smps, tr_trgs = next(iter(tm_train_loader))\n",
    "te_smps, te_trgs = next(iter(tm_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Demo: TMSimpleDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwissProt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.prot.swissprot import SPSimpleDataset, SPSetDataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
