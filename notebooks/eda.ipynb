{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "This few-shot benchmark tests various meta-learning methods in the context of\n",
    "biomedical applications. In particular, we are dealing with the [Tabula Muris]()\n",
    "and [SwissProt]() datasets. One is a cell type classification task based on\n",
    "single-cell gene expressions and the other is a protein function prediction task\n",
    "based on protein sequences. The goal of this notebook is to explore basic\n",
    "statistics about the two datasets, as well as understand how the data loading is\n",
    "implemented for the episodic training during meta-training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "First, let's import the relevant modules needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.782504Z",
     "start_time": "2023-11-24T00:16:39.613357Z"
    }
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Module imports\n",
    "import sys\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.808678Z",
     "start_time": "2023-11-24T00:16:39.783591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classes\n",
    "\n",
    "---\n",
    "\n",
    "Both datasets are implemented as subclasses of the `FewShotDataset` class and\n",
    "use some other generic utility classes. We will explore these here in detail.\n",
    "They are all defined in the `datasets.dataset` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotDataset\n",
    "\n",
    "The `FewShotDataset(torch.utils.data.Dataset)` is the base class for all\n",
    "few-shot datasets. It implements the `__getitem__` and `__len__` methods and has\n",
    "some utilities for checking the data validty. Furthermore, it is responsible for\n",
    "loading and extracting the dataset into the `root` directory if specified and\n",
    "not yet existent. However, as it is a abstract base class, it cannot be\n",
    "instantiated, e.g. it requires the `_dataset_name` and `_dataset_dir` as class\n",
    "attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.834211Z",
     "start_time": "2023-11-24T00:16:39.803639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Fails with error FewShotDataset must have attribute _dataset_name..\n"
     ]
    }
   ],
   "source": [
    "# Demo: FewShotDataset\n",
    "from datasets.dataset import FewShotDataset  # noqa\n",
    "\n",
    "try:\n",
    "    few_shot_dataset = FewShotDataset()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fails with error {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.894681Z",
     "start_time": "2023-11-24T00:16:39.827479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo FewShotSubDataset\n",
    "from datasets.dataset import FewShotSubDataset  # noqa\n",
    "\n",
    "# Create a random dataset with 100 samples, 5 features and 5 classes\n",
    "samples = torch.rand(100, 5)\n",
    "targets = torch.randint(0, 5, (100,))  # 5-way\n",
    "subset_target = 4\n",
    "\n",
    "# Get all samples that belong to class 4\n",
    "subset_samples = samples[targets == subset_target]\n",
    "\n",
    "# Create a few-shot dataset for class 4\n",
    "few_shot_sub_dataset = FewShotSubDataset(subset_samples, subset_target)\n",
    "\n",
    "# Sanity checks\n",
    "assert (\n",
    "    len(few_shot_sub_dataset) == (targets == subset_target).sum()\n",
    "), \"❌ Length of few-shot dataset is not correct.\"\n",
    "assert (\n",
    "    few_shot_sub_dataset.dim == samples.shape[1]\n",
    "), \"❌ Dimension of few-shot dataset is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Batch Sampler\n",
    "\n",
    "The `EpisodicBatchSampler` is a utility class that randomly samples `n_way`\n",
    "classes (out of a total of `n_classes`) for a total of `n_episodes`. It can be\n",
    "used in episodic training to sample the classes used in each episode.\n",
    "\n",
    "The sampler is `n_episodes` long and each time samples randomly (without\n",
    "replacement) from `{0, ..., n_classes-1}` `n_way` times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:39.895795Z",
     "start_time": "2023-11-24T00:16:39.856954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 3, Ways: 5, Classes: 10\n",
      "Episode 1 w/ classes [5 8 1 2 6]\n",
      "Episode 2 w/ classes [9 5 6 7 8]\n",
      "Episode 3 w/ classes [2 0 3 8 5]\n"
     ]
    }
   ],
   "source": [
    "# Demo: EpisodicBatchSampler\n",
    "from datasets.dataset import EpisodicBatchSampler  # noqa\n",
    "\n",
    "# Demo of EpisodicBatchSampler\n",
    "n_episodes, n_way, n_classes = 3, 5, 10\n",
    "episodic_batch_sampler = EpisodicBatchSampler(n_classes, n_way, n_episodes)\n",
    "\n",
    "print(f\"Episodes: {n_episodes}, Ways: {n_way}, Classes: {n_classes}\")\n",
    "for batch_idx, indices in enumerate(episodic_batch_sampler):\n",
    "    print(f\"Episode {batch_idx+1} w/ classes {indices.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabula Muris\n",
    "\n",
    "---\n",
    "\n",
    "**Tabula Muris** is a dataset of single cell transcriptome data (gene\n",
    "expressions) from mice, containing nearly `100,000` cells from `20` organs and\n",
    "tissues. The data allow for direct and controlled comparison of gene expression\n",
    "in cell types shared between tissues, such as immune cells from distinct\n",
    "anatomical locations. They also allow for a comparison of two distinct technical\n",
    "approaches:\n",
    "\n",
    "_More Resources_:\n",
    "\n",
    "- [Tabular Muris Website](https://tabula-muris.ds.czbiohub.org/)\n",
    "- [SF Biohub Article](https://www.czbiohub.org/sf/tabula-muris/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMSimpleDataset\n",
    "\n",
    "The `TMSimpleDataset` is a simple dataset class that is designed for regular\n",
    "multi-class classification training/ fine-tuning. It loads the entire\n",
    "(processed) dataset into memory and wraps inside a PyTorch Dataset object.\n",
    "Supports functionality for retrieving a single sample, a batched data loader and\n",
    "the dimensionality of the data.\n",
    "\n",
    "_Note: Upon first call, the `TMSimpleDataset` class will download the data into\n",
    "the `root` directory._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:50.756355Z",
     "start_time": "2023-11-24T00:16:39.875875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TabulaMuris train split (65812) loaded in 4.97 seconds.\n",
      "✅ TabulaMuris val split (14962) loaded in 2.08 seconds.\n",
      "✅ TabulaMuris test split (25065) loaded in 2.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Demo: TMSimpleDataset\n",
    "from datasets.cell.tabula_muris import TMSimpleDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "batch_size = 10\n",
    "root = \"../data\"\n",
    "min_samples = 20\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"root\": root,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "# Controls data split (returns subset of tissue types)\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Initialise TabulaMuris training dataset\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = TMSimpleDataset(**kwargs, mode=mode)\n",
    "    print(\n",
    "        f\"✅ TabulaMuris {mode} split ({len(data)}) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the raw data is downloaded and saved onto disk. Let's view the size of the\n",
    "raw and processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:51.179635Z",
     "start_time": "2023-11-24T00:16:50.751398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84M\t../data/tabula_muris/gene_association.mgi\r\n",
      " 32M\t../data/tabula_muris/go-basic.obo\r\n",
      "3.2G\t../data/tabula_muris/processed\r\n",
      "2.3G\t../data/tabula_muris/tabula-muris-comet.h5ad\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11732) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# This asssumes the data is already downloaded\n",
    "!du -sh ../data/tabula_muris/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:51.346564Z",
     "start_time": "2023-11-24T00:16:51.184369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11733) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0K\t../data/tabula_muris/processed/mapping.pkl\r\n",
      "3.2G\t../data/tabula_muris/processed/tabula-muris.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../data/tabula_muris/processed/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, let's take a look at the data. We will only use the training set for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:57.259225Z",
     "start_time": "2023-11-24T00:16:51.335762Z"
    }
   },
   "outputs": [],
   "source": [
    "data = TMSimpleDataset(**kwargs, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:57.358695Z",
     "start_time": "2023-11-24T00:16:57.262689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Tabula Muris dataset has 65812 train samples.\n",
      "ℹ️ Each sample has 2866 features (gene expression levels).\n",
      "ℹ️ Tabula Muris dataset has 57 unique classes.\n"
     ]
    }
   ],
   "source": [
    "# Statistics on the data splits\n",
    "num_samples = len(data)\n",
    "dim = data.dim\n",
    "unique_classes = np.unique(data.targets)\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "print(f\"ℹ️ Tabula Muris dataset has {num_samples} train samples.\")\n",
    "print(f\"ℹ️ Each sample has {dim} features (gene expression levels).\")\n",
    "print(f\"ℹ️ Tabula Muris dataset has {num_classes} unique classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sample by indexing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:59.094545Z",
     "start_time": "2023-11-24T00:16:57.323802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample shape: (2866,) and target 17\n"
     ]
    }
   ],
   "source": [
    "# Get sample by indexing\n",
    "x, y = data[0]\n",
    "\n",
    "print(f\"Training sample shape: {x.shape} and target {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense. Each sample in the processed data has 2866 features (gene\n",
    "expression levels) and a single integer indicating the target tissue type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:16:59.326447Z",
     "start_time": "2023-11-24T00:16:57.351505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Features torch.Size([10, 2866]) and target tensor([32, 24,  0, 26, 38, 39, 48,  0,  0, 16], dtype=torch.int32)\n",
      "Batch 2: Features torch.Size([10, 2866]) and target tensor([51, 30, 33, 26, 39, 39, 23, 12, 16, 39], dtype=torch.int32)\n",
      "Batch 3: Features torch.Size([10, 2866]) and target tensor([39, 24, 16, 39, 39, 48, 39, 39,  0, 48], dtype=torch.int32)\n",
      "Batch 4: Features torch.Size([10, 2866]) and target tensor([37, 33, 39, 11, 42, 55,  0, 16, 23, 17], dtype=torch.int32)\n",
      "Batch 5: Features torch.Size([10, 2866]) and target tensor([51, 18, 17, 12, 38, 40, 39, 17, 16, 16], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Get data loader\n",
    "data_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Create iterator\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Get five batches\n",
    "for batch_idx in range(5):\n",
    "    tr_smps, tr_trgs = next(data_iter)\n",
    "    print(f\"Batch {batch_idx+1}: Features {tr_smps.shape} and target {tr_trgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB 1:** On CPU setting `num_workers=0` and `pin_memory=False` is recommended\n",
    "and loads the batches instant. However, the original parameters in the code are\n",
    "`num_workers=4` and `pin_memory=True` which is likely GPU optimised. To have\n",
    "full control, the signature of the `get_data_loader` method has been changed to\n",
    "allow for customisation of the data loader.\n",
    "\n",
    "**NB 2:** Shuffling the data is crucial during training for regular fine-tuning.\n",
    "The raw data does not seem to be shuffled well which will likely hurt the\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMSetDataset\n",
    "\n",
    "The `TMSetDataset` is designed for few-shot learning. Most configurations are\n",
    "the same as for the `TMSimpleDataset`, but crucially the dataset class will\n",
    "return an episodic batch sampler based on the `n_way`, `n_support`, `n_query`\n",
    "and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:12.080431Z",
     "start_time": "2023-11-24T00:16:59.065850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TMSetDataset train split loaded in 8.39 seconds.\n",
      "✅ TMSetDataset val split loaded in 2.29 seconds.\n",
      "✅ TMSetDataset test split loaded in 3.71 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.cell.tabula_muris import TMSetDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "# Controls data split (returns subset of tissue types)\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    TMSetDataset(**kwargs, mode=mode)\n",
    "\n",
    "    print(\n",
    "        f\"✅ TMSetDataset {mode} split loaded in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few-shot learning dataset, a single \"sample\" is defined not as the feature\n",
    "vector, target tuple of the gene expression levels and the target tissue but as\n",
    "a set of support and query samples within a class. Thus, the `__getitem__`\n",
    "method returns a tuple of the support and query samples and targets for the\n",
    "`i`-th class. Thus, the returned tensor dimension will be\n",
    "`(n_support + n_query, n_features)` for the samples and\n",
    "`(n_support + n_query, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:17.913720Z",
     "start_time": "2023-11-24T00:17:12.073803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = TMSetDataset(**kwargs, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:17.974538Z",
     "start_time": "2023-11-24T00:17:17.904882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples shape: torch.Size([6, 2866]) and target tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "Support samples shape: torch.Size([3, 2866]) and target tensor([0, 0, 0], dtype=torch.int32)\n",
      "Query samples shape: torch.Size([3, 2866]) and target tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Get sample by indexing\n",
    "tr_smp, tr_trg = data[0]\n",
    "\n",
    "# Support samples and target\n",
    "sup_tr_smp, sup_tr_trg = tr_smp[:n_support], tr_trg[:n_support]\n",
    "\n",
    "# Query samples and target\n",
    "que_tr_smp, que_tr_trg = tr_smp[n_support:], tr_trg[n_support:]\n",
    "\n",
    "print(f\"Training samples shape: {tr_smp.shape} and target {tr_trg}\")\n",
    "print(f\"Support samples shape: {sup_tr_smp.shape} and target {sup_tr_trg}\")\n",
    "print(f\"Query samples shape: {sup_tr_smp.shape} and target {sup_tr_trg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader class combines the support-query sampler per class (as\n",
    "defined above) and the `EpisodicBatchSampler` to create a data loader that\n",
    "returns batches of episodes where each time we get `n_way` classes with\n",
    "`n_support` support samples and `n_query` query samples per class. First, the\n",
    "episodic batch sample samples the `n_way` random class indices and then the\n",
    "support-query sampler samples the support and query samples for each class.\n",
    "Thus, the final tensor shapes will be `(n_way, n_query + n_support, n_features)`\n",
    "for the samples and `(n_way, n_query + n_support, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:18.224954Z",
     "start_time": "2023-11-24T00:17:17.950859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch shape: torch.Size([5, 6, 2866]) and target torch.Size([5, 6])\n",
      "Targets:\n",
      "tensor([[24, 24, 24, 24, 24, 24],\n",
      "        [31, 31, 31, 31, 31, 31],\n",
      "        [10, 10, 10, 10, 10, 10],\n",
      "        [46, 46, 46, 46, 46, 46],\n",
      "        [ 1,  1,  1,  1,  1,  1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Get data loader\n",
    "train_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Get batch\n",
    "tr_smps, tr_trgs = next(iter(train_loader))\n",
    "\n",
    "print(f\"Training batch shape: {tr_smps.shape} and target {tr_trgs.shape}\")\n",
    "print(f\"Targets:\\n{tr_trgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB 1:** Shuffling in meta-learning tasks is not necessary because the episodic\n",
    "batch sampler and the sub-class sampler are already random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwissProt\n",
    "\n",
    "---\n",
    "\n",
    "SWISS-PROT is an annotated protein sequence database, which was created at the\n",
    "Department of Medical Biochemistry of the University of Geneva (first started\n",
    "1987). In SWISS-PROT two classes of data can be distinguished: the core data and\n",
    "the annotation. For each sequence entry the core data consists of the sequence\n",
    "data; the citation information (bibliographical references) and the taxonomic\n",
    "data (description of the biological source of the protein), while the annotation\n",
    "consists of the description of the following items:\n",
    "\n",
    "- Function(s) of the protein\n",
    "- Post-translational modification(s). For example carbohydrates,\n",
    "  phosphorylation, acetylation, GPI-anchor, etc.\n",
    "- Domains and sites. For example calcium binding regions, ATP-binding sites,\n",
    "  zinc fingers, homeoboxes, SH2 and SH3 domains, etc.\n",
    "- Secondary structure. For example alpha helix, beta sheet, etc.\n",
    "- Quaternary structure. For example homodimer, heterotrimer, etc.\n",
    "- Similarities to other proteins\n",
    "- Disease(s) associated with deficiencie(s) in the protein\n",
    "- Sequence conflicts, variants, etc.\n",
    "\n",
    "Within this project we will focus on the function annotation of proteins, thus\n",
    "given the protein sequence (string of amino acids) we want to predict the\n",
    "function of the protein.\n",
    "\n",
    "_More Resources_:\n",
    "\n",
    "- [National Library of Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102476/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPSimpleDataset\n",
    "\n",
    "The `SPSimpleDataset` is a simple dataset class that is designed for regular\n",
    "multi-class classification training/ fine-tuning. It loads the entire\n",
    "(processed) dataset into memory and wraps inside a PyTorch Dataset object.\n",
    "Supports functionality for retrieving a single sample, a batched data loader and\n",
    "the dimensionality of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:18.269243Z",
     "start_time": "2023-11-24T00:17:17.980370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111M\t../data/swissprot/embeds\r\n",
      " 47M\t../data/swissprot/filtered_goa_uniprot_all_noiea.gaf\r\n",
      " 30M\t../data/swissprot/go-basic.obo\r\n",
      " 74M\t../data/swissprot/processed\r\n",
      "8.2M\t../data/swissprot/sprot_ancestors.txt\r\n",
      "271M\t../data/swissprot/uniprot_sprot.fasta\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11747) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# The data has to be manually downloaded to this folder\n",
    "!du -sh ../data/swissprot/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:06.367946Z",
     "start_time": "2023-11-24T00:26:02.518382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SwissProt train split (10795 samples) loaded in 1.49 seconds.\n",
      "✅ SwissProt val split (1139 samples) loaded in 1.30 seconds.\n",
      "✅ SwissProt test split (1183 samples) loaded in 1.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.prot.swissprot import SPSimpleDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "batch_size = 10\n",
    "min_samples = 6\n",
    "\n",
    "kwargs = {\n",
    "    \"root\": root,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"min_samples\": min_samples,\n",
    "}\n",
    "\n",
    "# Show loading time for each split\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = SPSimpleDataset(**kwargs, mode=mode)\n",
    "\n",
    "    print(\n",
    "        f\"✅ SwissProt {mode} split ({len(data)} samples) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:19:14.670958Z",
     "start_time": "2023-11-24T00:19:14.506163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74M\t../data/swissprot/processed\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11920) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# This asssumes the data is downloaded manually\n",
    "!du -sh ../data/swissprot/processed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we can load the data fast and easy. The class also supports subsetting\n",
    "which is useful for debugging the training. However, because we are still\n",
    "loading all the data into memory before subsetting, we do not get any speed up\n",
    "(_NB: We could decide to save and load splitted data to disk, but this is not\n",
    "implemented yet_).\n",
    "\n",
    "**NB:** The SwissProt dataset as-is is relatively small and can be loaded into\n",
    "memory without many problems. In this case, subsetting the data decreases the\n",
    "number of classes that are included according to our specification of the\n",
    "minimum number of classes and is therefore generally not advised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the full training split again and do some exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:09.974262Z",
     "start_time": "2023-11-24T00:26:08.630771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ SwissProt train split has 10795 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ SwissProt train split has 168 classes.\n"
     ]
    }
   ],
   "source": [
    "# Demo: SPSimpleDataset\n",
    "data = SPSimpleDataset(**kwargs, mode=\"train\")\n",
    "\n",
    "# Get some basic statistics\n",
    "num_samples = len(data)\n",
    "dim = data.dim\n",
    "classes = np.unique([smp.annot for smp in data.samples])\n",
    "unique_classes = len(classes)\n",
    "\n",
    "print(f\"ℹ️ SwissProt train split has {len(data)} samples\")\n",
    "print(f\"ℹ️ Each sample is an encoded protein sequence of length {dim}\")\n",
    "print(f\"ℹ️ SwissProt train split has {unique_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a single sample by indexing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:26:23.605545Z",
     "start_time": "2023-11-24T00:26:23.567681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample (encoded protein sequence): torch.Size([1280]) and target 1474\n"
     ]
    }
   ],
   "source": [
    "# Sample by indexing\n",
    "x, y = data[0]\n",
    "\n",
    "print(f\"Training sample (encoded protein sequence): {x.shape} and target {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get a regular batch by using the data loader returned by the\n",
    "`get_data_loader` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:04.584714Z",
     "start_time": "2023-11-24T00:27:04.532062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Sequence: torch.Size([10, 1280]) and target tensor([1239,  736, 1789, 3111, 4164, 1508, 2251, 1042, 2251, 1060])\n",
      "Batch 2 Sequence: torch.Size([10, 1280]) and target tensor([1044, 2951, 1520, 1520, 1060,  736, 1789, 1789, 1796, 1789])\n",
      "Batch 3 Sequence: torch.Size([10, 1280]) and target tensor([ 474, 1042,  960, 4501, 1044, 1524, 1044, 1044, 4118, 1380])\n",
      "Batch 4 Sequence: torch.Size([10, 1280]) and target tensor([1812, 1044, 2145,  736,  736, 1044,  731, 3538, 1789, 1789])\n",
      "Batch 5 Sequence: torch.Size([10, 1280]) and target tensor([1044, 1087,  731, 1042, 1789, 1789, 1380, 1396, 1042, 1789])\n"
     ]
    }
   ],
   "source": [
    "# Sample using data loader (use pin_memory=True if using GPU)\n",
    "data_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Create iterator\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Get five batches\n",
    "for batch_idx in range(5):\n",
    "    xb, yb = next(data_iter)\n",
    "    print(f\"Batch {batch_idx+1} Sequence: {xb.shape} and target {yb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPSetDataset\n",
    "\n",
    "The `SPSetDataset` is designed for few-shot learning. Most configurations are\n",
    "the same as for the `SPSimpleDataset`, but crucially the dataset class will\n",
    "return an episodic batch sampler based on the `n_way`, `n_support`, `n_query`\n",
    "and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:36.427002Z",
     "start_time": "2023-11-24T00:27:33.050981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SPSetDataset train split (168 class data loaders) loaded in 1.09 seconds.\n",
      "✅ SPSetDataset val split (33 class data loaders) loaded in 1.24 seconds.\n",
      "✅ SPSetDataset test split (18 class data loaders) loaded in 1.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Demo: TMSetDataset\n",
    "from datasets.prot.swissprot import SPSetDataset  # noqa\n",
    "\n",
    "# Arguments to provide\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0\n",
    "\n",
    "kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "for mode in modes:\n",
    "    start = time.time()\n",
    "    data = SPSetDataset(**kwargs, mode=mode)\n",
    "    print(\n",
    "        f\"✅ SPSetDataset {mode} split ({len(data)} class data loaders) loaded in {time.time() - start:.2f} seconds.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the full training split again and do some exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:40.930550Z",
     "start_time": "2023-11-24T00:27:39.627158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demo: SPSetDataset\n",
    "data = SPSetDataset(**kwargs, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:27:45.424218Z",
     "start_time": "2023-11-24T00:27:45.401204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ SwissProt train split has 168 classes\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n"
     ]
    }
   ],
   "source": [
    "# Get some basic statistics\n",
    "num_samples = len(data)\n",
    "dim = data.dim\n",
    "\n",
    "print(f\"ℹ️ SwissProt train split has {len(data)} classes\")\n",
    "print(f\"ℹ️ Each sample is an encoded protein sequence of length {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in few-shot learning a single \"sample\" is defined not as the feature\n",
    "vector, target tuple but as a set of support and query samples within a class.\n",
    "Thus, the `__getitem__` method returns a tuple of the support and query samples\n",
    "and targets for the `i`-th class. Thus, the returned tensor dimension will be\n",
    "`(n_support + n_query, n_features)` for the samples and\n",
    "`(n_support + n_query, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:28:54.263329Z",
     "start_time": "2023-11-24T00:28:54.232381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample (encoded protein sequence): feature shape torch.Size([6, 1280]) and target shape torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Sample by indexing\n",
    "x, y = data[0]\n",
    "\n",
    "print(\n",
    "    f\"Training sample (encoded protein sequence): feature shape {x.shape} and target shape {y.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader class combines the support-query sampler per class (as\n",
    "defined above) and the `EpisodicBatchSampler` to create a data loader that\n",
    "returns batches of episodes where each time we get `n_way` classes with\n",
    "`n_support` support samples and `n_query` query samples per class. First, the\n",
    "episodic batch sample samples the `n_way` random class indices and then the\n",
    "support-query sampler samples the support and query samples for each class.\n",
    "Thus, the final tensor shapes will be `(n_way, n_query + n_support, n_features)`\n",
    "for the samples and `(n_way, n_query + n_support, )` for the targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:29:47.161915Z",
     "start_time": "2023-11-24T00:29:47.126149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 Sequence: torch.Size([5, 6, 1280]) and target torch.Size([5, 6])\n",
      "Target\n",
      "tensor([[4115, 4115, 4115, 4115, 4115, 4115],\n",
      "        [3218, 3218, 3218, 3218, 3218, 3218],\n",
      "        [3237, 3237, 3237, 3237, 3237, 3237],\n",
      "        [ 960,  960,  960,  960,  960,  960],\n",
      "        [1812, 1812, 1812, 1812, 1812, 1812]])\n"
     ]
    }
   ],
   "source": [
    "# Sample using data loader (use pin_memory=True if using GPU)\n",
    "data_loader = data.get_data_loader(num_workers=0, pin_memory=False)\n",
    "\n",
    "# Get one batches\n",
    "xb, yb = next(iter(data_loader))\n",
    "print(f\"Batch {batch_idx+1} Sequence: {xb.shape} and target {yb.shape}\")\n",
    "print(\"Target\")\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Explore Raw Tabula Muris Data\n",
    "\n",
    "**Note: Code here doesn't run anymore**\n",
    "\n",
    "The `MacaData` class is responsible for loading and processing the Tabula Muris\n",
    "dataset. Thus, before looking at the `TMSimpleDataset` and `TMSetDataset`, let's\n",
    "investigate the data loading/ processing first.\n",
    "\n",
    "**Changes to the original implementation:**\n",
    "\n",
    "Originally, the class loads the entire dataset and processes it within the\n",
    "constructor. This comes with several limitations:\n",
    "\n",
    "1. We cannot easily look at the raw data.\n",
    "\n",
    "2. We have to load and preprocess the entire dataset, even if we just want to\n",
    "   use samples within a specific data split.\n",
    "\n",
    "3. It does support any subsampling.\n",
    "\n",
    "To account for this, the `MacaData` class has been augmented in the following\n",
    "way.\n",
    "\n",
    "1. Data processing is not performed inside the constructor but has to be called\n",
    "   via public method `process_data`.\n",
    "\n",
    "2. The processed data may now be saved to disk via the public method\n",
    "   `save_data`.\n",
    "\n",
    "To support efficient loading of subsets and splits of the data, we later also\n",
    "implement the `MacaDataLoader` class which will be used to load the data during\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data\n",
    "\n",
    "We first look at the raw data. The `MacaData` class expects the path to a\n",
    "`.h5ad` file containing the data as input and loads the data as well as computes\n",
    "the class mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.654974Z",
     "start_time": "2023-11-24T00:17:29.603345Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets.cell.utils import MacaData  # noqa\n",
    "#\n",
    "# path = os.path.join(\"..\", \"data\", \"tabula_muris\", \"tabula-muris-comet.h5ad\")\n",
    "#\n",
    "# start = time.time()\n",
    "# maca_data = MacaData(path=path)\n",
    "# print(f\"⌛ Loaded data in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.656722Z",
     "start_time": "2023-11-24T00:17:29.622063Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Save attributes\n",
    "#       raw_data = maca_data.adata\n",
    "#       trg2idx, idx2trg = maca_data.trg2idx, maca_data.idx2trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MacaData` class stores the loaded data in the attribute `adata` (annotated\n",
    "data) as an object of type `anndata.AnnData`. It is a data structure that stores\n",
    "the data including annotations which is often used for bioinformatics data. We\n",
    "can get detailled information about the data by printing the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.673064Z",
     "start_time": "2023-11-24T00:17:29.639834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print meta-data of entire dataset\n",
    "#       raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the annotation for each cell (sample) and each gene (feature) by\n",
    "accessing the `obs` and `var` attributes of the `anndata.AnnData` object. The\n",
    "`obs` attribute is a `pandas.DataFrame` with the cell annotations and the `var`\n",
    "attribute is a `pandas.DataFrame` with the gene annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.683681Z",
     "start_time": "2023-11-24T00:17:29.659483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell annotations\n",
    "#       raw_data.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the meta data of each cell in the dataset. The meta data contains\n",
    "information about the mic (like the id, gender, age, etc.) and the cell type\n",
    "(like the cell type, (sub-)tissue, etc.) and much more. There are a total of\n",
    "105.960 cells in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.697427Z",
     "start_time": "2023-11-24T00:17:29.674833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gene annotations\n",
    "#       raw_data.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.709373Z",
     "start_time": "2023-11-24T00:17:29.692173Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "#       feature_matrix = raw_data.X\n",
    "#       targets = raw_data.obs[\"label\"].cat.codes.to_numpy()\n",
    "#\n",
    "#       print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "#       print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the distribution of the target tissues by showing the top 10\n",
    "most frequent tissues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.730372Z",
     "start_time": "2023-11-24T00:17:29.709967Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Plot Cell Type Distribution\n",
    "#       _, ax = plt.subplots(figsize=(20, 10))\n",
    "#       cell_types = [maca_data.idx2trg[idx] for idx in targets]\n",
    "#\n",
    "#       top_k = 10\n",
    "#       counts = collections.Counter(cell_types)\n",
    "#       counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "#\n",
    "#       sns.barplot(x=list(counts.keys()), y=list(counts.values()), palette=\"mako\", ax=ax)\n",
    "#       ax.set(\n",
    "#           xlabel=\"Cell type\", ylabel=\"Count\", title=f\"Cell Type Distribution (Top {top_k})\"\n",
    "#       )\n",
    "#       ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample we record the gene expression levels for all genes. The gene\n",
    "annotation contains some summary statistics about the expressivitiy of each\n",
    "genes as meta data. The index in this data frame is the column names in the gene\n",
    "expression feature matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processed Data\n",
    "\n",
    "Let's preprocess the data according to the original implementation. The\n",
    "following steps are performed in the `process_data` method:\n",
    "\n",
    "- Filter out cells with no target\n",
    "- Filter out genes that are expressed in less than 5 cells\n",
    "- Filter out cells with less than 5000 counts and 500 genes expressed\n",
    "- Normalize per cell (simple lib size normalization)\n",
    "- Filter out genes with low dispersion (retain the once with high variance)\n",
    "- Log transform and scale the data\n",
    "- Zero-imputation of Nans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.745505Z",
     "start_time": "2023-11-24T00:17:29.727746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "#       start = time.time()\n",
    "#       maca_data.process_data()\n",
    "#       print(f\"⌛ Processed data in {time.time() - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.765974Z",
     "start_time": "2023-11-24T00:17:29.745631Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Save attributes\n",
    "#       processed_data = maca_data.adata\n",
    "#       trg2idx, idx2trg = maca_data.trg2idx, maca_data.idx2trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.781434Z",
     "start_time": "2023-11-24T00:17:29.763839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell annotations\n",
    "#       processed_data.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.800711Z",
     "start_time": "2023-11-24T00:17:29.781561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gene annotations\n",
    "#       processed_data.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.817370Z",
     "start_time": "2023-11-24T00:17:29.799603Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can get the features and targets as numpy arrays (this is done in the TMDataset class as well)\n",
    "# feature_matrix = processed_data.X\n",
    "#       targets = processed_data.obs[\"label\"].cat.codes.to_numpy()\n",
    "#\n",
    "#       print(f\"Feature matrix: {feature_matrix.shape}, Targets: {targets.shape}\")\n",
    "#       print(f\"Number of target tissues: {len(np.unique(targets))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T00:17:29.835678Z",
     "start_time": "2023-11-24T00:17:29.817567Z"
    }
   },
   "outputs": [],
   "source": [
    "#       # Plot Cell Type Distribution\n",
    "#       _, ax = plt.subplots(figsize=(20, 10))\n",
    "#       cell_types = [maca_data.idx2trg[idx] for idx in targets]\n",
    "#\n",
    "#       top_k = 10\n",
    "#       counts = collections.Counter(cell_types)\n",
    "#       counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_k])\n",
    "#\n",
    "#       sns.barplot(x=list(counts.keys()), y=list(\n",
    "#           counts.values()), palette=\"mako\", ax=ax)\n",
    "#       ax.set(\n",
    "#           xlabel=\"Cell type\", ylabel=\"Count\", title=f\"Cell Type Distribution (Top {top_k})\"\n",
    "#       )\n",
    "#       ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
