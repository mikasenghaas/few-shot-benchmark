{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore the use of different `methods`. For that,\n",
    "we will be using `SwissProt` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "import sys\n",
    "sys.path.insert(0, '..') # add directory above current directory to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EXISTS: go-basic.obo\n",
      "go-basic.obo: fmt(1.2) rel(2023-11-15) 46,228 Terms; optional_attrs(relationship)\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Module imports\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Custom Modules imports\n",
    "from datasets.prot.swissprot import SPSimpleDataset, SPSetDataset  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we load the data from the `SwissProt` database which is the smaller of the two datasets. We will be using both the regular dataloader for standard few shot finetuning as well as episodic dataloader for episodic finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 11722 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ train split has 182 classes.\n",
      "\n",
      "ℹ️ val split has 600 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ val split has 26 classes.\n",
      "\n",
      "ℹ️ test split has 652 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ test split has 12 classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "batch_size = 10\n",
    "min_samples = 6\n",
    "\n",
    "kwargs = {\n",
    "    \"root\": root,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"min_samples\": min_samples,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "r_datasets = [SPSimpleDataset(**kwargs, mode=mode) for mode in modes]\n",
    "r_train, r_val, r_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in r_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(r_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} samples\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print(f\"ℹ️ {mode} split has {len(np.unique([smp.annot for smp in split.samples]))} classes.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 182 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ val split has 26 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ test split has 12 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0 # Load full dataset\n",
    "\n",
    "kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "e_datasets = [SPSetDataset(**kwargs, mode=mode) for mode in modes]\n",
    "e_train, e_val, e_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in e_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(e_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} samples\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose backbone\n",
    "# bm1 = fcnet.FCNet(x_dim=X.shape[-1])\n",
    "\n",
    "# # Define the method\n",
    "# m1 = baseline.Baseline(\n",
    "    # backbone=bm1, \n",
    "    # n_way=3, \n",
    "    # n_support=15, \n",
    "    # n_classes=len(dataset.categories),\n",
    "    # loss=\"softmax\",\n",
    "    # type=\"classification\"\n",
    "# )\n",
    "\n",
    "# # Define the optimizer for obtaining training\n",
    "# optimizer = optim.AdamW(m1.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(2):\n",
    "    # m1.train_loop(epoch, loader, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
