{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore the use of different `methods`. For that,\n",
    "we will be using `SwissProt` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "import sys\n",
    "sys.path.insert(0, '..') # add directory above current directory to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Module imports\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Custom Modules imports\n",
    "# - Datasets loading\n",
    "from datasets.prot.swissprot import SPSimpleDataset, SPSetDataset  # noqa\n",
    "\n",
    "# - Backbone of for this notebook\n",
    "from backbones.fcnet import FCNet  # noqa\n",
    "\n",
    "# - Methods\n",
    "from methods.baseline import Baseline  # noqa\n",
    "from methods.protonet import ProtoNet  # noqa\n",
    "from methods.matchingnet import MatchingNet  # noqa\n",
    "from methods.maml import MAML  # noqa\n",
    "from methods.self_optimal_transport import SOT # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# Use SOT\n",
    "USE_SOT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we load the data from the `SwissProt` database which is the smaller of the two datasets. We will be using both the regular dataloader for standard few shot finetuning as well as episodic dataloader for episodic finetuning.\n",
    "\n",
    "**NB 1:** Evaluation can be done on both types of loaders. However, when it comes to training, `baseline` method is trained on regular dataloader, `episodic` method is trained on episodic dataloader.\n",
    "\n",
    "**NB 2:** what is the correct way of getting samples `get_samples` or `get_samples_via_ic`?\n",
    "\n",
    "**NB 3:** We should make sure we all have the same preprocessed dataset since they they randomly choose the `GO` label for each protein out of all that are on level 5 if using `get_samples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 11072 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ train split has 189 classes.\n",
      "\n",
      "ℹ️ val split has 1230 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ val split has 26 classes.\n",
      "\n",
      "ℹ️ test split has 642 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ test split has 11 classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "batch_size = 10\n",
    "min_samples = 6\n",
    "\n",
    "rdata_kwargs = {\n",
    "    \"root\": root,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"use_ic_selection\": True,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "r_datasets = [SPSimpleDataset(**rdata_kwargs, mode=mode) for mode in modes]\n",
    "r_train, r_val, r_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in r_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(r_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} samples\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print(f\"ℹ️ {mode} split has {len(np.unique([smp.annot for smp in split.samples]))} classes.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 189 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ val split has 26 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ test split has 11 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0 # Load full dataset\n",
    "\n",
    "edata_kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "    \"use_ic_selection\": True,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "e_datasets = [SPSetDataset(**edata_kwargs, mode=mode) for mode in modes]\n",
    "e_train, e_val, e_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in e_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(e_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} number of classes.\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone choice\n",
    "\n",
    "---\n",
    "\n",
    "Before we initiliase our method, we need to choose which backbone we want to use. Backbone is simply a feature extractor that transforms the raw input embeddings to a more useful version which can then be later fed into our method of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the backbone works, we will run it on random batch of our data and see the output. For the regular dataloader, we can just take the output and feed it into the backbone as is since it is already in the expected shape. \n",
    "\n",
    "However, for the episodic dataloader, we obtain a tensor of shape `(n_way, n_support + n_query, hidden dim)` which we need to remap to `(n_way * (n_support * n_query), hidden dim)` before feeding it into the backbone. This is because the backbone expects a tensor of shape `(batch size, hidden dim)`. Thus, you can think of the mapping as basically flattening along the second dimension of the input tensor.\n",
    "\n",
    "**NB:** In the `MetaTemplate` class, and specifically its `parse_feature` method, the flattening is performed as:\n",
    "\n",
    "```python\n",
    "x = x.view(n_way * (n_support + n_query), *x.size()[2:])\n",
    "```\n",
    "\n",
    "This makes indeed only sense if we for instance have more than 1 last dimension, e.g. the input tensor would be `(n_way, n_support + n_query, seq, hidden dim)`. Based on the [eda notebook](eda.ipynb), we know that the input tensor is of shape `(n_way, n_support + n_query, hidden dim)` and thus we can simply flatten along the second dimension as:\n",
    "\n",
    "```python\n",
    "x = x.view(n_way * (n_support + n_query), -1)\n",
    "```\n",
    "\n",
    "With that being said, for the purpose of generalisation, we can keep the original code. TODO: discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch from the regular train loader\n",
    "r_seq, r_lab = next(iter(r_train))\n",
    "print(f\"ℹ️ r_batch has {len(r_seq)} samples each with dim {r_seq.shape[1]}\")\n",
    "\n",
    "# Sample a batch from the episodic train loader\n",
    "e_seq, e_lab = next(iter(e_train))\n",
    "n_way, n_support, n_query = edata_kwargs[\"n_way\"], edata_kwargs[\"n_support\"], edata_kwargs[\"n_query\"]\n",
    "e_seq = e_seq.contiguous().view(n_way*(n_support+n_query), -1) \n",
    "print(f\"ℹ️ e_batch has {e_seq.shape[0]} samples each with dim {e_seq.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level, we have three main modules that include different variations of backbones. The purpose of this notebook is on exploring the different methods and thus we will not go into details of the backbones. Instead, see the dedicated [backbones notebook](backbones.ipynb) for more details.\n",
    "\n",
    "In this notebook, we will use `FCNet`, a simple fully connected network where we can choose its depth as well as number of hidden units in each layer. Finally, each layer automatically includes `BatchNorm1d`, a `ReLU` activation function and a `Dropout` layer for which we can define the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the backbone parameters\n",
    "fcnet_kwargs = {\n",
    "    \"x_dim\": 1280,\n",
    "    \"layer_dim\": [512, 256, 128],\n",
    "    \"dropout\": 0.1,\n",
    "    \"fast_weight\": False,\n",
    "}\n",
    "output_dim =  fcnet_kwargs[\"layer_dim\"][-1]\n",
    "\n",
    "backbone = FCNet(**fcnet_kwargs).to(device)\n",
    "\n",
    "# Run the backbone on the random batch\n",
    "r_out = backbone(r_seq.to(device))\n",
    "assert r_out.shape[0] == r_seq.shape[0], \"❌ r_out should have same number of samples as r_seq\"\n",
    "print(f\"✅ r_out and r_seq have same number of samples\")\n",
    "assert r_out.shape[1] == output_dim, f\"❌ r_out should have dim {output_dim} as last dim, instead got {r_out.shape[1]}\"\n",
    "print(f\"✅ r_out has dim {output_dim} as last dim as expected\")\n",
    "\n",
    "# Run the backbone on the encoded batch\n",
    "e_out = backbone(e_seq.to(device))\n",
    "assert e_out.shape[0] == e_seq.shape[0], \"❌ e_out should have same number of samples as e_seq\"\n",
    "print(f\"✅ e_out and e_seq have same number of samples\")\n",
    "assert e_out.shape[1] == output_dim, f\"❌ e_out should have dim {output_dim} as last dim, instead got {e_out.shape[1]}\"\n",
    "print(f\"✅ e_out has dim {output_dim} as last dim as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOT\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: adde description here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SOT:\n",
    "    sot = SOT(\n",
    "        final_feat_dim=n_way*(n_support+n_query), # Cause of transductive setting\n",
    "        distance_metric=\"cosine\",\n",
    "        or_reg=0.1,\n",
    "        sinkhorn_iterations=10,\n",
    "        sigmoid=False,\n",
    "        mask_diag=True,\n",
    "        max_scale=True\n",
    "    )\n",
    "else:\n",
    "    sot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaTemplate\n",
    "\n",
    "---\n",
    "\n",
    "The `MetaTemplate` class serves as a base class for all the methods. It includes all the necessary methods that are needed for training and evaluation. The main methods are:\n",
    "\n",
    "- `forward`: this method runs backbone on the raw input embeddings. Before calling this method, make sure the feature the input is in the correct format by running `parse_featue` method.\n",
    "\n",
    "- `train_loop`: this method runs `set_forward_loss` (**must be implemented by the child**) method on the training data and updates the model parameters based on the loss.\n",
    "\n",
    "- `test_loop`: this method evaluates the model on the test data. It calls the `set_forward` method (**must be implemented by the child**) to obtain the predictions and then calculates the overall accuracy, i.e., total number of correct predictions accross all classes divided by the total number of predictions.\n",
    "\n",
    "- `set_forward_adaptation`: this method first splits the input test data into support and query sets. Then, it freezes the backbone, and finetunes a new softmax classifier on the support set. Finally, it returns the predictions on the query set. Therefore, the difference between the `test_loop` and `set_forward_adaptation` is that the former does not conduct any finetuning, while the latter does. NB: Have not seen this method being used anywhere yet.\n",
    "\n",
    "See more details in the [meta-template module](../methods/meta_template.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (++)\n",
    "\n",
    "---\n",
    "\n",
    "The baseline training and evaluation can be splitted into two parts:\n",
    "\n",
    "(I) **BackBone Pretraining:** In the very first step, we pretrain the backbone on the entire training dataset. We compute the loss on the given task (classification / regression) and then update the weights of the backbone.\n",
    "\n",
    "(II) **Finetuning:** We finetune the backbone on the support set of each class or for regression, we just finetune on the entire support set. Note that finetuning is done on the given \"test set\", i.e., either `validation` or `test` set. Once we are done with finetuning, we make predictions on the query set and compute the corresponding accuracy using `MetaTemplate`'s `correct` method.\n",
    "\n",
    "Both of these parts are visualised in the following figure:\n",
    "\n",
    "![baseline](../images/baseline_overview.png)\n",
    "\n",
    "Importantly, we differentiate between two types of classifiers in the pretraining step:\n",
    "\n",
    "- **Linear Classifier:** Standard Linear layer.\n",
    "\n",
    "- **Cosine Classifier:** Cosine similarity between the embedding and the weight embeddings for individual classes.\n",
    "\n",
    "Let's start with the **pretraining**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target dim which is the number of unique classes in the dataset\n",
    "target_dim = len(r_datasets[0].trg2idx)\n",
    "print(f\"ℹ️ Target dim is {target_dim}\")\n",
    "\n",
    "# Define baseline parameters\n",
    "baseline_kwargs = {\n",
    "    \"backbone\": backbone,\n",
    "    \"n_way\": n_way, # For finetuning part only\n",
    "    \"n_support\": n_support, # For fine-tuning part only\n",
    "    \"n_classes\": target_dim, # Defines the output dim of the head, important for pretraining the backbone\n",
    "    \"loss\": \"softmax\", # baseline uses 'softmax', baseline++ uses 'dist'\n",
    "    \"type\": \"classification\",\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 100, # Print every 100 batches\n",
    "    \"sot\": sot,\n",
    "}\n",
    "\n",
    "\n",
    "# Define the baseline model\n",
    "baseline = Baseline(**baseline_kwargs).to(device)\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(baseline.parameters(), lr=lr)\n",
    "\n",
    "# Train the baseline model\n",
    "for epoch in range(n_epochs):\n",
    "    baseline.train_loop(epoch, r_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can move on to the **finetuning**. Notice, that here instead of the regular dataloader, we use the episodic dataloader which alows us to finetune the support set and then make predictions on the query set. These predictions are then used to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = baseline.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Networks\n",
    "\n",
    "---\n",
    "\n",
    "We train the protonet using the **meta-learning framework**. In each epoch, we now instead of classical batches have episodes. Each episode consists of a support set and a query set for each class. The support set is used to compute the prototypes for each class. Then, we compute the similarity between the query set and the prototypes and use the class with the largest similarity as the prediction. As of now, as a measure of similarity we use negative distanc, i.e., the smaller the distance the bigger the similarity. Finally, we compute the loss based on the query set predictions and update the model parameters.\n",
    "\n",
    "For the **evaluation**, we split the data into support and query sets. Then, we compute the prototypes on the support set and use them to make predictions on the query set. Finally, we compute the accuracy based on the predictions.\n",
    "\n",
    "![protonet](../images/protonet_overview.png)\n",
    "\n",
    "**NB:** Notice that in both baseline and ProtoNet we use some of the validation set. In case of baseline, we use it to finetune the classifier, while in ProtoNet we use it to compute the prototypes. However, for the protonet, or any other model trained via meta learning, we could, for the inference phase, finetune specific classifier to make the predictions. In other words, we use the meta learning framework for obtaining optimal weights for the backbone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define protonet kwargs\n",
    "protonet_kwargs = {\n",
    "    \"backbone\": backbone,\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 5, # Print every X episodes\n",
    "    \"sot\": sot,\n",
    "}\n",
    "\n",
    "# Define the ProtoNet Model\n",
    "protonet = ProtoNet(**protonet_kwargs).to(device)\n",
    "\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(protonet.parameters(), lr=lr)\n",
    "\n",
    "# Train the protonet model\n",
    "for epoch in range(n_epochs):\n",
    "    protonet.train_loop(epoch, e_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the protonet as desribed above, i.e., we compute the prototypes on the support set and use them to make predictions on the query set. Finally, we compute the accuracy based on the predictions. This is done for several episodes which each consists of a random selection of classses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = baseline.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Networks\n",
    "\n",
    "---\n",
    "\n",
    "The matching network **training** can be splitted into two parts:\n",
    "\n",
    "(I) **Encode support embeddings:** we run the support set embeddings first via the backbone and then via the encoder. In our case, this is a **bidirectional-LSTM**. In particular, we take the outpout of both directions which each has the `feat_dim`, i.e., the same as the output of the backbone, and then add it to the output of the backbone. Thus, we can see this as sort of residual connection.\n",
    "\n",
    "(II) **Compute weighted sum of query embeddings:** We take the query embeddings and recompute their transformation via a single LSTM cell that uses an attention mechanism.\n",
    "\n",
    "(III) **Compute the logprobs:** In our final step, we take the output of the first and second step and compute the similarity between the support and query embeddings. This is done via a dot product between the support and query embeddings matrices. We then take the softmax, so that we obtain the probability distribution for each query embedding with respect to each support embedding. We then sum the probabilities for each class and take the log of the sum. This is our final output.\n",
    "\n",
    "![matchingnet](../images/matchingnet_overview.png)\n",
    "\n",
    "For the inference, we do the same as for the training, except that we do not update the model parameters and just return the logprobs for each query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matchingnet kwargs\n",
    "matchingnet_kwargs = {\n",
    "    \"backbone\": backbone,\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 5, # Print every X episodes\n",
    "    \"sot\": sot\n",
    "}\n",
    "\n",
    "# Define the MatchingNet Model\n",
    "matchingnet = MatchingNet(**matchingnet_kwargs).to(device)\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(matchingnet.parameters(), lr=lr)\n",
    "\n",
    "# Train the protonet model\n",
    "for epoch in range(n_epochs):\n",
    "    matchingnet.train_loop(epoch, e_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can evaluate as with any other method before using the `test_loop` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = matchingnet.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Agnostic Meta-Learning (MAML)\n",
    "\n",
    "---\n",
    "\n",
    "The key idea behind **MAML** is to train a model on a variety of tasks to learn an optimal initial set of parameters from which it will be easy after a few gradient steps to finetune to a new task. In other words, we want to learn a good initialization of the model parameters such that we can quickly adapt to a new task with a few gradient steps.\n",
    "\n",
    "The **meta-training** procedure can be described as follows. Our model, in this case backbone as well as the classifier are intiliased as usual with random weights, in the MAML setting, we call these **slow weights**. In each epoch, we then sample a batch of episodes where each epidode essentially represents a task. In particular each episode is represtented as a tensor of shape `(n_way, n_support + n_query, hidden_dim)`. Now, for each task, we first split the data into support and query sets as usual. Then, we initialise our fast weights from the slow weights. We then compute the loss on the support set and update the fast weights. We do this for a given number of steps, e.g., 5. Finally, we compute the loss on the query set. We do this for every epidode and then compute the sum of episode losses. This is our final loss which we use to update the slow weights. We then continue to another batch of episodes and repeat the same procedure.\n",
    "\n",
    "![maml](../images/maml_overview.png)\n",
    "\n",
    "For the **meta-testing** part, we do not update the slow weights. Instead, for now only **each episode**, we initialise the fast weights from the slow weights for and then compute the loss on the support set. We repeat this for a given number of steps and finally make the predictions on the query set. We then compute the accuracy based on the predictions. We then take the mean of the accuracies across all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epoch 001 | Batch/ Episode 0100/0100 | Loss 1.61067:   0%|                              | 0/100\n",
      "Training: Epoch 002 | Batch/ Episode 0100/0100 | Loss 1.60514:   0%|                              | 0/100\n",
      "Training: Epoch 003 | Batch/ Episode 0100/0100 | Loss 1.60325:   0%|                              | 0/100\n",
      "Training: Epoch 004 | Batch/ Episode 0100/0100 | Loss 1.59507:   0%|                              | 0/100\n",
      "Training: Epoch 005 | Batch/ Episode 0100/0100 | Loss 1.58955:   0%|                              | 0/100\n",
      "Training: Epoch 006 | Batch/ Episode 0100/0100 | Loss 1.58142:   0%|                              | 0/100\n",
      "Training: Epoch 007 | Batch/ Episode 0100/0100 | Loss 1.57533:   0%|                              | 0/100\n",
      "Training: Epoch 008 | Batch/ Episode 0100/0100 | Loss 1.56563:   0%|                              | 0/100\n",
      "Training: Epoch 009 | Batch/ Episode 0100/0100 | Loss 1.55766:   0%|                              | 0/100\n",
      "Training: Epoch 010 | Batch/ Episode 0100/0100 | Loss 1.55435:   0%|                              | 0/100\n",
      "Training: Epoch 011 | Batch/ Episode 0100/0100 | Loss 1.54830:   0%|                              | 0/100\n",
      "Training: Epoch 012 | Batch/ Episode 0100/0100 | Loss 1.53675:   0%|                              | 0/100\n",
      "Training: Epoch 013 | Batch/ Episode 0100/0100 | Loss 1.53438:   0%|                              | 0/100\n",
      "Training: Epoch 014 | Batch/ Episode 0100/0100 | Loss 1.52323:   0%|                              | 0/100\n",
      "Training: Epoch 015 | Batch/ Episode 0100/0100 | Loss 1.51498:   0%|                              | 0/100\n",
      "Training: Epoch 016 | Batch/ Episode 0100/0100 | Loss 1.50963:   0%|                              | 0/100\n",
      "Training: Epoch 017 | Batch/ Episode 0100/0100 | Loss 1.50013:   0%|                              | 0/100\n",
      "Training: Epoch 018 | Batch/ Episode 0100/0100 | Loss 1.49285:   0%|                              | 0/100\n",
      "Training: Epoch 019 | Batch/ Episode 0100/0100 | Loss 1.48611:   0%|                              | 0/100\n",
      "Training: Epoch 020 | Batch/ Episode 0100/0100 | Loss 1.47685:   0%|                              | 0/100\n"
     ]
    }
   ],
   "source": [
    "# Setup the backbone parameters\n",
    "maml_backbone_kwargs = {\n",
    "    \"x_dim\": 1280,\n",
    "    \"layer_dim\": [512, 256, 128],\n",
    "    \"dropout\": 0.1,\n",
    "    \"fast_weight\": True, # Use fast weights for MAML!\n",
    "}\n",
    "\n",
    "maml_backbone = FCNet(**maml_backbone_kwargs).to(device)\n",
    "\n",
    "# Define maml kwargs\n",
    "maml_kwargs = {\n",
    "    \"backbone\": maml_backbone,\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_task\": 4, # After how many tasks to update the meta-learner\n",
    "    \"task_update_num\": 3, # Number of gradient updates per task\n",
    "    \"inner_lr\": 1e-2, # Learning rate for the fast weight\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 1, # Print every X episodes\n",
    "    \"sot\": sot\n",
    "}\n",
    "\n",
    "# Define the Maml Model\n",
    "maml = MAML(**maml_kwargs).to(device)\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(maml.parameters(), lr=lr)\n",
    "\n",
    "# Train the protonet model\n",
    "for epoch in range(n_epochs):\n",
    "    maml.train_loop(epoch, e_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the model by adapting the model to the support set for the `task_update_num` and then making predictions on the query set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ The validation accuracy is 50.73 ± 8.48\n"
     ]
    }
   ],
   "source": [
    "eval_acc = maml.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
