{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore the use of different `methods`. For that,\n",
    "we will be using `SwissProt` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to load local modules\n",
    "import sys\n",
    "sys.path.insert(0, '..') # add directory above current directory to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EXISTS: go-basic.obo\n",
      "go-basic.obo: fmt(1.2) rel(2023-06-11) 46,420 Terms; optional_attrs(relationship)\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "# Reload modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Module imports\n",
    "import time\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Custom Modules imports\n",
    "# - Datasets loading\n",
    "from datasets.prot.swissprot import SPSimpleDataset, SPSetDataset  # noqa\n",
    "\n",
    "# - Backbone of for this notebook\n",
    "from backbones.fcnet import FCNet  # noqa\n",
    "\n",
    "# - Methods\n",
    "from methods.baseline import Baseline  # noqa\n",
    "from methods.protonet import ProtoNet  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we load the data from the `SwissProt` database which is the smaller of the two datasets. We will be using both the regular dataloader for standard few shot finetuning as well as episodic dataloader for episodic finetuning.\n",
    "\n",
    "**NB 1:** Evaluation can be done on both types of loaders. However, when it comes to training, `baseline` method is trained on regular dataloader, `episodic` method is trained on episodic dataloader.\n",
    "\n",
    "**NB 2:** what is the correct way of getting samples `get_samples` or `get_samples_via_ic`?\n",
    "\n",
    "**NB 3:** We should make sure we all have the same preprocessed dataset since they they randomly choose the `GO` label for each protein out of all that are on level 5 if using `get_samples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 11072 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ train split has 189 classes.\n",
      "\n",
      "ℹ️ val split has 1230 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ val split has 26 classes.\n",
      "\n",
      "ℹ️ test split has 642 samples\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "ℹ️ test split has 11 classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "batch_size = 10\n",
    "min_samples = 6\n",
    "\n",
    "rdata_kwargs = {\n",
    "    \"root\": root,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"use_ic_selection\": True,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "r_datasets = [SPSimpleDataset(**rdata_kwargs, mode=mode) for mode in modes]\n",
    "r_train, r_val, r_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in r_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(r_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} samples\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print(f\"ℹ️ {mode} split has {len(np.unique([smp.annot for smp in split.samples]))} classes.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ train split has 189 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ val split has 26 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n",
      "ℹ️ test split has 11 number of classes.\n",
      "ℹ️ Each sample is an encoded protein sequence of length 1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loading parameters\n",
    "root = \"../data\"\n",
    "n_way = 5\n",
    "n_support = 3\n",
    "n_query = 3\n",
    "subset = 1.0 # Load full dataset\n",
    "\n",
    "edata_kwargs = {\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"n_query\": n_query,\n",
    "    \"root\": root,\n",
    "    \"subset\": subset,\n",
    "    \"use_ic_selection\": True,\n",
    "}\n",
    "\n",
    "# Load SPSetDataset for each mode\n",
    "modes = [\"train\", \"val\", \"test\"]\n",
    "e_datasets = [SPSetDataset(**edata_kwargs, mode=mode) for mode in modes]\n",
    "e_train, e_val, e_test = [dataset.get_data_loader(num_workers=0, pin_memory=False) for dataset in e_datasets]\n",
    "\n",
    "# Get some basic statistics about each of the splits\n",
    "for split, mode in zip(e_datasets, modes):\n",
    "    print(f\"ℹ️ {mode} split has {len(split)} number of classes.\")\n",
    "    print(f\"ℹ️ Each sample is an encoded protein sequence of length {split.dim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone choice\n",
    "\n",
    "---\n",
    "\n",
    "Before we initiliase our method, we need to choose which backbone we want to use. Backbone is simply a feature extractor that transforms the raw input embeddings to a more useful version which can then be later fed into our method of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the backbone works, we will run it on random batch of our data and see the output. For the regular dataloader, we can just take the output and feed it into the backbone as is since it is already in the expected shape. \n",
    "\n",
    "However, for the episodic dataloader, we obtain a tensor of shape `(n_way, n_support + n_query, hidden dim)` which we need to remap to `(n_way * (n_support * n_query), hidden dim)` before feeding it into the backbone. This is because the backbone expects a tensor of shape `(batch size, hidden dim)`. Thus, you can think of the mapping as basically flattening along the second dimension of the input tensor.\n",
    "\n",
    "**NB:** In the `MetaTemplate` class, and specifically its `parse_feature` method, the flattening is performed as:\n",
    "\n",
    "```python\n",
    "x = x.view(n_way * (n_support + n_query), *x.size()[2:])\n",
    "```\n",
    "\n",
    "This makes indeed only sense if we for instance have more than 1 last dimension, e.g. the input tensor would be `(n_way, n_support + n_query, seq, hidden dim)`. Based on the [eda notebook](eda.ipynb), we know that the input tensor is of shape `(n_way, n_support + n_query, hidden dim)` and thus we can simply flatten along the second dimension as:\n",
    "\n",
    "```python\n",
    "x = x.view(n_way * (n_support + n_query), -1)\n",
    "```\n",
    "\n",
    "With that being said, for the purpose of generalisation, we can keep the original code. TODO: discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ r_batch has 10 samples each with dim 1280\n",
      "ℹ️ e_batch has 30 samples each with dim 1280\n"
     ]
    }
   ],
   "source": [
    "# Sample a batch from the regular train loader\n",
    "r_seq, r_lab = next(iter(r_train))\n",
    "print(f\"ℹ️ r_batch has {len(r_seq)} samples each with dim {r_seq.shape[1]}\")\n",
    "\n",
    "# Sample a batch from the episodic train loader\n",
    "e_seq, e_lab = next(iter(e_train))\n",
    "n_way, n_support, n_query = edata_kwargs[\"n_way\"], edata_kwargs[\"n_support\"], edata_kwargs[\"n_query\"]\n",
    "e_seq = e_seq.contiguous().view(n_way*(n_support+n_query), -1) \n",
    "print(f\"ℹ️ e_batch has {e_seq.shape[0]} samples each with dim {e_seq.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level, we have three main modules that include different variations of backbones. The purpose of this notebook is on exploring the different methods and thus we will not go into details of the backbones. Instead, see the dedicated [backbones notebook](backbones.ipynb) for more details.\n",
    "\n",
    "In this notebook, we will use `FCNet`, a simple fully connected network where we can choose its depth as well as number of hidden units in each layer. Finally, each layer automatically includes `BatchNorm1d`, a `ReLU` activation function and a `Dropout` layer for which we can define the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ r_out and r_seq have same number of samples\n",
      "✅ r_out has dim 128 as last dim as expected\n",
      "✅ e_out and e_seq have same number of samples\n",
      "✅ e_out has dim 128 as last dim as expected\n"
     ]
    }
   ],
   "source": [
    "# Setup the backbone parameters\n",
    "fcnet_kwargs = {\n",
    "    \"x_dim\": 1280,\n",
    "    \"layer_dim\": [512, 256, 128],\n",
    "    \"dropout\": 0.1,\n",
    "    \"fast_weight\": False,\n",
    "}\n",
    "output_dim =  fcnet_kwargs[\"layer_dim\"][-1]\n",
    "\n",
    "backbone = FCNet(**fcnet_kwargs).to(device)\n",
    "\n",
    "# Run the backbone on the random batch\n",
    "r_out = backbone(r_seq.to(device))\n",
    "assert r_out.shape[0] == r_seq.shape[0], \"❌ r_out should have same number of samples as r_seq\"\n",
    "print(f\"✅ r_out and r_seq have same number of samples\")\n",
    "assert r_out.shape[1] == output_dim, f\"❌ r_out should have dim {output_dim} as last dim, instead got {r_out.shape[1]}\"\n",
    "print(f\"✅ r_out has dim {output_dim} as last dim as expected\")\n",
    "\n",
    "# Run the backbone on the encoded batch\n",
    "e_out = backbone(e_seq.to(device))\n",
    "assert e_out.shape[0] == e_seq.shape[0], \"❌ e_out should have same number of samples as e_seq\"\n",
    "print(f\"✅ e_out and e_seq have same number of samples\")\n",
    "assert e_out.shape[1] == output_dim, f\"❌ e_out should have dim {output_dim} as last dim, instead got {e_out.shape[1]}\"\n",
    "print(f\"✅ e_out has dim {output_dim} as last dim as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaTemplate\n",
    "\n",
    "---\n",
    "\n",
    "The `MetaTemplate` class serves as a base class for all the methods. It includes all the necessary methods that are needed for training and evaluation. The main methods are:\n",
    "\n",
    "- `forward`: this method runs backbone on the raw input embeddings. Before calling this method, make sure the feature the input is in the correct format by running `parse_featue` method.\n",
    "\n",
    "- `train_loop`: this method runs `set_forward_loss` (**must be implemented by the child**) method on the training data and updates the model parameters based on the loss.\n",
    "\n",
    "- `test_loop`: this method evaluates the model on the test data. It calls the `set_forward` method (**must be implemented by the child**) to obtain the predictions and then calculates the overall accuracy, i.e., total number of correct predictions accross all classes divided by the total number of predictions.\n",
    "\n",
    "- `set_forward_adaptation`: this method first splits the input test data into support and query sets. Then, it freezes the backbone, and finetunes a new softmax classifier on the support set. Finally, it returns the predictions on the query set. Therefore, the difference between the `test_loop` and `set_forward_adaptation` is that the former does not conduct any finetuning, while the latter does. NB: Have not seen this method being used anywhere yet.\n",
    "\n",
    "See more details in the [meta-template module](../methods/meta_template.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (++)\n",
    "\n",
    "---\n",
    "\n",
    "The baseline training and evaluation can be splitted into two parts:\n",
    "\n",
    "(I) **BackBone Pretraining:** In the very first step, we pretrain the backbone on the entire training dataset. We compute the loss on the given task (classification / regression) and then update the weights of the backbone.\n",
    "\n",
    "(II) **Finetuning:** We finetune the backbone on the support set of each class or for regression, we just finetune on the entire support set. Note that finetuning is done on the given \"test set\", i.e., either `validation` or `test` set. Once we are done with finetuning, we make predictions on the query set and compute the corresponding accuracy using `MetaTemplate`'s `correct` method.\n",
    "\n",
    "Both of these parts are visualised in the following figure:\n",
    "\n",
    "![baseline](../images/baseline_overview.png)\n",
    "\n",
    "Importantly, we differentiate between two types of classifiers in the pretraining step:\n",
    "\n",
    "- **Linear Classifier:** Standard Linear layer.\n",
    "\n",
    "- **Cosine Classifier:** Cosine similarity between the embedding and the weight embeddings for individual classes.\n",
    "\n",
    "Let's start with the **pretraining**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Epoch 0 | Batch 1100/1108 | Loss 3.555414\n"
     ]
    }
   ],
   "source": [
    "# Define the target dim which is the number of unique classes in the dataset\n",
    "target_dim = len(r_datasets[0].trg2idx)\n",
    "print(f\"ℹ️ Target dim is {target_dim}\")\n",
    "\n",
    "# Define baseline parameters\n",
    "baseline_kwargs = {\n",
    "    \"backbone\": backbone,\n",
    "    \"n_way\": n_way, # For finetuning part only\n",
    "    \"n_support\": n_support, # For fine-tuning part only\n",
    "    \"n_classes\": target_dim, # Defines the output dim of the head, important for pretraining the backbone\n",
    "    \"loss\": \"dist\", # baseline uses 'softmax', baseline++ uses 'dist'\n",
    "    \"type\": \"classification\",\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 100, # Print every 100 batches\n",
    "}\n",
    "\n",
    "\n",
    "# Define the baseline model\n",
    "baseline = Baseline(**baseline_kwargs).to(device)\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 1\n",
    "lr = 0.001\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(baseline.parameters(), lr=lr)\n",
    "\n",
    "# Train the baseline model\n",
    "for epoch in range(n_epochs):\n",
    "    baseline.train_loop(epoch, r_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can move on to the **finetuning**. Notice, that here instead of the regular dataloader, we use the episodic dataloader which alows us to finetune the support set and then make predictions on the query set. These predictions are then used to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ The validation accuracy is 69.13 ± 14.04\n"
     ]
    }
   ],
   "source": [
    "eval_acc = baseline.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Networks\n",
    "\n",
    "---\n",
    "\n",
    "We train the protonet using the **meta-learning framework**. In each epoch, we now instead of classical batches have episodes. Each episode consists of a support set and a query set for each class. The support set is used to compute the prototypes for each class. Then, we compute the similarity between the query set and the prototypes and use the class with the largest similarity as the prediction. As of now, as a measure of similarity we use negative distanc, i.e., the smaller the distance the bigger the similarity. Finally, we compute the loss based on the query set predictions and update the model parameters.\n",
    "\n",
    "For the **evaluation**, we split the data into support and query sets. Then, we compute the prototypes on the support set and use them to make predictions on the query set. Finally, we compute the accuracy based on the predictions.\n",
    "\n",
    "![protonet](../images/protonet_overview.png)\n",
    "\n",
    "**NB:** Notice that in both baseline and ProtoNet we use some of the validation set. In case of baseline, we use it to finetune the classifier, while in ProtoNet we use it to compute the prototypes. However, for the protonet, or any other model trained via meta learning, we could, for the inference phase, finetune specific classifier to make the predictions. In other words, we use the meta learning framework for obtaining optimal weights for the backbone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ epoch 19 | batch/episode 100/100 | loss 1.908730\n"
     ]
    }
   ],
   "source": [
    "# Define protonet kwargs\n",
    "protonet_kwargs = {\n",
    "    \"backbone\": backbone,\n",
    "    \"n_way\": n_way,\n",
    "    \"n_support\": n_support,\n",
    "    \"log_wandb\": False,\n",
    "    \"print_freq\": 5, # Print every X episodes\n",
    "}\n",
    "\n",
    "# Define the ProtoNet Model\n",
    "protonet = ProtoNet(**protonet_kwargs).to(device)\n",
    "\n",
    "\n",
    "# Define training hyperparameters\n",
    "n_epochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the optimizer for obtaining training\n",
    "optimizer = AdamW(protonet.parameters(), lr=lr)\n",
    "\n",
    "# Train the protonet model\n",
    "for epoch in range(n_epochs):\n",
    "    protonet.train_loop(epoch, e_train, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the protonet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ The validation accuracy is 66.80 ± 15.08\n"
     ]
    }
   ],
   "source": [
    "eval_acc = baseline.test_loop(e_val, return_std=True)\n",
    "clear_output()\n",
    "print(f\"ℹ️ The validation accuracy is {eval_acc[0]:.2f} ± {eval_acc[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Networks\n",
    "\n",
    "---\n",
    "\n",
    "TODO: this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAML\n",
    "\n",
    "---\n",
    "\n",
    "TODO: this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
