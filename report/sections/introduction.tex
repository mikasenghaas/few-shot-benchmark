\section{Introduction}

Biomedical research often grapples with data scarcity, typically due to high costs of data collection and expert-dependent annotation processes. Traditional machine learning approaches often fall short in these data-limited settings as they require extensive training samples and iterations. Few-shot learning algorithms, tailored to discern distinct features from minimal data, offer a promising alternative. This study aims to assess the applicability of such methods in the biomedical domain. We examine four established few-shot learning techniques — Baseline(++)~\cite{baseline}, Matching Networks~\cite{matchingnet}, Prototypical Networks~\cite{protonet}, and Model-Agnostic Meta-Learning (MAML)~\cite{maml} — across two distinct biomedical tasks. The first involves predicting cell types based on gene expression, using the Tabula Muris dataset~\cite{tabula2018}. The second task focuses on predicting protein functions from their sequence embeddings, using the SwissProt dataset~\cite{uniprot2019}.

All of the above approaches rely on meaningful embeddings of features. However, a notable challenge arises due to the potential discrepancy in data distributions between samples seen during meta-training and samples of novel classes seen during meta-testing. This discrepancy can result in embeddings that are not fully transferable, leading to suboptimal performance in downstream tasks. Self-Optimal-Transport (SOT)~\cite{sot} is a feature transform module, grounded in probabilistic interpretations, that aims to mitigate this issue. We include the SOT feature transform module in all of the above mentioned few-shot learning methods to study its effectiveness in the biomedical domain.

In summary, our study presents two primary contributions. First, we train and assess leading few-shot learning algorithms on two unique biomedical tasks, varying the number of classes and the quantity of training samples. Second, we explore the influence of the Self-Optimal-Transport (SOT) feature transform on few-shot learning performance, delving into the dynamics of SOT's integration with other components within meta-learners.

\textit{All code and data to reproduce the experiments study is available on \href{https://github.com/mikasenghaas/few-shot-benchmark}{GitHub} and the reported experiments can be accessed on \href{https://wandb.ai/metameta-learners/few-shot-benchmark}{W\&B}.}