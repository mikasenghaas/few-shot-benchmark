\section{Introduction}

Biomedical research often grapples with data scarcity, typically due to high costs of data collection and expert-dependent annotation processes. Traditional machine learning approaches often fall short in these data-limited settings as they require extensive training samples and iterations. Few-shot learning algorithms, tailored to discern distinct features from minimal data, offer a promising alternative. Despite the abundance of few-shot learning tasks and the diverse set of methods proposed in recent literature~\cite{baseline, matchingnet, protonet, maml} their application has been largely confined to vision tasks (\textit{CITE}). This study aims to assess the applicability of these methods in the biomedical domain. We examine four established few-shot learning techniques — Baseline(++)~\cite{baseline}, Matching Networks~\cite{matchingnet}, Prototypical Networks~\cite{protonet}, and Model-Agnostic Meta-Learning (MAML)~\cite{maml} — across two distinct biomedical tasks. The first involves predicting cell types based on gene expression, using the Tabula Muris dataset~\cite{tabula2018}, with training, validation, and testing conducted on cells from separate tissues to facilitate cross-tissue knowledge transfer. The second task focuses on predicting protein functions from their sequence embeddings, using the SwissProt dataset~\cite{uniprot2019}. Here, the challenge is to meta-train on protein functions that are distinct from those in the validation and testing phases, ensuring a robust few-shot learning framework.

All of the above approaches rely on meaningful embed-
dings of features. However, a notable challenge arises due to the potential discrepancy in data distributions between the meta-training and testing phases. This discrepancy can result in embeddings that are not fully transferable, leading to suboptimal performance in downstream tasks. To address this issue, Self-Optimal-Transport (SOT)~\cite{sot} has been proposed. SOT is a parameter-less feature transform module, grounded in probabilistic interpretations, and has shown promising results in improving performance across various few-shot learning benchmarks. We include the SOT feature transform module in all of the above mentioned few-shot learning methods to study its effectiveness in the biomedical domain.

In summary, our study presents two primary contributions. First, we train and assess leading few-shot learning algorithms on two unique biomedical tasks, varying the number of classes and the quantity of training samples. Second, we explore the influence of the Self-Optimal-Transport (SOT) feature transform on few-shot learning performance, delving into the dynamics of SOT's integration with other components within meta-learners.