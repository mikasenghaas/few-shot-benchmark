\section{Introduction}

Learning from few samples remains a major challenge in machine learning, especially in biomedical tasks were data availability is often a limiting factor due to high costs of data collection and expert-dependent annotation processes. Traditional machine learning approaches often fall short in these data-limited settings as they require extensive training samples and iterations. 
Few-shot learning algorithms, tailored to discern distinct features from minimal data, offer a promising alternative. 
This study aims to assess the applicability of such methods in the biomedical domain. We examine four established few-shot learning techniques — Baseline(++)~\cite{baseline}, 
Matching Networks~\cite{matchingnet}, Prototypical Networks~\cite{protonet}, and Model-Agnostic Meta-Learning (MAML)~\cite{maml} — across two distinct biomedical tasks. 
The first involves predicting cell types based on gene expression, using the Tabula Muris dataset~\cite{tabula2018}. The second task focuses on predicting protein functions 
from their sequence embeddings, using the SwissProt dataset~\cite{uniprot2019}.

All of the above approaches rely on meaningful embeddings of features. However, a notable challenge arises due to the potential discrepancy in data 
distributions between samples seen during meta-training and samples of novel classes seen during meta-testing. This discrepancy may result in embeddings that are 
not fully transferable, leading to suboptimal performance in downstream tasks. Self-Optimal-Transport (SOT)~\cite{sot} is a feature transform module that aims to mitigate this issue. We include the SOT feature transform module in all of the above mentioned few-shot learning methods to 
study its effectiveness in the biomedical domain.

In summary, our study presents three primary contributions. First, we train and assess leading few-shot learning algorithms on two unique biomedical tasks. We report the performance of these algorithms with and without the SOT module. 
Second, we study the performance in varying few-shot learning settings by differing the number of classes to distinguish and number of samples per class to learn from.
Third, we ablate the hyperparameters of all models that include the SOT feature transform to understand their impact on performance. We hope that our study will serve as a benchmark for future research in the biomedical domain.


