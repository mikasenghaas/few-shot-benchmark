\section{Methodology}

An experiment in our study is defined as a combination of a few-shot learning \textit{method}, optionally including the \textit{SOT} feature transform, trained and evaluated on a \textit{dataset} within 
a specified few-shot learning setting, characterised by the number of classes (\textit{n-way}) and the number of samples per class (\textit{n-shot}).

\subsection{Experiment Setup}

% Backbone
\textbf{Backbone.} All experiments employ a fully-connected feed-forward neural network with batch normalisation, ReLU activation, and dropout. 
The network has two hidden layers, with hidden dimensions being tuned for each experiment.

% Model training
\textbf{Training.} Training of the models is conducted for a maximum of 40 epochs, employing the Adam optimiser with varying learning rates. 
We implement early stopping after five epochs of no improvement in validation accuracy. 

% Hyperparameter tuning
\textbf{Tuning.} Hyperparameter tuning is performed for all models that include the SOT module, unless specified otherwise. Tuning includes the learning rate ($\lambda = \{0.1, 0.01, 0.001\}$) for all methods as well as the backbone's hidden dimension size ($\kappa = \{64, 512, 1024\}$). For models including the SOT module, we adapt the hyperparameter grid of \citeauthor{sot}, namely the regularisation parameter ($\gamma = \{1.0, 0.1, 0.01\}$) and the choice of distance metric ($\delta = \{cosine, euclidean\}$). 
The model demonstrating the best performance on the validation split is evaluated on the test split and reported.

% Evaluation
\textbf{Evaluation.} A model performance's is reported through the mean and 95\% confidence interval of the few-shot accuracy, calculated over 600 episodes with each episode utilising five query samples per class.

\subsection{Experiments}

Due to the impracticality of exhaustive hyperparameter grid searching across all experimental configurations, 
we have structured our experiments into two distinct groups. Each group fixes certain parameters, allowing us to focus on the impact of the variables of interest.

\textbf{General Benchmark.} This experiment group evaluates models, with and without SOT feature transform, on both datasets in a 5-way-5-shot setting, 
comprising 20 experiments. The aim is to analyse the influence of the method, dataset, and SOT module on few-shot learning performance.

\textbf{Way-Shot Analysis.} In the second group we investigate the performance in various few-shot learning settings, exploring combinations of n-way ({2, 4, 6, 8, 10}) and n-shot ({1, 5, 10, 15, 20}). Here, we fix the method to \texttt{MN} and the dataset to \texttt{TM}, resulting in 50 experiments. For this experiment, we don't fine-tune the hyperparameters but instead use the best-performing hyperparameters from the general benchmark experiment.