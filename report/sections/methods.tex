\section{Methods}

Few-shot learning algorithms can be classified into two main categories: \textit{Transfer learning} involves a two-phase process of pre-training on a large dataset to learn general data representation, 
followed by fine-tuning on the target task with limited data. \textit{Meta-learning}, conversely, leverages past experiences from a series of related tasks to efficiently tackle a new task with sparse data. 
These algorithms undergo meta-training, where the model encounters various tasks, mimicking the target few-shot learning scenario. Each task comprises randomly chosen support and query samples from identical class sets, 
training the model to adapt to support samples and classify query samples.

In the following we describe the high-level idea of all methods considered within this study. For more details on the methods please refer to the original papers.

The \textbf{Baseline}~\cite{baseline} model implements the fine-tuning paradigm. During meta-testing, the model is fine-tuned on support samples and then classifies query samples. Within this study, we consider two variants of the trainable classification head - one learns a traditional linear and the other a cosine similarity layer. We refer to these as Baseline (\texttt{B}) and Baseline++ (\texttt{B++}), respectively.

\textbf{Prototypical Networks}~\cite{protonet} (\texttt{PN}) learn an embedding space that clusters samples from the same class close together. 
Query samples are then classified according to the distance to the average support sample (prototype) of each class.

\textbf{Matching Networks}~\cite{matchingnet} (\texttt{MN}) are similar to \texttt{PN}. However, in \texttt{MN}, the distance between a query sample is computed to all support samples and then aggregated. 
Importantly, before the distance computation, \texttt{MN} contextualises both support and query samples by re-embedding them using an LSTM.

Finally, \textbf{Model Agnostic Meta Learning}~\cite{maml} (\texttt{MAML}) is an optimisation-based meta-learning approach that aims to learn an effective weight initialisation that can be adapted to new tasks in a small number of gradient steps.

% TODO: Mention problems with reduced dimensionality of SOT embeddings.

The \textbf{Self-Optimal Transport} (SOT)~\cite{sot} feature transform is a parameterless and fully differentiable method for transforming feature vectors. SOT embeddings are notable for their interpretability and potential to upgrade a set of features to facilitate  downstream matching or grouping related tasks, as encountered frequently in few-shot learning settings.

SOT fundamentally utilises Optimal Transport (OT) on a square distance matrix (e.g. cosine similarity matrix) of input features, 
leading to embeddings that reflect the \textit{direct} similarity and \textit{third-party} agreement of samples to each other. 
Mathematically, SOT is a function \(T: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times n}\) that maps \(n\) samples in \(d\)-dimensions to a 
re-embedded SOT vectors in \(n\)-dimensions. The SOT embeddings are computed from an iterative optimisation algorithm known as the Sinkhorn-Knopp 
algorithm~\cite{sinkhorn-knopp} that solves a regularised version of the OT problem.

In few-shot learning contexts, SOT helps align independently embedded support and query samples by jointly embedding them according to their similarities to each other - an example of \textit{transductivity}. 
The SOT feature transform is used in state-of-the-art methods in common few-shot learning benchmarks~\cite{sot}. Within our study we employ the SOT feature transform module on the embeddings obtained from the backbone network. Critically, we shuffle the query samples before the forward-pass to avoid learning a trivial mapping from sample position to class label.