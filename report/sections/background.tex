\section{Background}

\subsection{Learning Paradigms}

The methods studied within this project can broadly be categorised into two paradigms of learning: supervised learning and meta-learning.

In the traditional supervised learning, a model is trained on a large corpus of labeled samples on a related task and then fine-tuned on the task at hand. This approach is often referred to as transfer learning. 

In contrast, meta-learning, also known as learning to learn, aims to teach the model to be able to learn quickly. Therefore, during the training, in each epoch, the model is presented with the given number of episodes where each episode can be viewed as a different task. Given that in our work, we solely focus on classification, each episode consists of the given number of classes (\texttt{n-way}) where for each class we have certain number of support samples (\texttt{n-shot}) and query samples. During inference, the model is evaluated on its ability to adapt to newly presented tasks.  It first uses the support vectors from the episode to adapt to the task at hand and then its performance is evaluated based on the query samples. The final performance of a model is a mean score across all episodes which should reflect its ability to adapt quickly to new tasks, even with a small amount of samples.

The methods presented in our work use two types of learning. In supervised learning, the objective of the model is to learn the general mapping from the observed samples seen during training to the given task and then be able to apply this knowledge on unseen samples during inference. However, in practice, there is often a need to use the model on a slightly different task than it was initially trained on. One possible solution is to fine tune the model on the subset of data for the new task. This, however, depending on the difficulty of the task might require large amount of labeled data. 

This motivated research in meta learning where the focus is towards teaching the model to be able to learn quickly. Therefore, during the training, in each epoch, the model is presented with the given number of episodes where each episode can be viewed as a different task. Given that in our work, we solely focus on classification, each episode consists of the given number of classes (\texttt{n-way}) where for each class we have certain number of support samples (\texttt{n-shot}) and query samples. During inference, the model is evaluated on its ability to adapt to newly presented tasks.  It first uses the support vectors from the episode to adapt to the task at hand and then its performance is evaluated based on the query samples. The final performance of a model is a mean score across all episodes which should reflect its ability to adapt quickly to new tasks, even with a small amount of samples.

Last but not the least, in our work we differentiate between inductive and transductive methods. The transducive methods, in contrast to the inductive ones, use as part of their adaption process query samples' features. This is indeed possible only in meta learning setting where we have access to the features of the samples we later evaluate our model on, in contrast to the classical batch learning.

\subsection{Methods}
For our analysis, we chose four state-of-the-art (SOTA) methods which place high accross several standard image classification tasks. Each of these methods expect each input sample to be embedded. For the embedding task, we therefore use the same fully connected feed forward neural network which is often refered to as \texttt{backbone}.

Prototypical Network~\cite{protonet} (\texttt{ProtoNet}) is a metric based meta-learning approach that first aggregates support samples within each class to create class prototypes. We then compare query samples to each of the respective class prototypes and assign the query to the closest prototype.

In contrast, Matching Network~\cite{matchingnet} (\texttt{MatchingNet}) employs pairwise comparisons, comparing the query sample to all support samples for each class. In addition, \texttt{MatchingNet} includes further transformation of both support and query samples. The support embeddings are contextualised via a bidirectional Long-Short-Term Memory network (LSTM). These are then compared to every query embedding via a cosine distance to determine how much much weight should be each support vector given when constructing the new version of query embedding. The final step involves contextualizing query embeddings through multiple iterations in a single LSTM Cell. Overall, these steps aim to enhance \texttt{MatchingNet}'s discriminative power for improved classification performance.

Model Agnostic Meta Learning~\cite{maml} (\texttt{MAML}) takes in contrast to the previous two methods metric driven approach. \texttt{MAML} focuses on learning an effective initialization of weights during training. Specifically, there are two kinds of weights in \texttt{MAML}. Slow weights are regular model weights which are being updated after a predefined number of episodes. Conversely, fast weights are initialised from the existing slow weights for each episode, and then based on episode's support samples, the fast weights are adapted for a predefined number of steps. After the adaptation procedure, the loss computed based on model's prediction on episodes' query samples is computed. This is repeated for predefined number of episodes after which the slow weights are updated based on query samples loss.

On the other hand, the recently proposed Baseline method~\cite{baseline} is trained conventionally in a supervised manner.  During inference, however, a new classification head is initialized from scratch and fine-tuned using stochastic gradient  descent on the provided support vectors. In the classical Baseline configuration, the classification head is a single linear layer. However, there is also an alternative configuration, where the classification head is a cosine similarity layer in which for each  class, we have a trainable class prototype. The cosine similarity layer then computes the cosine similarity between the query sample  and each class prototype.

% The primary strategies for few-shot classification can be categorised into two approaches. The first involves training a classifier from scratch or adapting pre-trained models through fine-tuning. This approach is exemplified by Model-Agnostic Meta-Learning (MAML)~\cite{maml}, which extends beyond basic fine-tuning by integrating a meta-training phase. This phase is designed to optimise weight initialisation, such that the model can generalise well to any downstream few-shot classification task.

% The second category encompasses metric-based methods, which focus on learning discriminative embeddings through meta-learning. Unlike the first approach, these methods do not directly map features to targets. Instead, they use the learned embeddings to classify new instances based on simple distance-based heuristics. Notable methods in this category include Matching Networks~\cite{matchingnet} and Prototypical Networks (ProtoNet)~\cite{protonet}.



\subsection{Self-Optimal Transport}

% Outline
% 0. High level: 
% In the second phase of our benchmark study, we enhance each method with a Self-Optimal-Transport (SOT) feature transform module \cite{sot}  to explore its potential in improving overall classification performance.  SOT, operating through pair-wise cosine distance computations, works towards alignment of a given sample with the most similar  samples in the dataset. Consequently, samples from the same class should ideally exhibit similar embeddings, thereby facilitating  subsequent classification. By definition, this effect is particularly advantageous for distance metric-based methods.
% 1. Cost matrix computation
% 2. Sinkhorn 
% The \textbf{Sinkhorn algorithm} is used. It is an iterative procedure that adjusts the elements of a matrix to make its rows and columns sum up to specified target vectors (usually probability distributions). We used a variant that operates in log space for improved numerical stability, especially when dealing with very small or very large numbers. 
% 3. Important properties - explanatibility, direct and indirect comparison

In the second phase of our benchmark study, we enhance each method with the Self-optimal transport (SOT) feature transform~\cite{sot}. This method is used to transform features produced by the backbone. Each new embedded feature vector created by the SOT encodes similarities to all other feature vectors, thereby improving the performance of the classifier.  At its core, SOT employs an \textbf{Optimal Transport (OT)}, which is a mathematical theory for efficiently transforming one probability distribution into another. 

The feature set $V$ containing $n$ vectors of dimension $d$ is re-embedded using a transform $T$, to obtain a new set of features $W = T(V)$, where $W \in \mathbb{R}^{n \times n}$. The proposed transform $T: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times n}$ acts on the original feature set \( V \) as follows. It begins by computing the squared cosine pairwise distances matrix $D$. $W$ will be computed as the optimal transport plan matrix between the $n$-dimensional all-ones vector $\mathbf{1}_n$ and itself, under the cost matrix $D_{\infty}$, which is the distance matrix $D$  with a very large scalar $\alpha$ replacing each of the entries on its diagonal. We used $\alpha=1000$ as it was the constant the authors of SOT also used.

\( W \) is defined to be the doubly-stochastic matrix, that is the minimizes Frobenius dot-product between $D_{\infty}$ and $W$. To compute $W$, authors use the highly efficient \textbf{Sinkhorn-Knopp} method, which is an iterative scheme that optimizes an entropy-regularized version of the problem, where each iteration takes $\Theta(n^2)$. We used $10$ Sinkhorn iterations as it was also used by the authors.

The transport-plan matrix $W$ is the result of the transform, the final set of features $W$ is obtained by replacing the diagonal entries from $0$s to $1$s. Each row is the re-embedding of the corresponding row in $V$. $W$ is doubly stochastic and symmetric.

In the $i$-th feature vector, the $j$-th value represents the relative belief that feature vectors $i$ and $j$ belong to the same `class'. The reason behind this interpretation falls outside the scope of this paper. However, for a more comprehensive understanding and further details, please refer to the original paper \cite{sot}.

An important property of the SOT embedding is that by comparing embedded vectors $w_i$ and $w_j$ we acquire both direct and indirect information about the similarity between the features. This can be seen if we look at the different coordinates $k$ of the absolute difference vector $a=|w_i - w_j|$. When $k\in\{i,j\}$, we have $a_k = 1 - w_{ij} = 1 - w_{ji}$. If \( a_k \) is small, it means the features are directly similar.

When $k \notin \{i,j\}$, we have $a_k = |wik - wjk|$. If $a_k$ is small it means that features $i$ and $j$ have similar beliefs or relationships with feature $k$. They are indirectly similar through their common relationship with the $k$-th feature vector.

Other perks of this method are parameterless-ness, differentiability, equivariance to permutation of the features, and explainability. On the other hand, the fact that the output dimension depends on (is equal to) the number of feature vectors causes problems when the downstream calculation that follows the feature embedding expects a fixed input size.