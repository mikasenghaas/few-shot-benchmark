\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amsfonts}

\usepackage[accepted]{cs502}

% \icmltitlerunning{Submission and Formatting Instructions for ICML 2021}

\begin{document}

\twocolumn[
\icmltitle{
Benchmarking SOT Feature Transforms for Biomedical Few-Shot Learning Tasks
}

\begin{icmlauthorlist}
\icmlauthor{Mika Senghaas}{mika.senghaas@epfl.ch}
\icmlauthor{Ludek Cizinsky}{ludek.cizinsky@epfl.ch}
\icmlauthor{Adam Barla}{adam.barla@epfl.ch}
\end{icmlauthorlist}

\vskip 0.3in
]

\begin{abstract}
    Learning from few samples remains a major challenge in machine learning, especially in biomedical tasks were data availability is often a limiting factor. Our study conducts a comprehensive evaluation of prominent few-shot learning algorithms, as cited in~\cite{baseline, matchingnet, protonet, maml, sot} applied to two biomedical tasks  adapted for few-shot learning: single-cell type prediction using the Tabula Muris dataset~\cite{tabula2018} and protein function prediction with SwissProt data~\cite{uniprot2019}.
    The results demonstrate the robustness of these algorithms in learning effective representations even from limited data. Notably, employing the transductive SOT feature transform combined with meta-learners emerged as a highly effective approach, facilitating rapid and precise knowledge transfer.
\end{abstract}

\section{Introduction}

Biomedical research often grapples with data scarcity, typically due to high costs of data collection and expert-dependent annotation processes. Traditional machine learning approaches often fall short in these data-limited settings as they require extensive training samples and iterations. Few-shot learning algorithms, tailored to discern distinct features from minimal data, offer a promising alternative. Despite the abundance of few-shot learning tasks and the diverse set of methods proposed in recent literature~\cite{baseline, matchingnet, protonet, maml} their application has been largely confined to vision tasks (\textit{CITE}). This study aims to assess the applicability of these methods in the biomedical domain. We examine four established few-shot learning techniques — Baseline(++)~\cite{baseline}, Matching Networks~\cite{matchingnet}, Prototypical Networks~\cite{protonet}, and Model-Agnostic Meta-Learning (MAML)~\cite{maml} — across two distinct biomedical tasks. The first involves predicting cell types based on gene expression, using the Tabula Muris dataset~\cite{tabula2018}, with training, validation, and testing conducted on cells from separate tissues to facilitate cross-tissue knowledge transfer. The second task focuses on predicting protein functions from their sequence embeddings, using the SwissProt dataset~\cite{uniprot2019}. Here, the challenge is to meta-train on protein functions that are distinct from those in the validation and testing phases, ensuring a robust few-shot learning framework.

All of the above approaches rely on meaningful embed-
dings of features. However, a notable challenge arises due to the potential discrepancy in data distributions between the meta-training and testing phases. This discrepancy can result in embeddings that are not fully transferable, leading to suboptimal performance in downstream tasks. To address this issue, Self-Optimal-Transport (SOT)~\cite{sot} has been proposed. SOT is a parameter-less feature transform module, grounded in probabilistic interpretations, and has shown promising results in improving performance across various few-shot learning benchmarks. We include the SOT feature transform module in all of the above mentioned few-shot learning methods to study its effectiveness in the biomedical domain.

In summary, our study presents two primary contributions. First, we train and assess leading few-shot learning algorithms on two unique biomedical tasks, varying the number of classes and the quantity of training samples. Second, we explore the influence of the Self-Optimal-Transport (SOT) feature transform on few-shot learning performance, delving into the dynamics of SOT's integration with other components within meta-learners.

\section{Dataset}

% Introduce datasets
This project utilises two biomedical datasets adapted for few-shot learning. Table~\ref{tab:data} details relevant statistics about the meta-splits for both datasets, including information about the number of samples, targets and potential overlap.

% TODO: (Optional) Statistics about splits for both datasets (#samples, #targets, dimensionality)
% TODO: (Optional) Figure about difficulty of the task by showing PCA features of samples within k episodes (visually explore the class separation)

% Tabula Muris
\subsection{Tabula Muris}

% TODO: Find out total number of samples/ targets
% TODO: Mention pre-processing (filtering cells/ genes, log-transform, zero imputation)
% TODO: Discuss the overlap a bit more
% TODO: Figure out the other pre-processing steps (filter cells with <5000 counts? normalise per cell?)

The first dataset, Tabula Muris~\cite{tabula2018} (\texttt{TM}), comprises over 100,000 mouse cells' gene expression data and annotations about the cell ontology class (cell type). The task is to predict the cell type based on the gene expression data. To address the sparsity and skewed distribution of raw gene expressions, preprocessing included gene filtering, cell filtering, log-transformation, capping at 10, and mean normalization, with zero imputation for non-zero entries. Post-processing, the dataset features 105,960 cells across 125 cell types.

For the few-shot learning task, the focus is on generalisation across different tissues. The dataset is divided into meta-training, meta-validation, and meta-testing sets, each representing distinct tissue types: 15 for training and four each for validation and testing. Despite some overlap in cell types across tissues this structure ensures diverse tissue representation and makes the task of predicting the cell type in novel tissues challenging.

% #Samples (after pre-processing)
% Train: 65846 (0,6214231786)
% Val: 15031 (0,1418554171)
% Test: 25083 (0,2367214043)
% Total: 105.960 

% #Targets
% Train: 59
% Val: 47
% Test: 37
% Total: 125

\subsection{SwissProt}

% TODO: Get more clarity on gene ontology (GO) annotations
% TODO: Possibly give some examples of protein functions for clarity
The second dataset, Swissprot~\cite{uniprot2019} (denoted as \texttt{SP}), is an extensively annotated protein sequence database featuring 14,251 sequences, enriched with comprehensive information on their functions, structures, and biological roles. The project utilises pre-computed embeddings of these protein sequences to predict protein functions. The dataset encompasses 884 unique annotated protein functions. To facilitate a few-shot learning environment, the dataset is divided into meta-training, meta-validation, and meta-testing splits, with each split comprising protein sequences with distinct functions.

% #Samples
% Train: 12141 (0,8519402147) 
% Val: 1407 (0,0987299137)
% Test: 703 (0,0493298716)
% Total: 14.251

% #Targets
% Train: 636
% Val: 159
% Test: 89
% Total: 884

\begin{table}[h]
    \centering
    \caption{Dataset Statistics}
    \label{tab:data}
    \begin{tabular}{lllll}
        \toprule
        & \textbf{Split} & \textbf{\#Samples} (\%) & \textbf{\#Targets} & \textbf{Overlap} \\
        \midrule
        \multirow{3}{*}{\texttt{TM}} 
        & Train. & 65,846 (62\%) & 59 & N/A \\
        & Val. & 15,031 (14\%) & 47 & 10 \\
        & Test. & 25,083 (24\%) & 37 & 4 \\
        \\
        \multirow{3}{*}{\texttt{SP}} 
        & Train. & 12,141 (85\%) & 636 & N/A \\
        & Val. & 1,407 (10\%) & 159 & 0 \\
        & Test. & 703 (5\%) & 89 & 0 \\
        \bottomrule
    \end{tabular}
\end{table}


\section{Methodology}
\subsection{Learning paradigms}
The methods presented in our work use two types of learning. In supervised learning, the objective of the model is to learn the general mapping from the observed samples seen during training to the given task and then be able to apply this knowledge on unseen samples during inference. However, in practice, there is often a need to use the model on a slightly different task than it was initially trained on. One possible solution is to fine tune the model on the subset of data for the new task. This, however, depending on the difficulty of the task might require large amount of labeled data. 

This motivated research in meta learning where the focus is towards teaching the model to be able to learn quickly. Therefore, during the training, in each epoch, the model is presented with the given number of episodes where each episode can be viewed as a different task. Given that in our work, we solely focus on classification, each episode consists of the given number of classes (\texttt{n-way}) where for each class we have certain number of support samples (\texttt{n-shot}) and query samples. During inference, the model is evaluated on its ability to adapt to newly presented tasks.  It first uses the support vectors from the episode to adapt to the task at hand and then its performance is evaluated based on the query samples. The final performance of a model is a mean score across all episodes which should reflect its ability to adapt quickly to new tasks, even with a small amount of samples.

Last but not the least, in our work we differentiate between inductive and transductive methods. The transducive methods, in contrast to the inductive ones, use as part of their adaption process query samples' features. This is indeed possible only in meta learning setting where we have access to the features of the samples we later evaluate our model on, in contrast to the classical batch learning.

\subsection{Methods}
For our analysis, we chose four state-of-the-art (SOTA) methods which place high accross several standard image classification tasks. Each of these methods expect each input sample to be embedded. For the embedding task, we therefore use the same fully connected feed forward neural network which is often refered to as \texttt{backbone}.

Prototypical Network~\cite{protonet} (\texttt{ProtoNet}) is a metric based meta-learning approach that first aggregates support samples within each class to create class prototypes. We then compare query samples to each of the respective class prototypes and assign the query to the closest prototype.

In contrast, Matching Network~\cite{matchingnet} (\texttt{MatchingNet}) employs pairwise comparisons, comparing the query sample to all support samples for each class. In addition, \texttt{MatchingNet} includes further transformation of both support and query samples. The support embeddings are contextualised via a bidirectional Long-Short-Term Memory network (LSTM). These are then compared to every query embedding via a cosine distance to determine how much much weight should be each support vector given when constructing the new version of query embedding. The final step involves contextualizing query embeddings through multiple iterations in a single LSTM Cell. Overall, these steps aim to enhance \texttt{MatchingNet}'s discriminative power for improved classification performance.

Model Agnostic Meta Learning~\cite{maml} (\texttt{MAML}) takes in contrast to the previous two methods metric driven approach. \texttt{MAML} focuses on learning an effective initialization of weights during training. Specifically, there are two kinds of weights in \texttt{MAML}. Slow weights are regular model weights which are being updated after a predefined number of episodes. Conversely, fast weights are initialised from the existing slow weights for each episode, and then based on episode's support samples, the fast weights are adapted for a predefined number of steps. After the adaptation procedure, the loss computed based on model's prediction on episodes' query samples is computed. This is repeated for predefined number of episodes after which the slow weights are updated based on query samples loss.

On the other hand, the recently proposed Baseline method~\cite{baseline} is trained conventionally in a supervised manner.  During inference, however, a new classification head is initialized from scratch and fine-tuned using stochastic gradient  descent on the provided support vectors. In the classical Baseline configuration, the classification head is a single linear layer. However, there is also an alternative configuration, where the classification head is a cosine similarity layer in which for each  class, we have a trainable class prototype. The cosine similarity layer then computes the cosine similarity between the query sample  and each class prototype.

% The primary strategies for few-shot classification can be categorised into two approaches. The first involves training a classifier from scratch or adapting pre-trained models through fine-tuning. This approach is exemplified by Model-Agnostic Meta-Learning (MAML)~\cite{maml}, which extends beyond basic fine-tuning by integrating a meta-training phase. This phase is designed to optimise weight initialisation, such that the model can generalise well to any downstream few-shot classification task.

% The second category encompasses metric-based methods, which focus on learning discriminative embeddings through meta-learning. Unlike the first approach, these methods do not directly map features to targets. Instead, they use the learned embeddings to classify new instances based on simple distance-based heuristics. Notable methods in this category include Matching Networks~\cite{matchingnet} and Prototypical Networks (ProtoNet)~\cite{protonet}.



\subsection{Self-Optimal Transport}

% Outline
% 0. High level: 
% In the second phase of our benchmark study, we enhance each method with a Self-Optimal-Transport (SOT) feature transform module \cite{sot}  to explore its potential in improving overall classification performance.  SOT, operating through pair-wise cosine distance computations, works towards alignment of a given sample with the most similar  samples in the dataset. Consequently, samples from the same class should ideally exhibit similar embeddings, thereby facilitating  subsequent classification. By definition, this effect is particularly advantageous for distance metric-based methods.
% 1. Cost matrix computation
% 2. Sinkhorn 
% The \textbf{Sinkhorn algorithm} is used. It is an iterative procedure that adjusts the elements of a matrix to make its rows and columns sum up to specified target vectors (usually probability distributions). We used a variant that operates in log space for improved numerical stability, especially when dealing with very small or very large numbers. 
% 3. Important properties - explanatibility, direct and indirect comparison

In the second phase of our benchmark study, we enhance each method with the Self-optimal transport (SOT) feature transform~\cite{sot}. This method is used to transform features produced by the backbone. Each new embedded feature vector created by the SOT encodes similarities to all other feature vectors, thereby improving the performance of the classifier.  At its core, SOT employs an \textbf{Optimal Transport (OT)}, which is a mathematical theory for efficiently transforming one probability distribution into another. 

The feature set $V$ containing $n$ vectors of dimension $d$ is re-embedded using a transform $T$, to obtain a new set of features $W = T(V)$, where $W \in \mathbb{R}^{n \times n}$. The proposed transform $T: \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times n}$ acts on the original feature set \( V \) as follows. It begins by computing the squared cosine pairwise distances matrix $D$. $W$ will be computed as the optimal transport plan matrix between the $n$-dimensional all-ones vector $\mathbf{1}_n$ and itself, under the cost matrix $D_{\infty}$, which is the distance matrix $D$  with a very large scalar $\alpha$ replacing each of the entries on its diagonal. We used $\alpha=1000$ as it was the constant the authors of SOT also used.

\( W \) is defined to be the doubly-stochastic matrix, that is the minimizes Frobenius dot-product between $D_{\infty}$ and $W$. To compute $W$, authors use the highly efficient \textbf{Sinkhorn-Knopp} method, which is an iterative scheme that optimizes an entropy-regularized version of the problem, where each iteration takes $\Theta(n^2)$. We used $10$ Sinkhorn iterations as it was also used by the authors.

The transport-plan matrix $W$ is the result of the transform, the final set of features $W$ is obtained by replacing the diagonal entries from $0$s to $1$s. Each row is the re-embedding of the corresponding row in $V$. $W$ is doubly stochastic and symmetric.

In the $i$-th feature vector, the $j$-th value represents the relative belief that feature vectors $i$ and $j$ belong to the same `class'. The reason behind this interpretation falls outside the scope of this paper. However, for a more comprehensive understanding and further details, please refer to the original paper \cite{sot}.

An important property of the SOT embedding is that by comparing embedded vectors $w_i$ and $w_j$ we acquire both direct and indirect information about the similarity between the features. This can be seen if we look at the different coordinates $k$ of the absolute difference vector $a=|w_i - w_j|$. When $k\in\{i,j\}$, we have $a_k = 1 - w_{ij} = 1 - w_{ji}$. If \( a_k \) is small, it means the features are directly similar.

When $k \notin \{i,j\}$, we have $a_k = |wik - wjk|$. If $a_k$ is small it means that features $i$ and $j$ have similar beliefs or relationships with feature $k$. They are indirectly similar through their common relationship with the $k$-th feature vector.

Other perks of this method are parameterless-ness, differentiability, equivariance to permutation of the features, and explainability. On the other hand, the fact that the output dimension depends on (is equal to) the number of feature vectors causes problems when the downstream calculation that follows the feature embedding expects a fixed input size.

\subsection{Experimental setup}
In our setup, we use the same backbone for all methods. The backbone is a fully connected feed forward neural network with batch normalization
layer, ReLU activation and dropout. For the \texttt{TM} dataset, the backbone two hidden layers each with 64 neurons. For the \texttt{SP} dataset,
the backbone also has two hidden layers, but with 512 neurons each. 

For each experiment, we train the model for a maximum of 40 epochs. We use early stopping with a patience of 5 epochs. In addition, we use 5 query samples per class for each episode across all experiments, and evaluate on fixed number of epochs (600) to reduce variance.  Each experiment is defined by five parameters: \texttt{method}, \texttt{dataset}, \texttt{way}, \texttt{shot} and \texttt{use sot}.

In the first batch of experiments, we fix \texttt{way} and \texttt{shot} to 5 and vary the remaining three parameters. Thus, in total we have 20 experiments. 
This way, we should be able to answer the impact of \texttt{method}, \texttt{dataset} and \texttt{use sot} on the performance of meta-learning based methods.
Through the second batch of experiments, we aim to answer impact of \texttt{way} and \texttt{shot} on the performance of meta-learning based methods.
We therefore fix \texttt{method} to be \texttt{ProtoNet} and \texttt{dataset} to \texttt{TM}. Then we vary binary flag \texttt{use sot}, \texttt{way} to be 2, 4, 6, 8, 10, and \texttt{shot} to be 1, 5, 10, 15 and 20, in total we have therefore 50 experiments.

% TODO: we need to mention along which dimensions we have finetuned each of the methods including SOT!

\section{Results}
Table \ref{tab:tuned-benchmark} displays the results for the first part of our benchmark study. We fine-tuned each method on the given dataset and then evaluated it across 600 episodes. In the table, we present the mean accuracy, accompanied by a 95\% confidence interval across the episodes. The \texttt{Diff} column indicates the change in mean accuracy when we introduce the \texttt{SOT} module to the method.

% TODO: add 5 way-5shot without SOT --> pretty solid baseline performance
First, we can see that already, without the inclusion of \texttt{SOT} module, all methods reach 


Initially, we observe that the presence of the \texttt{SOT} module enhances all methods except for the Baseline. However, for \texttt{B++}, the performance improvement is only marginal. Conversely, for the remaining three methods, \texttt{SOT} improved the performance for the \texttt{TM} dataset by an average of 12\% and by 48\% for the \texttt{SP} dataset. Therefore, for both datasets, we see a large effect size after integration of the \texttt{SOT} module.

\begin{table}[!ht]
\caption{Results of the benchmark experiment.}
\label{tab:tuned-benchmark}
\centering
\begin{tabular}{llllr}
\toprule
 &  & Acc & Acc w/ SOT & Diff (\%) \\
\midrule
\multirow[c]{5}{*}{Tabula Muris} & B & 90.73 ± 0.69 & 86.34 ± 0.85 & -4.84 \\
 & B++ & 81.88 ± 0.88 & 82.76 ± 0.93 & \bfseries 1.07 \\
 & MAML & 92.84 ± 0.55 & 99.21 ± 0.14 & \bfseries 6.86 \\
 & MT & 84.57 ± 0.80 & 99.71 ± 0.09 & \bfseries 17.90 \\
 & PT & 87.10 ± 0.81 & 98.62 ± 0.21 & \bfseries 13.23 \\
\midrule
\multirow[c]{5}{*}{SwissProt} & B & 69.18 ± 0.72 & 55.65 ± 0.76 & -19.55 \\
 & B++ & 64.12 ± 0.69 & 64.61 ± 0.72 & \bfseries 0.77 \\
 & MAML & 68.65 ± 0.71 & 98.03 ± 0.22 & \bfseries 42.79 \\
 & MT & 68.16 ± 0.76 & 99.83 ± 0.07 & \bfseries 46.46 \\
 & PT & 63.54 ± 0.73 & 99.29 ± 0.14 & \bfseries 56.26 \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\newpage
\bibliography{main}
\bibliographystyle{cs502}

\end{document}

